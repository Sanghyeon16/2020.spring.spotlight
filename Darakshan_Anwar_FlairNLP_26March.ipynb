{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlairNLP\n",
    "\n",
    "FlairNLP is a simple Pytorch based framework that facilitates the application of state-of-art natural language processing models such as part of speech tagging, name entity recognition, emerging entity detection , text classification etc to our texts. It also supports growing number of languages as it includes 'one model, many languages' taggers, i.e. single models that predict PoS or NER tags for input text in various languages.\n",
    "\n",
    "The framework presents a simple interface for different types of word and document embeddings which hides all embedding specific complexit and allows users to mix and match various embeddings as a stacked embedding to improve the performance.\n",
    "\n",
    "The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation \n",
    "For installing flair on your system simply do :\n",
    "\n",
    "pip install flair\n",
    "\n",
    "For anaconda environment follow steps in below link :\n",
    "https://medium.com/@taras.priadka/how-to-install-flair-for-jupyter-notebook-755929c5f04f\n",
    "\n",
    "Note: This requires you to install pytorch with version 1.2.0. You can use below command for correct version :\n",
    "\n",
    "conda install pytorch torchvision cudatoolkit=10.0 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLP Base types \n",
    "\n",
    "Two major objects discussed here are: Sentence and Token Objects.\n",
    "The sentence objects holds a sentence that we may want to embed or tag. A sentence is a list of token. \n",
    "We can have a sentence already tokenized ( a whitespace tokenized string) or can use a customised tokeniser to tokenise the sentence with the help of flair.  \n",
    "\n",
    "Both cases are shown below: \n",
    "1) A Whitespace tokenised sentence is taken and we count the number of tokens.\n",
    "2) A untokenised sentence is taken and a customised tokeniser is passed and we count the number of tokens in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Whitespaced tokenised string here .\" - 5 Tokens\n",
      "Token: 4 here\n",
      "Token: 4 here\n",
      "\n",
      "Print all the token in the string\n",
      "Token: 1 Whitespaced\n",
      "Token: 2 tokenised\n",
      "Token: 3 string\n",
      "Token: 4 here\n",
      "Token: 5 .\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "sentence = Sentence('Whitespaced tokenised string here .')\n",
    "print(sentence)\n",
    "\n",
    "#Access token using token id or index\n",
    "print(sentence.get_token(4))\n",
    "print(sentence[3])\n",
    "\n",
    "print(\"\\nPrint all the token in the string\")\n",
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Untokenised string here .\" - 4 Tokens\n",
      "Sentence: \"Untokenised string here .\" - 4 Tokens\n"
     ]
    }
   ],
   "source": [
    "#Untokenised strin -- As there is no space between here and . \n",
    "#Using a default tokeniser \n",
    "sentence = Sentence('Untokenised string here.', use_tokenizer=True)\n",
    "print(sentence)\n",
    "\n",
    "#Adding Custom Tokenizers\n",
    "from flair.data import Sentence, segtok_tokenizer\n",
    "sentence = Sentence('Untokenised string here.', use_tokenizer=segtok_tokenizer)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding Tags to Tokens\n",
    "A Token has fields for linguistic annotation, such as lemmas, part-of-speech tags or named entity tags. \n",
    "Tags can be added to token by specifying the tag type and the tag value.\n",
    "In the example below, We are adding a NER (name-entity Recognition) tag of type 'place' to word Texas and tag of type 'person' to Bob.\n",
    "\n",
    "Each tag is of class Label which consists of value and score. Score here indicates confidence.\n",
    "In our example below, all the tags have confidence 1 as we have manually added the tag.If a tag is predicted by a sequence labeler, the score indicates classifier confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob <person> visited Texas <place>\n",
      "\"Token: 3 Texas\" is tagged as \"place\" with confidence score \"1.0\"\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('Bob visited Texas')\n",
    "sentence[2].add_tag('ner', 'place')\n",
    "sentence[0].add_tag('ner', 'person')\n",
    "# print the sentence with all tags.\n",
    "print(sentence.to_tagged_string())\n",
    "\n",
    "token = sentence[2]\n",
    "tag = token.get_tag('ner')\n",
    "\n",
    "# print token alongwth the tag value and score\n",
    "print(f'\"{token}\" is tagged as \"{tag.value}\" with confidence score \"{tag.score}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Labels to Sentences\n",
    "We can add labels to our sentences which can be useful in text classfication tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the labels\n",
      "College (1.0)\n",
      "Education (1.0)\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('This is Computer Science Department of Texas A&M Univerisity college Station')\n",
    "\n",
    "# adding label to a sentence\n",
    "sentence.add_label('Education')\n",
    "\n",
    "#adding more than one labels to a sentence\n",
    "sentence.add_labels(['College', 'Education'])\n",
    "\n",
    "# Adding labels while initialising a sentence\n",
    "sentence = Sentence('This is Computer Science Department of Texas A&M Univerisity college Station', labels=['College', 'Education'])\n",
    "\n",
    "print(\"Printing the labels\")\n",
    "for label in sentence.labels:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Texts with Pre-Trained Sequence Tagging Models\n",
    "\n",
    "Flair provides various pre-trained models for Named Entity Recognition, Syntactic Chunking, Part-of-Speech Tagging, Semantic Frame Detection. These models are trained on a particular dataset. These pre-trained models can be used to tag our texts.Flair also provides some smaller models that can run faster on CPU.\n",
    "\n",
    "Also, it has multi-lingual support as it distributes models that are capable of handling text in multiple language.\n",
    "The NER models are trained over 4 languages (English, German, Dutch and Spanish) and the PoS models over 12 languages (English, German, French, Italian, Dutch, Polish, Spanish, Swedish, Danish, Norwegian, Finnish and Czech).\n",
    "\n",
    "Find the details of these models in below table.\n",
    "\n",
    "| ID | Task | Training Dataset |\n",
    "| --- | --- | --- |\n",
    "| 'ner' | 4-class Named Entity Recognition | Conll-03 |\n",
    "| 'ner-ontonotes' | 18-class Named Entity Recognition | Ontonotes|\n",
    "| 'pos' | Part-of-Speech Tagging | Ontonotes|\n",
    "| 'chunk' | Syntactic Chunking\t| Conll-2000 |\n",
    "| 'frame' | Semantic Frame Detection | Propbank 3.0|\n",
    "| 'ner-fast' | 4-class Named Entity Recognition | Conll-03 |\n",
    "| 'ner-multi' | 4-class Named Entity Recognition | Conll-03 (4 languages) |\n",
    "| 'de-ner' | German Language 4-class Named Entity Recognition | Conll-03(German) |\n",
    "| 'pos-multi' | Part-of-Speech Tagging for multi-lingual | Universal Dependency Treebank (12 languages) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, pre-trained model for named entity recognition (NER) is used. This model is trained over the English CoNLL-03 task and can recognize 4 different entity types.\n",
    "\n",
    "Different pre-trained model can be used by passing the corresponding ID( from the above table) to the load method in SequenceTagged class. Then we can use predict method of the tagger to add the predicted tags to the token in the given sentence.\n",
    "Also, some sequence labeller annotate spans which consists of more than one word.Using get_spans method, we get list of these spans. Each Span has a text, a value, position in the sentence and score that indicating prediction confidence. We can get all these details using to_dict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 21:34:30,865 loading file C:\\Users\\DARKHO\\.flair\\models\\en-ner-conll03-v0.4.pt\n",
      "\n",
      "Print sentence with predicted tags\n",
      "\n",
      "Bob <B-PER> Marley <E-PER> visited Texas <S-LOC> .\n",
      "\n",
      "Print Span for each tag type\n",
      "PER-span [1,2]: \"Bob Marley\"\n",
      "LOC-span [4]: \"Texas\"\n",
      "\n",
      "Print Span Details \n",
      "{'text': 'Bob Marley visited Texas .', 'labels': [], 'entities': [{'text': 'Bob Marley', 'start_pos': 0, 'end_pos': 10, 'type': 'PER', 'confidence': 0.9997925162315369}, {'text': 'Texas', 'start_pos': 19, 'end_pos': 24, 'type': 'LOC', 'confidence': 0.9999349117279053}]}\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "tagger_model = SequenceTagger.load('ner')\n",
    "\n",
    "sentence = Sentence('Bob Marley visited Texas .')\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(\"\\nPrint sentence with predicted tags\\n\")\n",
    "print(sentence.to_tagged_string())\n",
    "\n",
    "print(\"\\nPrint Span for each tag type\")\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)\n",
    "print(\"\\nPrint Span Details \")\n",
    "print(sentence.to_dict(tag_type='ner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example for a german sentence : here 'de-ner' model is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 22:31:04,539 loading file C:\\Users\\DARKHO\\.flair\\models\\de-ner-conll03-v0.4.pt\n",
      "\n",
      "Print sentence with predicted tags\n",
      "\n",
      "Bob <B-PER> Marley <E-PER> ging nach Texas <S-LOC> .\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('de-ner')\n",
    "sentence = Sentence('Bob Marley ging nach Texas .')\n",
    "tagger.predict(sentence)\n",
    "print(\"\\nPrint sentence with predicted tags\\n\")\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for a multi lingual Text : Here we are taking a sentence with combination of English and German words. For this case we use \"pos-multi\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 21:46:46,593 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/release-dodekapos-512-l2-multi/pos-multi-v0.1.pt not found in cache, downloading to C:\\Users\\DARKHO\\AppData\\Local\\Temp\\tmpv_lj_qhe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 314055714/314055714 [01:11<00:00, 4369834.26B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 21:47:59,349 copying C:\\Users\\DARKHO\\AppData\\Local\\Temp\\tmpv_lj_qhe to cache at C:\\Users\\DARKHO\\.flair\\models\\pos-multi-v0.1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 21:47:59,934 removing temp file C:\\Users\\DARKHO\\AppData\\Local\\Temp\\tmpv_lj_qhe\n",
      "2020-03-26 21:47:59,972 loading file C:\\Users\\DARKHO\\.flair\\models\\pos-multi-v0.1.pt\n",
      "\n",
      "Print sentence with predicted tags\n",
      "\n",
      "Bob <PROPN> Marly <PROPN> visited <VERB> Texas. <PROPN> Er <ADV> ging <VERB> reiten <VERB>\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('pos-multi')\n",
    "sentence = Sentence('Bob Marly visited Texas. Er ging reiten')\n",
    "tagger.predict(sentence)\n",
    "print(\"\\nPrint sentence with predicted tags\\n\")\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging with Pre-Trained Text Classification Models\n",
    "Flair provides some pre-trained text classifier models also. Details can be found below.\n",
    "\n",
    "| ID | Task | Training Dataset |\n",
    "| --- | --- | --- |\n",
    "| 'en-sentiment' | detecting positive and negative sentiment in English | movie reviews from IMDB |\n",
    "| ''de-offensive-language' | detecting offensive language in German | GermEval 2018 Task 1|\n",
    "\n",
    "\n",
    "Here, We have taken an example of detecting negative and positive reviews using 'en-sentiment' model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 21:54:23,348 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/classy-imdb-en-rnn-cuda%3A0/imdb-v0.4.pt not found in cache, downloading to C:\\Users\\DARKHO\\AppData\\Local\\Temp\\tmpfzxddapc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 1501979561/1501979561 [05:50<00:00, 4279875.25B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 22:00:14,974 copying C:\\Users\\DARKHO\\AppData\\Local\\Temp\\tmpfzxddapc to cache at C:\\Users\\DARKHO\\.flair\\models\\imdb-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 22:00:24,394 removing temp file C:\\Users\\DARKHO\\AppData\\Local\\Temp\\tmpfzxddapc\n",
      "2020-03-26 22:00:24,473 loading file C:\\Users\\DARKHO\\.flair\\models\\imdb-v0.4.pt\n",
      "[NEGATIVE (0.9980487823486328)]\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "sentence = Sentence('This film doesnt make any sense. It is so bad that I am confused.')\n",
    "\n",
    "classifier.predict(sentence)\n",
    "\n",
    "# print sentence with predicted labels\n",
    "print(sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings with Flair\n",
    "Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text.\n",
    "Flair provides interfaces that allow to use and combine different word and document embedding like Flair embeddings, BERT embeddings etc.\n",
    "\n",
    "Classic word embeddings are static and word-level, meaning that each distinct word gets exactly one pre-computed embedding.\n",
    "\n",
    "Contextual string embeddings are powerful embeddings that capture latent syntactic-semantic information that goes beyond standard word embeddings. Key differences are: (1) they are trained without any explicit notion of words and thus fundamentally model words as sequences of characters. And (2) they are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use.\n",
    "\n",
    "Stacked embeddings are one of the most important concepts of this library. You can use them to combine different embeddings together, for instance if you want to use both traditional embeddings together with contextual string embeddings. Stacked embeddings allow you to mix and match. We find that a combination of embeddings often gives best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-0.0382, -0.2449,  0.7281,  ..., -0.0065, -0.0053,  0.0090])\n",
      "Token: 2 grass\n",
      "tensor([-0.8135,  0.9404, -0.2405,  ...,  0.0354, -0.0255, -0.0143])\n",
      "Token: 3 is\n",
      "tensor([-5.4264e-01,  4.1476e-01,  1.0322e+00,  ..., -5.3691e-04,\n",
      "        -9.6750e-03, -2.7541e-02])\n",
      "Token: 4 green\n",
      "tensor([-0.6791,  0.3491, -0.2398,  ..., -0.0007, -0.1333,  0.0161])\n",
      "Token: 5 .\n",
      "tensor([-0.3398,  0.2094,  0.4635,  ...,  0.0005, -0.0177,  0.0032])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, CharacterEmbeddings\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "#clasical word embedding\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "# Flair forward and backwards embeddings\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
    "\n",
    "# create a StackedEmbedding combining glove and forward/backward flair embeddings\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "                                        glove_embedding,\n",
    "                                        flair_embedding_forward,\n",
    "                                        flair_embedding_backward,\n",
    "                                       ])\n",
    "sentence = Sentence('The grass is green .')\n",
    "stacked_embeddings.embed(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a Model ( A Text Classifier or Sequence Labeling Model)\n",
    "First, We need to load our training data. We call collection of text as Corpus which can be splitted into training, validation and test data. Corpus can be used to train and test our model.\n",
    "\n",
    "Flair provides a list of prepared datasets and automatically downloads and sets up the data on the first call of  the corresponding constructor ID. Example of one such dataset is given below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 22:41:05,172 Reading data from C:\\Users\\DARKHO\\.flair\\datasets\\ud_english\n",
      "2020-03-26 22:41:05,173 Train: C:\\Users\\DARKHO\\.flair\\datasets\\ud_english\\en_ewt-ud-train.conllu\n",
      "2020-03-26 22:41:05,174 Test: C:\\Users\\DARKHO\\.flair\\datasets\\ud_english\\en_ewt-ud-test.conllu\n",
      "2020-03-26 22:41:05,175 Dev: C:\\Users\\DARKHO\\.flair\\datasets\\ud_english\\en_ewt-ud-dev.conllu\n",
      "12543\n",
      "2077\n",
      "2002\n"
     ]
    }
   ],
   "source": [
    "import flair.datasets\n",
    "corpus = flair.datasets.UD_ENGLISH()\n",
    "# print the number of Sentences in the train split\n",
    "print(len(corpus.train))\n",
    "\n",
    "# print the number of Sentences in the test split\n",
    "print(len(corpus.test))\n",
    "\n",
    "# print the number of Sentences in the dev split\n",
    "print(len(corpus.dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading your own dataset \n",
    "Instead of using provided dataset, we can also load our own dataset in Flair and use it for training and testing.\n",
    "\n",
    "We can load our own dataset using ColumnCorpus object. Most sequence labeling datasets in NLP use some sort of column format in which each line is a word and each column is one level of linguistic annotation.For example: The first column can be the word , the second column has PoS tags, and the third can have NER tags. Empty line separates sentences. To read such a dataset,we have to define the column structure as a dictionary and instantiate a ColumnCorpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'pos', 2: 'ner'}\n",
    "\n",
    "# provide the path for train, test and validation file.\n",
    "data_folder = '/home/Users/Darakshan/IRS_spotlight/'\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train.txt',\n",
    "                              test_file='test.txt',\n",
    "                              dev_file='dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model\n",
    "Below we have taken an example of training Sequence Labeling Model of name-entity recognition type. For training, one of the flair provided dataset i.e WNUT_17. This has six classes of enitity i.e Person, Location, Corporation, Product, Creative Work and Group. In our example, dataset is downsampled by 10 percent for fast computation. Glove embedding provided by flair is used here. \n",
    "\n",
    "We initiate the model class i.e Sequeunce Tagger and pass the embedding being used,tag type of the model, loss function to be used,hidden layer size and the tag dictionary to be used. We made a tag dictionary from the corpus using method make_tag_dictionary method. We pass tag type as NER as we want to train a Name entity recognition type of squence tagger. \n",
    "\n",
    "Then this model is passed the corpus on which it has to be trained and is trained with a fixed hyperparamaters. We can optimise our model by changing these hyperparameters such as learning rate, number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 23:24:31,074 Reading data from C:\\Users\\DARKHO\\.flair\\datasets\\wnut_17\n",
      "2020-03-26 23:24:31,077 Train: C:\\Users\\DARKHO\\.flair\\datasets\\wnut_17\\wnut17train.conll\n",
      "2020-03-26 23:24:31,080 Dev: C:\\Users\\DARKHO\\.flair\\datasets\\wnut_17\\emerging.dev.conll\n",
      "2020-03-26 23:24:31,081 Test: C:\\Users\\DARKHO\\.flair\\datasets\\wnut_17\\emerging.test.annotated\n",
      "Corpus: 339 train + 101 dev + 129 test sentences\n",
      "Dictionary with 28 tags: <unk>, O, S-person, S-corporation, S-location, B-person, E-person, B-location, E-location, B-corporation, E-corporation, S-creative-work, B-creative-work, I-creative-work, E-creative-work, I-location, S-group, B-product, I-product, E-product, I-person, B-group, E-group, S-product, I-group, I-corporation, <START>, <STOP>\n",
      "2020-03-26 23:24:35,685 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:35,686 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings('glove')\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=28, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-03-26 23:24:35,686 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:35,686 Corpus: \"Corpus: 339 train + 101 dev + 129 test sentences\"\n",
      "2020-03-26 23:24:35,686 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:35,687 Parameters:\n",
      "2020-03-26 23:24:35,687  - learning_rate: \"0.1\"\n",
      "2020-03-26 23:24:35,688  - mini_batch_size: \"32\"\n",
      "2020-03-26 23:24:35,688  - patience: \"3\"\n",
      "2020-03-26 23:24:35,689  - anneal_factor: \"0.5\"\n",
      "2020-03-26 23:24:35,690  - max_epochs: \"50\"\n",
      "2020-03-26 23:24:35,690  - shuffle: \"True\"\n",
      "2020-03-26 23:24:35,691  - train_with_dev: \"False\"\n",
      "2020-03-26 23:24:35,691  - batch_growth_annealing: \"False\"\n",
      "2020-03-26 23:24:35,692 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:35,693 Model training base path: \"resources\\taggers\\example-ner\"\n",
      "2020-03-26 23:24:35,694 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:35,694 Device: cpu\n",
      "2020-03-26 23:24:35,695 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:35,695 Embeddings storage mode: cpu\n",
      "2020-03-26 23:24:35,699 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:36,136 epoch 1 - iter 1/11 - loss 61.51676559 - samples/sec: 73.42\n",
      "2020-03-26 23:24:36,595 epoch 1 - iter 2/11 - loss 51.30698586 - samples/sec: 83.12\n",
      "2020-03-26 23:24:37,855 epoch 1 - iter 3/11 - loss 44.00020027 - samples/sec: 27.38\n",
      "2020-03-26 23:24:38,550 epoch 1 - iter 4/11 - loss 36.52518916 - samples/sec: 53.21\n",
      "2020-03-26 23:24:39,255 epoch 1 - iter 5/11 - loss 32.04660549 - samples/sec: 52.09\n",
      "2020-03-26 23:24:39,872 epoch 1 - iter 6/11 - loss 27.59013581 - samples/sec: 61.00\n",
      "2020-03-26 23:24:41,540 epoch 1 - iter 7/11 - loss 24.95498977 - samples/sec: 20.42\n",
      "2020-03-26 23:24:42,181 epoch 1 - iter 8/11 - loss 23.11528021 - samples/sec: 57.40\n",
      "2020-03-26 23:24:43,005 epoch 1 - iter 9/11 - loss 21.09208261 - samples/sec: 43.77\n",
      "2020-03-26 23:24:43,696 epoch 1 - iter 10/11 - loss 19.74099288 - samples/sec: 53.48\n",
      "2020-03-26 23:24:44,258 epoch 1 - iter 11/11 - loss 18.61662934 - samples/sec: 68.56\n",
      "2020-03-26 23:24:44,353 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:44,355 EPOCH 1 done: loss 18.6166 - lr 0.1000\n",
      "2020-03-26 23:24:44,869 DEV : loss 7.904824256896973 - score 0.0\n",
      "2020-03-26 23:24:44,877 BAD EPOCHS (no improvement): 0\n",
      "2020-03-26 23:24:48,324 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:48,864 epoch 2 - iter 1/11 - loss 7.77115345 - samples/sec: 59.53\n",
      "2020-03-26 23:24:49,553 epoch 2 - iter 2/11 - loss 6.14606619 - samples/sec: 53.93\n",
      "2020-03-26 23:24:50,265 epoch 2 - iter 3/11 - loss 6.99842707 - samples/sec: 51.83\n",
      "2020-03-26 23:24:50,985 epoch 2 - iter 4/11 - loss 6.82323527 - samples/sec: 51.25\n",
      "2020-03-26 23:24:51,739 epoch 2 - iter 5/11 - loss 6.37667770 - samples/sec: 48.54\n",
      "2020-03-26 23:24:52,333 epoch 2 - iter 6/11 - loss 6.33654221 - samples/sec: 63.79\n",
      "2020-03-26 23:24:52,928 epoch 2 - iter 7/11 - loss 7.05396863 - samples/sec: 64.30\n",
      "2020-03-26 23:24:53,723 epoch 2 - iter 8/11 - loss 7.03527528 - samples/sec: 45.71\n",
      "2020-03-26 23:24:54,331 epoch 2 - iter 9/11 - loss 7.14096106 - samples/sec: 63.16\n",
      "2020-03-26 23:24:54,911 epoch 2 - iter 10/11 - loss 6.83242898 - samples/sec: 65.88\n",
      "2020-03-26 23:24:55,434 epoch 2 - iter 11/11 - loss 6.97793644 - samples/sec: 74.62\n",
      "2020-03-26 23:24:55,528 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:24:55,531 EPOCH 2 done: loss 6.9779 - lr 0.1000\n",
      "2020-03-26 23:24:55,958 DEV : loss 7.276970863342285 - score 0.0\n",
      "2020-03-26 23:24:55,966 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:24:59,543 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:00,146 epoch 3 - iter 1/11 - loss 7.37911606 - samples/sec: 53.21\n",
      "2020-03-26 23:25:00,863 epoch 3 - iter 2/11 - loss 6.84849930 - samples/sec: 51.34\n",
      "2020-03-26 23:25:01,490 epoch 3 - iter 3/11 - loss 6.84534915 - samples/sec: 60.20\n",
      "2020-03-26 23:25:02,064 epoch 3 - iter 4/11 - loss 6.79267955 - samples/sec: 66.43\n",
      "2020-03-26 23:25:02,700 epoch 3 - iter 5/11 - loss 7.07355309 - samples/sec: 58.23\n",
      "2020-03-26 23:25:03,199 epoch 3 - iter 6/11 - loss 6.75067782 - samples/sec: 78.64\n",
      "2020-03-26 23:25:03,774 epoch 3 - iter 7/11 - loss 7.00481667 - samples/sec: 66.02\n",
      "2020-03-26 23:25:04,408 epoch 3 - iter 8/11 - loss 6.90621787 - samples/sec: 58.87\n",
      "2020-03-26 23:25:05,105 epoch 3 - iter 9/11 - loss 6.51030583 - samples/sec: 53.30\n",
      "2020-03-26 23:25:05,684 epoch 3 - iter 10/11 - loss 6.31816096 - samples/sec: 66.16\n",
      "2020-03-26 23:25:06,147 epoch 3 - iter 11/11 - loss 6.10366069 - samples/sec: 86.72\n",
      "2020-03-26 23:25:06,245 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:06,247 EPOCH 3 done: loss 6.1037 - lr 0.1000\n",
      "2020-03-26 23:25:06,699 DEV : loss 7.2233757972717285 - score 0.0\n",
      "2020-03-26 23:25:06,710 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:25:10,399 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:10,844 epoch 4 - iter 1/11 - loss 4.05522442 - samples/sec: 72.26\n",
      "2020-03-26 23:25:11,386 epoch 4 - iter 2/11 - loss 5.56052899 - samples/sec: 71.14\n",
      "2020-03-26 23:25:12,118 epoch 4 - iter 3/11 - loss 5.71243302 - samples/sec: 50.69\n",
      "2020-03-26 23:25:12,838 epoch 4 - iter 4/11 - loss 5.80352449 - samples/sec: 51.42\n",
      "2020-03-26 23:25:13,458 epoch 4 - iter 5/11 - loss 6.39114246 - samples/sec: 60.88\n",
      "2020-03-26 23:25:14,095 epoch 4 - iter 6/11 - loss 6.16195671 - samples/sec: 58.44\n",
      "2020-03-26 23:25:14,801 epoch 4 - iter 7/11 - loss 6.19980424 - samples/sec: 52.00\n",
      "2020-03-26 23:25:15,540 epoch 4 - iter 8/11 - loss 6.08293563 - samples/sec: 49.67\n",
      "2020-03-26 23:25:16,217 epoch 4 - iter 9/11 - loss 5.93063158 - samples/sec: 55.42\n",
      "2020-03-26 23:25:16,915 epoch 4 - iter 10/11 - loss 5.91921573 - samples/sec: 52.95\n",
      "2020-03-26 23:25:17,478 epoch 4 - iter 11/11 - loss 5.91990657 - samples/sec: 68.27\n",
      "2020-03-26 23:25:17,572 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:17,574 EPOCH 4 done: loss 5.9199 - lr 0.1000\n",
      "2020-03-26 23:25:18,123 DEV : loss 6.796761512756348 - score 0.0\n",
      "2020-03-26 23:25:18,137 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:25:22,475 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:22,920 epoch 5 - iter 1/11 - loss 4.27794647 - samples/sec: 71.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 23:25:23,730 epoch 5 - iter 2/11 - loss 5.98095655 - samples/sec: 45.32\n",
      "2020-03-26 23:25:24,336 epoch 5 - iter 3/11 - loss 6.34879907 - samples/sec: 63.79\n",
      "2020-03-26 23:25:24,932 epoch 5 - iter 4/11 - loss 5.93334746 - samples/sec: 63.16\n",
      "2020-03-26 23:25:25,577 epoch 5 - iter 5/11 - loss 5.54926205 - samples/sec: 57.19\n",
      "2020-03-26 23:25:26,189 epoch 5 - iter 6/11 - loss 5.47720019 - samples/sec: 61.23\n",
      "2020-03-26 23:25:27,039 epoch 5 - iter 7/11 - loss 5.27363477 - samples/sec: 43.89\n",
      "2020-03-26 23:25:27,608 epoch 5 - iter 8/11 - loss 5.20725775 - samples/sec: 67.41\n",
      "2020-03-26 23:25:28,203 epoch 5 - iter 9/11 - loss 5.37558900 - samples/sec: 63.28\n",
      "2020-03-26 23:25:28,846 epoch 5 - iter 10/11 - loss 5.55969872 - samples/sec: 58.55\n",
      "2020-03-26 23:25:29,309 epoch 5 - iter 11/11 - loss 5.65306412 - samples/sec: 86.25\n",
      "2020-03-26 23:25:29,408 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:29,410 EPOCH 5 done: loss 5.6531 - lr 0.1000\n",
      "2020-03-26 23:25:29,822 DEV : loss 6.518157958984375 - score 0.0\n",
      "Epoch     4: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-03-26 23:25:29,833 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:25:33,988 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:34,417 epoch 6 - iter 1/11 - loss 5.94816208 - samples/sec: 74.97\n",
      "2020-03-26 23:25:34,957 epoch 6 - iter 2/11 - loss 4.20432818 - samples/sec: 70.67\n",
      "2020-03-26 23:25:35,484 epoch 6 - iter 3/11 - loss 4.43427300 - samples/sec: 73.25\n",
      "2020-03-26 23:25:36,276 epoch 6 - iter 4/11 - loss 4.96585053 - samples/sec: 45.97\n",
      "2020-03-26 23:25:36,962 epoch 6 - iter 5/11 - loss 5.11503596 - samples/sec: 53.39\n",
      "2020-03-26 23:25:37,819 epoch 6 - iter 6/11 - loss 5.32354732 - samples/sec: 41.67\n",
      "2020-03-26 23:25:38,423 epoch 6 - iter 7/11 - loss 5.86600763 - samples/sec: 64.30\n",
      "2020-03-26 23:25:38,990 epoch 6 - iter 8/11 - loss 5.67720744 - samples/sec: 67.41\n",
      "2020-03-26 23:25:39,590 epoch 6 - iter 9/11 - loss 5.37687204 - samples/sec: 62.79\n",
      "2020-03-26 23:25:40,155 epoch 6 - iter 10/11 - loss 5.41075456 - samples/sec: 66.84\n",
      "2020-03-26 23:25:40,701 epoch 6 - iter 11/11 - loss 5.40676548 - samples/sec: 70.21\n",
      "2020-03-26 23:25:40,794 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:40,796 EPOCH 6 done: loss 5.4068 - lr 0.0500\n",
      "2020-03-26 23:25:41,287 DEV : loss 6.564648151397705 - score 0.0\n",
      "2020-03-26 23:25:41,296 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:25:45,458 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:45,938 epoch 7 - iter 1/11 - loss 4.98649025 - samples/sec: 67.12\n",
      "2020-03-26 23:25:46,775 epoch 7 - iter 2/11 - loss 6.97089672 - samples/sec: 43.71\n",
      "2020-03-26 23:25:47,552 epoch 7 - iter 3/11 - loss 6.31024377 - samples/sec: 47.82\n",
      "2020-03-26 23:25:48,712 epoch 7 - iter 4/11 - loss 6.54205704 - samples/sec: 31.03\n",
      "2020-03-26 23:25:49,592 epoch 7 - iter 5/11 - loss 6.23880339 - samples/sec: 42.72\n",
      "2020-03-26 23:25:50,314 epoch 7 - iter 6/11 - loss 5.96573559 - samples/sec: 51.25\n",
      "2020-03-26 23:25:51,086 epoch 7 - iter 7/11 - loss 5.58787220 - samples/sec: 47.12\n",
      "2020-03-26 23:25:51,789 epoch 7 - iter 8/11 - loss 5.29280075 - samples/sec: 53.48\n",
      "2020-03-26 23:25:52,429 epoch 7 - iter 9/11 - loss 5.51665250 - samples/sec: 58.23\n",
      "2020-03-26 23:25:53,084 epoch 7 - iter 10/11 - loss 5.36227300 - samples/sec: 59.09\n",
      "2020-03-26 23:25:53,608 epoch 7 - iter 11/11 - loss 5.20976567 - samples/sec: 74.97\n",
      "2020-03-26 23:25:53,705 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:53,707 EPOCH 7 done: loss 5.2098 - lr 0.0500\n",
      "2020-03-26 23:25:54,225 DEV : loss 6.944819450378418 - score 0.0\n",
      "2020-03-26 23:25:54,239 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:25:57,699 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:25:58,132 epoch 8 - iter 1/11 - loss 4.64063644 - samples/sec: 73.93\n",
      "2020-03-26 23:25:58,681 epoch 8 - iter 2/11 - loss 5.11620736 - samples/sec: 68.85\n",
      "2020-03-26 23:25:59,208 epoch 8 - iter 3/11 - loss 5.20498339 - samples/sec: 73.42\n",
      "2020-03-26 23:26:00,562 epoch 8 - iter 4/11 - loss 5.19191229 - samples/sec: 25.32\n",
      "2020-03-26 23:26:01,389 epoch 8 - iter 5/11 - loss 5.29718103 - samples/sec: 45.51\n",
      "2020-03-26 23:26:02,066 epoch 8 - iter 6/11 - loss 5.60914969 - samples/sec: 55.70\n",
      "2020-03-26 23:26:02,752 epoch 8 - iter 7/11 - loss 5.58671849 - samples/sec: 53.39\n",
      "2020-03-26 23:26:03,479 epoch 8 - iter 8/11 - loss 5.33734503 - samples/sec: 50.45\n",
      "2020-03-26 23:26:04,140 epoch 8 - iter 9/11 - loss 5.44709749 - samples/sec: 56.09\n",
      "2020-03-26 23:26:04,772 epoch 8 - iter 10/11 - loss 5.22540519 - samples/sec: 59.42\n",
      "2020-03-26 23:26:05,278 epoch 8 - iter 11/11 - loss 5.15416780 - samples/sec: 76.94\n",
      "2020-03-26 23:26:05,372 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:05,374 EPOCH 8 done: loss 5.1542 - lr 0.0500\n",
      "2020-03-26 23:26:06,085 DEV : loss 6.734771728515625 - score 0.0\n",
      "2020-03-26 23:26:06,100 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:26:09,528 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:09,998 epoch 9 - iter 1/11 - loss 5.94170856 - samples/sec: 68.56\n",
      "2020-03-26 23:26:10,521 epoch 9 - iter 2/11 - loss 5.25591946 - samples/sec: 73.59\n",
      "2020-03-26 23:26:11,371 epoch 9 - iter 3/11 - loss 5.73146470 - samples/sec: 42.11\n",
      "2020-03-26 23:26:12,148 epoch 9 - iter 4/11 - loss 5.40218759 - samples/sec: 47.39\n",
      "2020-03-26 23:26:12,705 epoch 9 - iter 5/11 - loss 5.40074978 - samples/sec: 69.60\n",
      "2020-03-26 23:26:13,397 epoch 9 - iter 6/11 - loss 5.52839168 - samples/sec: 53.03\n",
      "2020-03-26 23:26:14,018 epoch 9 - iter 7/11 - loss 5.31644705 - samples/sec: 59.97\n",
      "2020-03-26 23:26:14,618 epoch 9 - iter 8/11 - loss 5.20587176 - samples/sec: 63.04\n",
      "2020-03-26 23:26:15,240 epoch 9 - iter 9/11 - loss 5.05637428 - samples/sec: 60.09\n",
      "2020-03-26 23:26:15,753 epoch 9 - iter 10/11 - loss 5.02152982 - samples/sec: 75.50\n",
      "2020-03-26 23:26:16,398 epoch 9 - iter 11/11 - loss 5.02051631 - samples/sec: 57.60\n",
      "2020-03-26 23:26:16,525 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:16,527 EPOCH 9 done: loss 5.0205 - lr 0.0500\n",
      "2020-03-26 23:26:17,399 DEV : loss 6.723943710327148 - score 0.0\n",
      "Epoch     8: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2020-03-26 23:26:17,415 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:26:21,932 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:22,590 epoch 10 - iter 1/11 - loss 3.09871101 - samples/sec: 48.76\n",
      "2020-03-26 23:26:23,578 epoch 10 - iter 2/11 - loss 5.30206299 - samples/sec: 37.14\n",
      "2020-03-26 23:26:24,248 epoch 10 - iter 3/11 - loss 4.69703054 - samples/sec: 58.34\n",
      "2020-03-26 23:26:24,953 epoch 10 - iter 4/11 - loss 4.73894918 - samples/sec: 53.56\n",
      "2020-03-26 23:26:25,711 epoch 10 - iter 5/11 - loss 4.49612393 - samples/sec: 49.67\n",
      "2020-03-26 23:26:26,344 epoch 10 - iter 6/11 - loss 4.57797221 - samples/sec: 61.82\n",
      "2020-03-26 23:26:26,948 epoch 10 - iter 7/11 - loss 4.86734823 - samples/sec: 66.29\n",
      "2020-03-26 23:26:27,665 epoch 10 - iter 8/11 - loss 4.84639356 - samples/sec: 52.17\n",
      "2020-03-26 23:26:28,265 epoch 10 - iter 9/11 - loss 4.74784538 - samples/sec: 62.79\n",
      "2020-03-26 23:26:28,978 epoch 10 - iter 10/11 - loss 4.91002016 - samples/sec: 51.75\n",
      "2020-03-26 23:26:29,485 epoch 10 - iter 11/11 - loss 5.19351795 - samples/sec: 76.21\n",
      "2020-03-26 23:26:29,578 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:29,580 EPOCH 10 done: loss 5.1935 - lr 0.0250\n",
      "2020-03-26 23:26:30,033 DEV : loss 6.304490089416504 - score 0.0\n",
      "2020-03-26 23:26:30,040 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:26:33,927 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:34,802 epoch 11 - iter 1/11 - loss 3.23838902 - samples/sec: 36.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 23:26:35,526 epoch 11 - iter 2/11 - loss 4.38022470 - samples/sec: 52.17\n",
      "2020-03-26 23:26:36,363 epoch 11 - iter 3/11 - loss 5.07503859 - samples/sec: 43.30\n",
      "2020-03-26 23:26:37,146 epoch 11 - iter 4/11 - loss 4.58468968 - samples/sec: 46.91\n",
      "2020-03-26 23:26:37,746 epoch 11 - iter 5/11 - loss 4.08287988 - samples/sec: 63.66\n",
      "2020-03-26 23:26:38,523 epoch 11 - iter 6/11 - loss 4.30796182 - samples/sec: 47.12\n",
      "2020-03-26 23:26:39,290 epoch 11 - iter 7/11 - loss 4.67478858 - samples/sec: 47.53\n",
      "2020-03-26 23:26:40,042 epoch 11 - iter 8/11 - loss 4.85528252 - samples/sec: 49.06\n",
      "2020-03-26 23:26:40,678 epoch 11 - iter 9/11 - loss 5.03357991 - samples/sec: 58.55\n",
      "2020-03-26 23:26:41,302 epoch 11 - iter 10/11 - loss 4.95337565 - samples/sec: 59.75\n",
      "2020-03-26 23:26:41,833 epoch 11 - iter 11/11 - loss 5.01884762 - samples/sec: 72.63\n",
      "2020-03-26 23:26:41,925 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:41,927 EPOCH 11 done: loss 5.0188 - lr 0.0250\n",
      "2020-03-26 23:26:42,407 DEV : loss 6.367506504058838 - score 0.0\n",
      "2020-03-26 23:26:42,413 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:26:45,875 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:46,344 epoch 12 - iter 1/11 - loss 4.21895218 - samples/sec: 68.71\n",
      "2020-03-26 23:26:46,897 epoch 12 - iter 2/11 - loss 3.93926215 - samples/sec: 68.56\n",
      "2020-03-26 23:26:47,486 epoch 12 - iter 3/11 - loss 5.25953182 - samples/sec: 64.17\n",
      "2020-03-26 23:26:48,188 epoch 12 - iter 4/11 - loss 5.04582846 - samples/sec: 52.60\n",
      "2020-03-26 23:26:48,915 epoch 12 - iter 5/11 - loss 4.92599230 - samples/sec: 50.61\n",
      "2020-03-26 23:26:49,811 epoch 12 - iter 6/11 - loss 5.31409129 - samples/sec: 41.14\n",
      "2020-03-26 23:26:50,854 epoch 12 - iter 7/11 - loss 5.24530779 - samples/sec: 33.74\n",
      "2020-03-26 23:26:51,679 epoch 12 - iter 8/11 - loss 4.97482723 - samples/sec: 44.01\n",
      "2020-03-26 23:26:52,294 epoch 12 - iter 9/11 - loss 4.92676020 - samples/sec: 61.94\n",
      "2020-03-26 23:26:52,983 epoch 12 - iter 10/11 - loss 5.18324347 - samples/sec: 53.65\n",
      "2020-03-26 23:26:53,498 epoch 12 - iter 11/11 - loss 4.94786230 - samples/sec: 76.03\n",
      "2020-03-26 23:26:53,592 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:53,594 EPOCH 12 done: loss 4.9479 - lr 0.0250\n",
      "2020-03-26 23:26:54,140 DEV : loss 6.5182390213012695 - score 0.0\n",
      "2020-03-26 23:26:54,148 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:26:57,658 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:26:58,143 epoch 13 - iter 1/11 - loss 4.22757721 - samples/sec: 66.16\n",
      "2020-03-26 23:26:58,703 epoch 13 - iter 2/11 - loss 3.91338146 - samples/sec: 67.98\n",
      "2020-03-26 23:26:59,275 epoch 13 - iter 3/11 - loss 4.70656800 - samples/sec: 66.84\n",
      "2020-03-26 23:26:59,866 epoch 13 - iter 4/11 - loss 4.22330737 - samples/sec: 63.79\n",
      "2020-03-26 23:27:00,469 epoch 13 - iter 5/11 - loss 4.37235689 - samples/sec: 62.42\n",
      "2020-03-26 23:27:01,174 epoch 13 - iter 6/11 - loss 4.71639721 - samples/sec: 52.26\n",
      "2020-03-26 23:27:01,828 epoch 13 - iter 7/11 - loss 4.68275479 - samples/sec: 57.92\n",
      "2020-03-26 23:27:02,543 epoch 13 - iter 8/11 - loss 4.69095111 - samples/sec: 51.34\n",
      "2020-03-26 23:27:03,233 epoch 13 - iter 9/11 - loss 4.60214766 - samples/sec: 54.66\n",
      "2020-03-26 23:27:03,878 epoch 13 - iter 10/11 - loss 4.66713946 - samples/sec: 58.13\n",
      "2020-03-26 23:27:04,368 epoch 13 - iter 11/11 - loss 5.00804964 - samples/sec: 80.82\n",
      "2020-03-26 23:27:04,461 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:04,463 EPOCH 13 done: loss 5.0080 - lr 0.0250\n",
      "2020-03-26 23:27:04,981 DEV : loss 6.35585880279541 - score 0.0\n",
      "Epoch    12: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2020-03-26 23:27:04,992 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:27:08,942 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:09,344 epoch 14 - iter 1/11 - loss 3.93586183 - samples/sec: 80.01\n",
      "2020-03-26 23:27:09,955 epoch 14 - iter 2/11 - loss 3.88498974 - samples/sec: 61.70\n",
      "2020-03-26 23:27:10,581 epoch 14 - iter 3/11 - loss 4.13549948 - samples/sec: 60.09\n",
      "2020-03-26 23:27:11,224 epoch 14 - iter 4/11 - loss 4.20193303 - samples/sec: 58.13\n",
      "2020-03-26 23:27:11,879 epoch 14 - iter 5/11 - loss 4.22149372 - samples/sec: 56.79\n",
      "2020-03-26 23:27:12,604 epoch 14 - iter 6/11 - loss 4.33671943 - samples/sec: 50.61\n",
      "2020-03-26 23:27:13,237 epoch 14 - iter 7/11 - loss 4.50361817 - samples/sec: 60.09\n",
      "2020-03-26 23:27:13,858 epoch 14 - iter 8/11 - loss 4.82703495 - samples/sec: 60.88\n",
      "2020-03-26 23:27:14,451 epoch 14 - iter 9/11 - loss 4.86763144 - samples/sec: 63.41\n",
      "2020-03-26 23:27:15,020 epoch 14 - iter 10/11 - loss 4.94075241 - samples/sec: 66.98\n",
      "2020-03-26 23:27:15,493 epoch 14 - iter 11/11 - loss 4.68200350 - samples/sec: 81.64\n",
      "2020-03-26 23:27:15,586 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:15,588 EPOCH 14 done: loss 4.6820 - lr 0.0125\n",
      "2020-03-26 23:27:15,975 DEV : loss 6.374848365783691 - score 0.0\n",
      "2020-03-26 23:27:15,987 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:27:19,469 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:19,922 epoch 15 - iter 1/11 - loss 5.95858383 - samples/sec: 70.99\n",
      "2020-03-26 23:27:20,592 epoch 15 - iter 2/11 - loss 5.98572993 - samples/sec: 55.42\n",
      "2020-03-26 23:27:21,275 epoch 15 - iter 3/11 - loss 5.31551798 - samples/sec: 54.38\n",
      "2020-03-26 23:27:21,859 epoch 15 - iter 4/11 - loss 5.13053936 - samples/sec: 65.48\n",
      "2020-03-26 23:27:22,571 epoch 15 - iter 5/11 - loss 5.07341371 - samples/sec: 51.58\n",
      "2020-03-26 23:27:23,421 epoch 15 - iter 6/11 - loss 5.08974493 - samples/sec: 42.39\n",
      "2020-03-26 23:27:24,145 epoch 15 - iter 7/11 - loss 5.19805203 - samples/sec: 51.01\n",
      "2020-03-26 23:27:25,015 epoch 15 - iter 8/11 - loss 4.84733209 - samples/sec: 41.19\n",
      "2020-03-26 23:27:25,832 epoch 15 - iter 9/11 - loss 5.16672598 - samples/sec: 44.81\n",
      "2020-03-26 23:27:26,633 epoch 15 - iter 10/11 - loss 4.90618472 - samples/sec: 46.57\n",
      "2020-03-26 23:27:27,188 epoch 15 - iter 11/11 - loss 4.61839997 - samples/sec: 69.00\n",
      "2020-03-26 23:27:27,300 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:27,303 EPOCH 15 done: loss 4.6184 - lr 0.0125\n",
      "2020-03-26 23:27:27,975 DEV : loss 6.367483139038086 - score 0.0\n",
      "2020-03-26 23:27:27,982 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:27:32,271 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:32,895 epoch 16 - iter 1/11 - loss 4.06783676 - samples/sec: 51.42\n",
      "2020-03-26 23:27:33,534 epoch 16 - iter 2/11 - loss 5.08504629 - samples/sec: 58.66\n",
      "2020-03-26 23:27:34,273 epoch 16 - iter 3/11 - loss 4.28474649 - samples/sec: 49.82\n",
      "2020-03-26 23:27:34,932 epoch 16 - iter 4/11 - loss 4.09496194 - samples/sec: 57.09\n",
      "2020-03-26 23:27:35,581 epoch 16 - iter 5/11 - loss 4.43819985 - samples/sec: 57.19\n",
      "2020-03-26 23:27:36,427 epoch 16 - iter 6/11 - loss 4.19294961 - samples/sec: 43.59\n",
      "2020-03-26 23:27:37,095 epoch 16 - iter 7/11 - loss 4.76397160 - samples/sec: 56.29\n",
      "2020-03-26 23:27:37,690 epoch 16 - iter 8/11 - loss 4.57825807 - samples/sec: 63.29\n",
      "2020-03-26 23:27:38,276 epoch 16 - iter 9/11 - loss 4.70262525 - samples/sec: 64.82\n",
      "2020-03-26 23:27:38,931 epoch 16 - iter 10/11 - loss 4.90766542 - samples/sec: 56.49\n",
      "2020-03-26 23:27:39,644 epoch 16 - iter 11/11 - loss 4.75862180 - samples/sec: 51.83\n",
      "2020-03-26 23:27:39,742 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:39,744 EPOCH 16 done: loss 4.7586 - lr 0.0125\n",
      "2020-03-26 23:27:40,303 DEV : loss 6.27579927444458 - score 0.0\n",
      "2020-03-26 23:27:40,310 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:27:43,708 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:44,166 epoch 17 - iter 1/11 - loss 6.22350502 - samples/sec: 69.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 23:27:44,868 epoch 17 - iter 2/11 - loss 5.96031761 - samples/sec: 51.92\n",
      "2020-03-26 23:27:45,565 epoch 17 - iter 3/11 - loss 5.84162712 - samples/sec: 53.57\n",
      "2020-03-26 23:27:46,142 epoch 17 - iter 4/11 - loss 5.65354919 - samples/sec: 65.35\n",
      "2020-03-26 23:27:46,720 epoch 17 - iter 5/11 - loss 5.20449018 - samples/sec: 65.48\n",
      "2020-03-26 23:27:47,260 epoch 17 - iter 6/11 - loss 4.93186577 - samples/sec: 70.83\n",
      "2020-03-26 23:27:47,852 epoch 17 - iter 7/11 - loss 4.99751588 - samples/sec: 63.79\n",
      "2020-03-26 23:27:48,468 epoch 17 - iter 8/11 - loss 4.95588726 - samples/sec: 60.77\n",
      "2020-03-26 23:27:49,110 epoch 17 - iter 9/11 - loss 4.78222182 - samples/sec: 58.23\n",
      "2020-03-26 23:27:49,936 epoch 17 - iter 10/11 - loss 4.66078951 - samples/sec: 43.71\n",
      "2020-03-26 23:27:50,610 epoch 17 - iter 11/11 - loss 4.85121053 - samples/sec: 55.51\n",
      "2020-03-26 23:27:50,711 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:50,713 EPOCH 17 done: loss 4.8512 - lr 0.0125\n",
      "2020-03-26 23:27:51,357 DEV : loss 6.318920135498047 - score 0.0\n",
      "Epoch    16: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2020-03-26 23:27:51,370 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:27:54,660 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:27:55,141 epoch 18 - iter 1/11 - loss 6.33286381 - samples/sec: 66.71\n",
      "2020-03-26 23:27:55,757 epoch 18 - iter 2/11 - loss 6.38130045 - samples/sec: 61.12\n",
      "2020-03-26 23:27:56,584 epoch 18 - iter 3/11 - loss 6.21656783 - samples/sec: 43.71\n",
      "2020-03-26 23:27:57,277 epoch 18 - iter 4/11 - loss 5.92399848 - samples/sec: 54.20\n",
      "2020-03-26 23:27:57,936 epoch 18 - iter 5/11 - loss 6.05919561 - samples/sec: 56.69\n",
      "2020-03-26 23:27:58,562 epoch 18 - iter 6/11 - loss 5.70008659 - samples/sec: 60.42\n",
      "2020-03-26 23:27:59,120 epoch 18 - iter 7/11 - loss 5.38730482 - samples/sec: 69.00\n",
      "2020-03-26 23:27:59,659 epoch 18 - iter 8/11 - loss 5.10732332 - samples/sec: 72.59\n",
      "2020-03-26 23:28:00,173 epoch 18 - iter 9/11 - loss 4.83069452 - samples/sec: 76.03\n",
      "2020-03-26 23:28:00,772 epoch 18 - iter 10/11 - loss 4.94550238 - samples/sec: 63.54\n",
      "2020-03-26 23:28:01,274 epoch 18 - iter 11/11 - loss 4.82319073 - samples/sec: 77.69\n",
      "2020-03-26 23:28:01,392 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:28:01,394 EPOCH 18 done: loss 4.8232 - lr 0.0063\n",
      "2020-03-26 23:28:02,052 DEV : loss 6.301069259643555 - score 0.0\n",
      "2020-03-26 23:28:02,063 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:28:05,378 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:28:05,785 epoch 19 - iter 1/11 - loss 5.87249851 - samples/sec: 79.03\n",
      "2020-03-26 23:28:06,326 epoch 19 - iter 2/11 - loss 6.35082078 - samples/sec: 68.85\n",
      "2020-03-26 23:28:07,271 epoch 19 - iter 3/11 - loss 6.15066274 - samples/sec: 37.35\n",
      "2020-03-26 23:28:08,559 epoch 19 - iter 4/11 - loss 5.88029861 - samples/sec: 26.89\n",
      "2020-03-26 23:28:09,398 epoch 19 - iter 5/11 - loss 5.82066307 - samples/sec: 43.13\n",
      "2020-03-26 23:28:10,178 epoch 19 - iter 6/11 - loss 5.13896569 - samples/sec: 46.70\n",
      "2020-03-26 23:28:11,016 epoch 19 - iter 7/11 - loss 4.91094307 - samples/sec: 43.24\n",
      "2020-03-26 23:28:12,698 epoch 19 - iter 8/11 - loss 4.82415953 - samples/sec: 20.49\n",
      "2020-03-26 23:28:13,840 epoch 19 - iter 9/11 - loss 4.63465450 - samples/sec: 30.65\n",
      "2020-03-26 23:28:14,836 epoch 19 - iter 10/11 - loss 4.71465499 - samples/sec: 36.59\n",
      "2020-03-26 23:28:15,834 epoch 19 - iter 11/11 - loss 4.81899576 - samples/sec: 35.62\n",
      "2020-03-26 23:28:15,942 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:28:15,945 EPOCH 19 done: loss 4.8190 - lr 0.0063\n",
      "2020-03-26 23:28:16,933 DEV : loss 6.292349815368652 - score 0.0\n",
      "2020-03-26 23:28:16,948 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:28:24,120 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:28:25,348 epoch 20 - iter 1/11 - loss 4.24445248 - samples/sec: 26.13\n",
      "2020-03-26 23:28:26,339 epoch 20 - iter 2/11 - loss 3.43680358 - samples/sec: 39.32\n",
      "2020-03-26 23:28:27,292 epoch 20 - iter 3/11 - loss 3.82774591 - samples/sec: 37.53\n",
      "2020-03-26 23:28:28,149 epoch 20 - iter 4/11 - loss 4.04621923 - samples/sec: 42.11\n",
      "2020-03-26 23:28:28,884 epoch 20 - iter 5/11 - loss 4.26517782 - samples/sec: 50.06\n",
      "2020-03-26 23:28:29,792 epoch 20 - iter 6/11 - loss 4.44189223 - samples/sec: 38.99\n",
      "2020-03-26 23:28:30,841 epoch 20 - iter 7/11 - loss 4.75554255 - samples/sec: 33.42\n",
      "2020-03-26 23:28:31,678 epoch 20 - iter 8/11 - loss 4.63178250 - samples/sec: 42.90\n",
      "2020-03-26 23:28:32,411 epoch 20 - iter 9/11 - loss 4.90635175 - samples/sec: 49.67\n",
      "2020-03-26 23:28:32,987 epoch 20 - iter 10/11 - loss 4.84382188 - samples/sec: 64.56\n",
      "2020-03-26 23:28:33,412 epoch 20 - iter 11/11 - loss 4.65757721 - samples/sec: 93.82\n",
      "2020-03-26 23:28:33,503 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:28:33,505 EPOCH 20 done: loss 4.6576 - lr 0.0063\n",
      "2020-03-26 23:28:34,079 DEV : loss 6.251804828643799 - score 0.0\n",
      "2020-03-26 23:28:34,089 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:28:38,014 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:28:38,457 epoch 21 - iter 1/11 - loss 6.73172855 - samples/sec: 72.43\n",
      "2020-03-26 23:28:39,104 epoch 21 - iter 2/11 - loss 5.94015503 - samples/sec: 57.40\n",
      "2020-03-26 23:28:39,676 epoch 21 - iter 3/11 - loss 5.04542287 - samples/sec: 66.43\n",
      "2020-03-26 23:28:40,342 epoch 21 - iter 4/11 - loss 5.38248360 - samples/sec: 54.85\n",
      "2020-03-26 23:28:41,042 epoch 21 - iter 5/11 - loss 5.58304968 - samples/sec: 52.60\n",
      "2020-03-26 23:28:41,742 epoch 21 - iter 6/11 - loss 4.98402435 - samples/sec: 52.17\n",
      "2020-03-26 23:28:42,344 epoch 21 - iter 7/11 - loss 4.63892281 - samples/sec: 62.67\n",
      "2020-03-26 23:28:43,052 epoch 21 - iter 8/11 - loss 4.77145867 - samples/sec: 51.58\n",
      "2020-03-26 23:28:43,743 epoch 21 - iter 9/11 - loss 4.60244437 - samples/sec: 53.48\n",
      "2020-03-26 23:28:44,413 epoch 21 - iter 10/11 - loss 4.64363679 - samples/sec: 54.94\n",
      "2020-03-26 23:28:44,828 epoch 21 - iter 11/11 - loss 4.73327471 - samples/sec: 98.72\n",
      "2020-03-26 23:28:44,919 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:28:44,921 EPOCH 21 done: loss 4.7333 - lr 0.0063\n",
      "2020-03-26 23:28:45,462 DEV : loss 6.265791893005371 - score 0.0\n",
      "Epoch    20: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2020-03-26 23:28:45,475 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:28:49,191 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:28:49,619 epoch 22 - iter 1/11 - loss 6.05149174 - samples/sec: 75.14\n",
      "2020-03-26 23:28:50,279 epoch 22 - iter 2/11 - loss 5.47876692 - samples/sec: 55.70\n",
      "2020-03-26 23:28:51,032 epoch 22 - iter 3/11 - loss 4.50203371 - samples/sec: 48.54\n",
      "2020-03-26 23:28:51,711 epoch 22 - iter 4/11 - loss 4.86044455 - samples/sec: 54.75\n",
      "2020-03-26 23:28:52,453 epoch 22 - iter 5/11 - loss 4.69134092 - samples/sec: 48.84\n",
      "2020-03-26 23:28:53,022 epoch 22 - iter 6/11 - loss 4.98305480 - samples/sec: 66.98\n",
      "2020-03-26 23:28:53,696 epoch 22 - iter 7/11 - loss 5.00930732 - samples/sec: 54.94\n",
      "2020-03-26 23:28:54,339 epoch 22 - iter 8/11 - loss 4.95235598 - samples/sec: 57.40\n",
      "2020-03-26 23:28:54,884 epoch 22 - iter 9/11 - loss 4.62005548 - samples/sec: 70.06\n",
      "2020-03-26 23:28:55,477 epoch 22 - iter 10/11 - loss 4.77881743 - samples/sec: 63.54\n",
      "2020-03-26 23:28:55,939 epoch 22 - iter 11/11 - loss 4.69177702 - samples/sec: 85.56\n",
      "2020-03-26 23:28:56,022 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:28:56,024 EPOCH 22 done: loss 4.6918 - lr 0.0031\n",
      "2020-03-26 23:28:56,587 DEV : loss 6.257781505584717 - score 0.0\n",
      "2020-03-26 23:28:56,600 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:29:00,159 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 23:29:00,707 epoch 23 - iter 1/11 - loss 4.41720009 - samples/sec: 58.44\n",
      "2020-03-26 23:29:01,370 epoch 23 - iter 2/11 - loss 4.76208043 - samples/sec: 56.79\n",
      "2020-03-26 23:29:02,051 epoch 23 - iter 3/11 - loss 5.32447704 - samples/sec: 53.65\n",
      "2020-03-26 23:29:02,637 epoch 23 - iter 4/11 - loss 5.36186349 - samples/sec: 64.69\n",
      "2020-03-26 23:29:03,303 epoch 23 - iter 5/11 - loss 5.42072172 - samples/sec: 55.42\n",
      "2020-03-26 23:29:03,952 epoch 23 - iter 6/11 - loss 5.11152105 - samples/sec: 57.19\n",
      "2020-03-26 23:29:04,526 epoch 23 - iter 7/11 - loss 5.33630831 - samples/sec: 65.48\n",
      "2020-03-26 23:29:05,156 epoch 23 - iter 8/11 - loss 5.13060927 - samples/sec: 58.76\n",
      "2020-03-26 23:29:05,846 epoch 23 - iter 9/11 - loss 4.81907543 - samples/sec: 53.12\n",
      "2020-03-26 23:29:06,535 epoch 23 - iter 10/11 - loss 4.61036334 - samples/sec: 53.83\n",
      "2020-03-26 23:29:06,933 epoch 23 - iter 11/11 - loss 4.71408289 - samples/sec: 102.51\n",
      "2020-03-26 23:29:07,023 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:07,024 EPOCH 23 done: loss 4.7141 - lr 0.0031\n",
      "2020-03-26 23:29:07,562 DEV : loss 6.244278430938721 - score 0.0\n",
      "2020-03-26 23:29:07,575 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:29:11,144 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:11,595 epoch 24 - iter 1/11 - loss 7.24926329 - samples/sec: 71.14\n",
      "2020-03-26 23:29:12,186 epoch 24 - iter 2/11 - loss 5.68847013 - samples/sec: 63.41\n",
      "2020-03-26 23:29:12,846 epoch 24 - iter 3/11 - loss 5.72048744 - samples/sec: 55.90\n",
      "2020-03-26 23:29:13,484 epoch 24 - iter 4/11 - loss 5.51400661 - samples/sec: 58.34\n",
      "2020-03-26 23:29:14,196 epoch 24 - iter 5/11 - loss 5.24252367 - samples/sec: 51.17\n",
      "2020-03-26 23:29:14,991 epoch 24 - iter 6/11 - loss 4.75284119 - samples/sec: 45.64\n",
      "2020-03-26 23:29:15,675 epoch 24 - iter 7/11 - loss 4.78576248 - samples/sec: 53.74\n",
      "2020-03-26 23:29:16,364 epoch 24 - iter 8/11 - loss 4.65606135 - samples/sec: 53.48\n",
      "2020-03-26 23:29:16,995 epoch 24 - iter 9/11 - loss 4.95432366 - samples/sec: 59.31\n",
      "2020-03-26 23:29:17,618 epoch 24 - iter 10/11 - loss 4.78191035 - samples/sec: 59.86\n",
      "2020-03-26 23:29:18,133 epoch 24 - iter 11/11 - loss 4.77879297 - samples/sec: 75.85\n",
      "2020-03-26 23:29:18,222 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:18,224 EPOCH 24 done: loss 4.7788 - lr 0.0031\n",
      "2020-03-26 23:29:18,698 DEV : loss 6.250983715057373 - score 0.0\n",
      "2020-03-26 23:29:18,711 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:29:22,580 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:23,040 epoch 25 - iter 1/11 - loss 5.44573116 - samples/sec: 69.90\n",
      "2020-03-26 23:29:23,653 epoch 25 - iter 2/11 - loss 4.33054876 - samples/sec: 60.54\n",
      "2020-03-26 23:29:24,300 epoch 25 - iter 3/11 - loss 5.46613614 - samples/sec: 57.60\n",
      "2020-03-26 23:29:25,019 epoch 25 - iter 4/11 - loss 4.94250846 - samples/sec: 50.85\n",
      "2020-03-26 23:29:25,617 epoch 25 - iter 5/11 - loss 4.65907221 - samples/sec: 63.41\n",
      "2020-03-26 23:29:26,218 epoch 25 - iter 6/11 - loss 4.49485695 - samples/sec: 61.94\n",
      "2020-03-26 23:29:26,849 epoch 25 - iter 7/11 - loss 4.88781762 - samples/sec: 58.76\n",
      "2020-03-26 23:29:27,447 epoch 25 - iter 8/11 - loss 4.91271481 - samples/sec: 62.30\n",
      "2020-03-26 23:29:28,125 epoch 25 - iter 9/11 - loss 4.61969921 - samples/sec: 53.93\n",
      "2020-03-26 23:29:28,676 epoch 25 - iter 10/11 - loss 4.57418790 - samples/sec: 68.71\n",
      "2020-03-26 23:29:29,203 epoch 25 - iter 11/11 - loss 4.85141451 - samples/sec: 72.76\n",
      "2020-03-26 23:29:29,293 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:29,295 EPOCH 25 done: loss 4.8514 - lr 0.0031\n",
      "2020-03-26 23:29:29,871 DEV : loss 6.235997200012207 - score 0.0\n",
      "Epoch    24: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2020-03-26 23:29:29,877 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:29:34,269 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:34,896 epoch 26 - iter 1/11 - loss 3.74782157 - samples/sec: 51.25\n",
      "2020-03-26 23:29:35,702 epoch 26 - iter 2/11 - loss 4.21253288 - samples/sec: 44.50\n",
      "2020-03-26 23:29:36,322 epoch 26 - iter 3/11 - loss 4.61616000 - samples/sec: 60.42\n",
      "2020-03-26 23:29:36,997 epoch 26 - iter 4/11 - loss 4.49948424 - samples/sec: 54.85\n",
      "2020-03-26 23:29:37,675 epoch 26 - iter 5/11 - loss 4.31329145 - samples/sec: 54.11\n",
      "2020-03-26 23:29:38,287 epoch 26 - iter 6/11 - loss 4.66680662 - samples/sec: 62.06\n",
      "2020-03-26 23:29:38,934 epoch 26 - iter 7/11 - loss 4.92148713 - samples/sec: 56.99\n",
      "2020-03-26 23:29:39,575 epoch 26 - iter 8/11 - loss 4.57135612 - samples/sec: 58.02\n",
      "2020-03-26 23:29:40,190 epoch 26 - iter 9/11 - loss 4.48094405 - samples/sec: 60.42\n",
      "2020-03-26 23:29:40,853 epoch 26 - iter 10/11 - loss 4.69199195 - samples/sec: 56.00\n",
      "2020-03-26 23:29:41,425 epoch 26 - iter 11/11 - loss 4.77082838 - samples/sec: 66.57\n",
      "2020-03-26 23:29:41,514 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:41,516 EPOCH 26 done: loss 4.7708 - lr 0.0016\n",
      "2020-03-26 23:29:41,964 DEV : loss 6.235550880432129 - score 0.0\n",
      "2020-03-26 23:29:41,972 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:29:45,316 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:45,711 epoch 27 - iter 1/11 - loss 3.65485859 - samples/sec: 81.23\n",
      "2020-03-26 23:29:46,352 epoch 27 - iter 2/11 - loss 4.94066596 - samples/sec: 57.50\n",
      "2020-03-26 23:29:47,312 epoch 27 - iter 3/11 - loss 5.02925571 - samples/sec: 37.66\n",
      "2020-03-26 23:29:48,091 epoch 27 - iter 4/11 - loss 4.57695323 - samples/sec: 47.25\n",
      "2020-03-26 23:29:48,903 epoch 27 - iter 5/11 - loss 4.79974675 - samples/sec: 44.50\n",
      "2020-03-26 23:29:49,692 epoch 27 - iter 6/11 - loss 4.48874422 - samples/sec: 45.77\n",
      "2020-03-26 23:29:50,345 epoch 27 - iter 7/11 - loss 4.32780286 - samples/sec: 57.30\n",
      "2020-03-26 23:29:51,222 epoch 27 - iter 8/11 - loss 4.38100427 - samples/sec: 41.03\n",
      "2020-03-26 23:29:51,987 epoch 27 - iter 9/11 - loss 4.63304456 - samples/sec: 47.53\n",
      "2020-03-26 23:29:52,856 epoch 27 - iter 10/11 - loss 4.68598032 - samples/sec: 41.40\n",
      "2020-03-26 23:29:53,320 epoch 27 - iter 11/11 - loss 4.70264964 - samples/sec: 85.33\n",
      "2020-03-26 23:29:53,411 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:53,413 EPOCH 27 done: loss 4.7026 - lr 0.0016\n",
      "2020-03-26 23:29:53,866 DEV : loss 6.229235649108887 - score 0.0\n",
      "2020-03-26 23:29:53,872 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:29:57,627 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:29:58,163 epoch 28 - iter 1/11 - loss 6.06903219 - samples/sec: 60.09\n",
      "2020-03-26 23:29:58,794 epoch 28 - iter 2/11 - loss 7.03438067 - samples/sec: 59.64\n",
      "2020-03-26 23:29:59,436 epoch 28 - iter 3/11 - loss 5.96683494 - samples/sec: 57.50\n",
      "2020-03-26 23:30:00,031 epoch 28 - iter 4/11 - loss 5.72365898 - samples/sec: 63.54\n",
      "2020-03-26 23:30:00,771 epoch 28 - iter 5/11 - loss 5.32080908 - samples/sec: 49.06\n",
      "2020-03-26 23:30:01,400 epoch 28 - iter 6/11 - loss 4.96437506 - samples/sec: 59.53\n",
      "2020-03-26 23:30:02,069 epoch 28 - iter 7/11 - loss 5.11927887 - samples/sec: 55.22\n",
      "2020-03-26 23:30:02,705 epoch 28 - iter 8/11 - loss 4.73826477 - samples/sec: 59.20\n",
      "2020-03-26 23:30:03,461 epoch 28 - iter 9/11 - loss 4.97541494 - samples/sec: 48.25\n",
      "2020-03-26 23:30:04,153 epoch 28 - iter 10/11 - loss 4.73830822 - samples/sec: 53.65\n",
      "2020-03-26 23:30:04,611 epoch 28 - iter 11/11 - loss 4.79891307 - samples/sec: 87.43\n",
      "2020-03-26 23:30:04,698 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:30:04,701 EPOCH 28 done: loss 4.7989 - lr 0.0016\n",
      "2020-03-26 23:30:05,257 DEV : loss 6.218147277832031 - score 0.0\n",
      "2020-03-26 23:30:05,270 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:30:09,201 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 23:30:09,674 epoch 29 - iter 1/11 - loss 3.06639290 - samples/sec: 67.98\n",
      "2020-03-26 23:30:10,299 epoch 29 - iter 2/11 - loss 3.69753551 - samples/sec: 59.09\n",
      "2020-03-26 23:30:10,933 epoch 29 - iter 3/11 - loss 4.16075579 - samples/sec: 59.20\n",
      "2020-03-26 23:30:11,518 epoch 29 - iter 4/11 - loss 3.63665193 - samples/sec: 64.56\n",
      "2020-03-26 23:30:12,139 epoch 29 - iter 5/11 - loss 4.37444320 - samples/sec: 59.86\n",
      "2020-03-26 23:30:12,827 epoch 29 - iter 6/11 - loss 4.52557512 - samples/sec: 53.12\n",
      "2020-03-26 23:30:13,476 epoch 29 - iter 7/11 - loss 4.39445301 - samples/sec: 57.30\n",
      "2020-03-26 23:30:14,141 epoch 29 - iter 8/11 - loss 4.45169255 - samples/sec: 55.70\n",
      "2020-03-26 23:30:14,812 epoch 29 - iter 9/11 - loss 4.68458300 - samples/sec: 55.70\n",
      "2020-03-26 23:30:15,537 epoch 29 - iter 10/11 - loss 4.77451117 - samples/sec: 50.85\n",
      "2020-03-26 23:30:16,085 epoch 29 - iter 11/11 - loss 4.64633879 - samples/sec: 69.45\n",
      "2020-03-26 23:30:16,171 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:30:16,174 EPOCH 29 done: loss 4.6463 - lr 0.0016\n",
      "2020-03-26 23:30:16,596 DEV : loss 6.211106300354004 - score 0.0\n",
      "Epoch    28: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2020-03-26 23:30:16,608 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:30:21,256 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:30:21,876 epoch 30 - iter 1/11 - loss 5.66438198 - samples/sec: 51.92\n",
      "2020-03-26 23:30:22,515 epoch 30 - iter 2/11 - loss 4.63602555 - samples/sec: 60.09\n",
      "2020-03-26 23:30:23,152 epoch 30 - iter 3/11 - loss 4.80392480 - samples/sec: 58.98\n",
      "2020-03-26 23:30:23,885 epoch 30 - iter 4/11 - loss 4.96517342 - samples/sec: 49.29\n",
      "2020-03-26 23:30:24,531 epoch 30 - iter 5/11 - loss 4.89183640 - samples/sec: 57.92\n",
      "2020-03-26 23:30:25,142 epoch 30 - iter 6/11 - loss 4.90136405 - samples/sec: 61.35\n",
      "2020-03-26 23:30:25,740 epoch 30 - iter 7/11 - loss 4.44396479 - samples/sec: 63.04\n",
      "2020-03-26 23:30:26,385 epoch 30 - iter 8/11 - loss 4.25102991 - samples/sec: 61.94\n",
      "2020-03-26 23:30:27,027 epoch 30 - iter 9/11 - loss 4.41313569 - samples/sec: 57.81\n",
      "2020-03-26 23:30:27,625 epoch 30 - iter 10/11 - loss 4.73234868 - samples/sec: 62.18\n",
      "2020-03-26 23:30:28,152 epoch 30 - iter 11/11 - loss 4.81856875 - samples/sec: 72.76\n",
      "2020-03-26 23:30:28,238 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:30:28,240 EPOCH 30 done: loss 4.8186 - lr 0.0008\n",
      "2020-03-26 23:30:28,682 DEV : loss 6.212576866149902 - score 0.0\n",
      "2020-03-26 23:30:28,689 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:30:32,309 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:30:32,727 epoch 31 - iter 1/11 - loss 3.61392832 - samples/sec: 76.94\n",
      "2020-03-26 23:30:33,353 epoch 31 - iter 2/11 - loss 4.42050409 - samples/sec: 58.98\n",
      "2020-03-26 23:30:33,991 epoch 31 - iter 3/11 - loss 4.98633846 - samples/sec: 58.76\n",
      "2020-03-26 23:30:34,828 epoch 31 - iter 4/11 - loss 4.69873476 - samples/sec: 42.39\n",
      "2020-03-26 23:30:35,631 epoch 31 - iter 5/11 - loss 4.84330997 - samples/sec: 45.32\n",
      "2020-03-26 23:30:36,415 epoch 31 - iter 6/11 - loss 4.69481452 - samples/sec: 46.37\n",
      "2020-03-26 23:30:37,215 epoch 31 - iter 7/11 - loss 4.34072590 - samples/sec: 45.38\n",
      "2020-03-26 23:30:37,921 epoch 31 - iter 8/11 - loss 4.29780507 - samples/sec: 52.00\n",
      "2020-03-26 23:30:38,576 epoch 31 - iter 9/11 - loss 4.41829946 - samples/sec: 56.99\n",
      "2020-03-26 23:30:39,295 epoch 31 - iter 10/11 - loss 4.70274725 - samples/sec: 51.01\n",
      "2020-03-26 23:30:39,675 epoch 31 - iter 11/11 - loss 4.55134151 - samples/sec: 109.51\n",
      "2020-03-26 23:30:39,762 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:30:39,764 EPOCH 31 done: loss 4.5513 - lr 0.0008\n",
      "2020-03-26 23:30:40,344 DEV : loss 6.217165946960449 - score 0.0\n",
      "2020-03-26 23:30:40,355 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:30:44,282 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:30:44,829 epoch 32 - iter 1/11 - loss 6.28506231 - samples/sec: 58.76\n",
      "2020-03-26 23:30:45,453 epoch 32 - iter 2/11 - loss 5.03184092 - samples/sec: 59.53\n",
      "2020-03-26 23:30:46,087 epoch 32 - iter 3/11 - loss 5.32064541 - samples/sec: 58.87\n",
      "2020-03-26 23:30:46,720 epoch 32 - iter 4/11 - loss 5.11205310 - samples/sec: 58.98\n",
      "2020-03-26 23:30:47,322 epoch 32 - iter 5/11 - loss 4.78933930 - samples/sec: 62.54\n",
      "2020-03-26 23:30:47,870 epoch 32 - iter 6/11 - loss 4.57742647 - samples/sec: 68.71\n",
      "2020-03-26 23:30:48,520 epoch 32 - iter 7/11 - loss 4.47545467 - samples/sec: 56.29\n",
      "2020-03-26 23:30:49,180 epoch 32 - iter 8/11 - loss 4.61892527 - samples/sec: 56.00\n",
      "2020-03-26 23:30:49,876 epoch 32 - iter 9/11 - loss 4.43556791 - samples/sec: 52.86\n",
      "2020-03-26 23:30:50,669 epoch 32 - iter 10/11 - loss 4.48237233 - samples/sec: 45.90\n",
      "2020-03-26 23:30:51,582 epoch 32 - iter 11/11 - loss 4.97899875 - samples/sec: 39.37\n",
      "2020-03-26 23:30:51,680 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:30:51,683 EPOCH 32 done: loss 4.9790 - lr 0.0008\n",
      "2020-03-26 23:30:52,818 DEV : loss 6.217624664306641 - score 0.0\n",
      "2020-03-26 23:30:52,829 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:30:56,989 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:30:57,625 epoch 33 - iter 1/11 - loss 3.04232883 - samples/sec: 50.53\n",
      "2020-03-26 23:30:58,530 epoch 33 - iter 2/11 - loss 2.95197654 - samples/sec: 39.13\n",
      "2020-03-26 23:30:59,399 epoch 33 - iter 3/11 - loss 4.01589123 - samples/sec: 41.19\n",
      "2020-03-26 23:31:00,210 epoch 33 - iter 4/11 - loss 3.99670535 - samples/sec: 45.06\n",
      "2020-03-26 23:31:01,013 epoch 33 - iter 5/11 - loss 4.70022988 - samples/sec: 45.25\n",
      "2020-03-26 23:31:02,267 epoch 33 - iter 6/11 - loss 4.97108956 - samples/sec: 27.68\n",
      "2020-03-26 23:31:03,092 epoch 33 - iter 7/11 - loss 5.37303492 - samples/sec: 44.01\n",
      "2020-03-26 23:31:04,214 epoch 33 - iter 8/11 - loss 5.15834650 - samples/sec: 31.43\n",
      "2020-03-26 23:31:04,993 epoch 33 - iter 9/11 - loss 4.87878821 - samples/sec: 46.52\n",
      "2020-03-26 23:31:06,506 epoch 33 - iter 10/11 - loss 4.74194098 - samples/sec: 22.76\n",
      "2020-03-26 23:31:07,073 epoch 33 - iter 11/11 - loss 4.71947002 - samples/sec: 69.12\n",
      "2020-03-26 23:31:07,164 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:31:07,166 EPOCH 33 done: loss 4.7195 - lr 0.0008\n",
      "2020-03-26 23:31:07,803 DEV : loss 6.223827362060547 - score 0.0\n",
      "Epoch    32: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2020-03-26 23:31:07,816 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:31:12,252 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:31:12,688 epoch 34 - iter 1/11 - loss 3.93636107 - samples/sec: 73.76\n",
      "2020-03-26 23:31:13,385 epoch 34 - iter 2/11 - loss 4.60586560 - samples/sec: 52.51\n",
      "2020-03-26 23:31:14,177 epoch 34 - iter 3/11 - loss 4.80980976 - samples/sec: 45.77\n",
      "2020-03-26 23:31:14,858 epoch 34 - iter 4/11 - loss 4.46325290 - samples/sec: 53.65\n",
      "2020-03-26 23:31:15,508 epoch 34 - iter 5/11 - loss 4.95332603 - samples/sec: 57.09\n",
      "2020-03-26 23:31:16,226 epoch 34 - iter 6/11 - loss 4.98411862 - samples/sec: 51.25\n",
      "2020-03-26 23:31:16,844 epoch 34 - iter 7/11 - loss 5.16540139 - samples/sec: 60.65\n",
      "2020-03-26 23:31:17,482 epoch 34 - iter 8/11 - loss 4.97522295 - samples/sec: 57.71\n",
      "2020-03-26 23:31:18,055 epoch 34 - iter 9/11 - loss 4.75014636 - samples/sec: 66.16\n",
      "2020-03-26 23:31:18,673 epoch 34 - iter 10/11 - loss 4.82213614 - samples/sec: 60.20\n",
      "2020-03-26 23:31:19,171 epoch 34 - iter 11/11 - loss 4.60444197 - samples/sec: 77.69\n",
      "2020-03-26 23:31:19,268 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:31:19,270 EPOCH 34 done: loss 4.6044 - lr 0.0004\n",
      "2020-03-26 23:31:19,944 DEV : loss 6.225837707519531 - score 0.0\n",
      "2020-03-26 23:31:19,961 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:31:24,359 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 23:31:25,069 epoch 35 - iter 1/11 - loss 2.90722179 - samples/sec: 45.25\n",
      "2020-03-26 23:31:25,818 epoch 35 - iter 2/11 - loss 3.51498508 - samples/sec: 49.29\n",
      "2020-03-26 23:31:26,508 epoch 35 - iter 3/11 - loss 4.17668851 - samples/sec: 53.57\n",
      "2020-03-26 23:31:27,195 epoch 35 - iter 4/11 - loss 4.49413133 - samples/sec: 54.11\n",
      "2020-03-26 23:31:27,848 epoch 35 - iter 5/11 - loss 4.72742071 - samples/sec: 56.89\n",
      "2020-03-26 23:31:28,571 epoch 35 - iter 6/11 - loss 4.77602228 - samples/sec: 50.45\n",
      "2020-03-26 23:31:29,304 epoch 35 - iter 7/11 - loss 5.12288386 - samples/sec: 49.59\n",
      "2020-03-26 23:31:29,982 epoch 35 - iter 8/11 - loss 4.77089012 - samples/sec: 54.02\n",
      "2020-03-26 23:31:30,654 epoch 35 - iter 9/11 - loss 4.86593676 - samples/sec: 54.85\n",
      "2020-03-26 23:31:31,303 epoch 35 - iter 10/11 - loss 4.76158216 - samples/sec: 57.50\n",
      "2020-03-26 23:31:31,802 epoch 35 - iter 11/11 - loss 4.59224051 - samples/sec: 79.42\n",
      "2020-03-26 23:31:31,895 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:31:31,898 EPOCH 35 done: loss 4.5922 - lr 0.0004\n",
      "2020-03-26 23:31:32,360 DEV : loss 6.227533340454102 - score 0.0\n",
      "2020-03-26 23:31:32,370 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:31:36,079 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:31:36,514 epoch 36 - iter 1/11 - loss 4.82562351 - samples/sec: 73.93\n",
      "2020-03-26 23:31:37,219 epoch 36 - iter 2/11 - loss 5.62351489 - samples/sec: 52.09\n",
      "2020-03-26 23:31:37,832 epoch 36 - iter 3/11 - loss 5.31443008 - samples/sec: 60.77\n",
      "2020-03-26 23:31:38,465 epoch 36 - iter 4/11 - loss 5.45118225 - samples/sec: 59.20\n",
      "2020-03-26 23:31:39,203 epoch 36 - iter 5/11 - loss 5.19151888 - samples/sec: 49.29\n",
      "2020-03-26 23:31:40,002 epoch 36 - iter 6/11 - loss 5.55343517 - samples/sec: 45.45\n",
      "2020-03-26 23:31:40,864 epoch 36 - iter 7/11 - loss 5.50936529 - samples/sec: 42.00\n",
      "2020-03-26 23:31:41,759 epoch 36 - iter 8/11 - loss 5.24834698 - samples/sec: 40.26\n",
      "2020-03-26 23:31:42,655 epoch 36 - iter 9/11 - loss 4.91146607 - samples/sec: 40.11\n",
      "2020-03-26 23:31:43,312 epoch 36 - iter 10/11 - loss 4.76459503 - samples/sec: 56.79\n",
      "2020-03-26 23:31:43,762 epoch 36 - iter 11/11 - loss 4.56308022 - samples/sec: 90.13\n",
      "2020-03-26 23:31:43,874 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:31:43,876 EPOCH 36 done: loss 4.5631 - lr 0.0004\n",
      "2020-03-26 23:31:44,354 DEV : loss 6.226059913635254 - score 0.0\n",
      "2020-03-26 23:31:44,361 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:31:47,891 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:31:48,444 epoch 37 - iter 1/11 - loss 2.45277405 - samples/sec: 57.92\n",
      "2020-03-26 23:31:49,016 epoch 37 - iter 2/11 - loss 3.23051715 - samples/sec: 67.12\n",
      "2020-03-26 23:31:49,639 epoch 37 - iter 3/11 - loss 4.22313833 - samples/sec: 59.53\n",
      "2020-03-26 23:31:50,294 epoch 37 - iter 4/11 - loss 4.82343960 - samples/sec: 56.69\n",
      "2020-03-26 23:31:50,977 epoch 37 - iter 5/11 - loss 4.67986088 - samples/sec: 54.11\n",
      "2020-03-26 23:31:51,766 epoch 37 - iter 6/11 - loss 4.58512743 - samples/sec: 45.97\n",
      "2020-03-26 23:31:52,417 epoch 37 - iter 7/11 - loss 4.86984001 - samples/sec: 57.19\n",
      "2020-03-26 23:31:53,022 epoch 37 - iter 8/11 - loss 4.76277679 - samples/sec: 61.70\n",
      "2020-03-26 23:31:53,697 epoch 37 - iter 9/11 - loss 4.70297681 - samples/sec: 56.39\n",
      "2020-03-26 23:31:54,410 epoch 37 - iter 10/11 - loss 4.60003738 - samples/sec: 51.50\n",
      "2020-03-26 23:31:54,887 epoch 37 - iter 11/11 - loss 4.67478028 - samples/sec: 82.27\n",
      "2020-03-26 23:31:54,980 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:31:54,983 EPOCH 37 done: loss 4.6748 - lr 0.0004\n",
      "2020-03-26 23:31:55,558 DEV : loss 6.22720193862915 - score 0.0\n",
      "Epoch    36: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2020-03-26 23:31:55,572 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:31:59,345 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:31:59,840 epoch 38 - iter 1/11 - loss 6.23535728 - samples/sec: 65.08\n",
      "2020-03-26 23:32:00,457 epoch 38 - iter 2/11 - loss 5.18112826 - samples/sec: 59.09\n",
      "2020-03-26 23:32:01,036 epoch 38 - iter 3/11 - loss 4.84535662 - samples/sec: 65.21\n",
      "2020-03-26 23:32:01,620 epoch 38 - iter 4/11 - loss 4.79140139 - samples/sec: 64.30\n",
      "2020-03-26 23:32:02,260 epoch 38 - iter 5/11 - loss 4.85164566 - samples/sec: 57.71\n",
      "2020-03-26 23:32:02,814 epoch 38 - iter 6/11 - loss 5.00285920 - samples/sec: 69.00\n",
      "2020-03-26 23:32:03,427 epoch 38 - iter 7/11 - loss 4.91382292 - samples/sec: 61.23\n",
      "2020-03-26 23:32:04,161 epoch 38 - iter 8/11 - loss 4.80996591 - samples/sec: 49.59\n",
      "2020-03-26 23:32:04,716 epoch 38 - iter 9/11 - loss 4.73198356 - samples/sec: 69.00\n",
      "2020-03-26 23:32:05,467 epoch 38 - iter 10/11 - loss 4.62829790 - samples/sec: 47.96\n",
      "2020-03-26 23:32:05,874 epoch 38 - iter 11/11 - loss 4.74406507 - samples/sec: 98.42\n",
      "2020-03-26 23:32:05,970 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:05,972 EPOCH 38 done: loss 4.7441 - lr 0.0002\n",
      "2020-03-26 23:32:06,455 DEV : loss 6.226927757263184 - score 0.0\n",
      "2020-03-26 23:32:06,461 BAD EPOCHS (no improvement): 1\n",
      "2020-03-26 23:32:10,168 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:10,782 epoch 39 - iter 1/11 - loss 3.39893246 - samples/sec: 52.51\n",
      "2020-03-26 23:32:11,448 epoch 39 - iter 2/11 - loss 3.65985894 - samples/sec: 55.90\n",
      "2020-03-26 23:32:12,054 epoch 39 - iter 3/11 - loss 3.46630700 - samples/sec: 62.42\n",
      "2020-03-26 23:32:12,690 epoch 39 - iter 4/11 - loss 4.39291346 - samples/sec: 58.98\n",
      "2020-03-26 23:32:13,386 epoch 39 - iter 5/11 - loss 4.08146949 - samples/sec: 52.95\n",
      "2020-03-26 23:32:14,075 epoch 39 - iter 6/11 - loss 4.23267869 - samples/sec: 53.57\n",
      "2020-03-26 23:32:15,455 epoch 39 - iter 7/11 - loss 4.39441037 - samples/sec: 25.20\n",
      "2020-03-26 23:32:16,251 epoch 39 - iter 8/11 - loss 4.19044247 - samples/sec: 45.84\n",
      "2020-03-26 23:32:17,111 epoch 39 - iter 9/11 - loss 4.55736857 - samples/sec: 43.30\n",
      "2020-03-26 23:32:17,924 epoch 39 - iter 10/11 - loss 4.68763545 - samples/sec: 46.50\n",
      "2020-03-26 23:32:18,548 epoch 39 - iter 11/11 - loss 4.71554160 - samples/sec: 62.42\n",
      "2020-03-26 23:32:18,646 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:18,651 EPOCH 39 done: loss 4.7155 - lr 0.0002\n",
      "2020-03-26 23:32:19,167 DEV : loss 6.226314067840576 - score 0.0\n",
      "2020-03-26 23:32:19,177 BAD EPOCHS (no improvement): 2\n",
      "2020-03-26 23:32:23,061 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:23,592 epoch 40 - iter 1/11 - loss 6.72210407 - samples/sec: 60.65\n",
      "2020-03-26 23:32:24,323 epoch 40 - iter 2/11 - loss 6.56367636 - samples/sec: 50.53\n",
      "2020-03-26 23:32:25,043 epoch 40 - iter 3/11 - loss 5.24652481 - samples/sec: 51.34\n",
      "2020-03-26 23:32:25,679 epoch 40 - iter 4/11 - loss 5.53977823 - samples/sec: 59.75\n",
      "2020-03-26 23:32:26,579 epoch 40 - iter 5/11 - loss 5.27468662 - samples/sec: 39.76\n",
      "2020-03-26 23:32:27,394 epoch 40 - iter 6/11 - loss 5.15177409 - samples/sec: 44.63\n",
      "2020-03-26 23:32:28,132 epoch 40 - iter 7/11 - loss 4.85851070 - samples/sec: 50.48\n",
      "2020-03-26 23:32:29,376 epoch 40 - iter 8/11 - loss 5.01691085 - samples/sec: 27.90\n",
      "2020-03-26 23:32:30,360 epoch 40 - iter 9/11 - loss 4.77096905 - samples/sec: 37.04\n",
      "2020-03-26 23:32:31,081 epoch 40 - iter 10/11 - loss 4.78925121 - samples/sec: 51.64\n",
      "2020-03-26 23:32:31,704 epoch 40 - iter 11/11 - loss 4.65773587 - samples/sec: 61.37\n",
      "2020-03-26 23:32:31,804 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:31,806 EPOCH 40 done: loss 4.6577 - lr 0.0002\n",
      "2020-03-26 23:32:32,301 DEV : loss 6.22625732421875 - score 0.0\n",
      "2020-03-26 23:32:32,314 BAD EPOCHS (no improvement): 3\n",
      "2020-03-26 23:32:36,033 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 23:32:36,594 epoch 41 - iter 1/11 - loss 3.82429576 - samples/sec: 57.24\n",
      "2020-03-26 23:32:37,760 epoch 41 - iter 2/11 - loss 4.69402993 - samples/sec: 30.71\n",
      "2020-03-26 23:32:38,667 epoch 41 - iter 3/11 - loss 4.69954848 - samples/sec: 39.68\n",
      "2020-03-26 23:32:39,653 epoch 41 - iter 4/11 - loss 4.45952392 - samples/sec: 36.37\n",
      "2020-03-26 23:32:40,274 epoch 41 - iter 5/11 - loss 4.55916910 - samples/sec: 61.06\n",
      "2020-03-26 23:32:40,904 epoch 41 - iter 6/11 - loss 4.71630939 - samples/sec: 59.86\n",
      "2020-03-26 23:32:41,485 epoch 41 - iter 7/11 - loss 4.70949759 - samples/sec: 65.48\n",
      "2020-03-26 23:32:42,076 epoch 41 - iter 8/11 - loss 4.79695046 - samples/sec: 64.30\n",
      "2020-03-26 23:32:42,799 epoch 41 - iter 9/11 - loss 4.84136746 - samples/sec: 50.77\n",
      "2020-03-26 23:32:43,415 epoch 41 - iter 10/11 - loss 4.64022799 - samples/sec: 61.58\n",
      "2020-03-26 23:32:43,918 epoch 41 - iter 11/11 - loss 4.88631569 - samples/sec: 78.26\n",
      "2020-03-26 23:32:44,010 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:44,012 EPOCH 41 done: loss 4.8863 - lr 0.0002\n",
      "2020-03-26 23:32:44,570 DEV : loss 6.224457740783691 - score 0.0\n",
      "Epoch    40: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2020-03-26 23:32:44,581 BAD EPOCHS (no improvement): 4\n",
      "2020-03-26 23:32:48,380 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:48,381 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:48,383 learning rate too small - quitting training!\n",
      "2020-03-26 23:32:48,384 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:53,126 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-26 23:32:53,127 Testing using best model ...\n",
      "2020-03-26 23:32:53,129 loading file resources\\taggers\\example-ner\\best-model.pt\n",
      "2020-03-26 23:32:55,175 0.0\t0.0\t0.0\n",
      "2020-03-26 23:32:55,177 \n",
      "MICRO_AVG: acc 0.0 - f1-score 0.0\n",
      "MACRO_AVG: acc 0.0 - f1-score 0.0\n",
      "corporation tp: 0 - fp: 0 - fn: 7 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "creative-work tp: 0 - fp: 0 - fn: 12 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "group      tp: 0 - fp: 0 - fn: 15 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "location   tp: 0 - fp: 0 - fn: 18 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "person     tp: 0 - fp: 0 - fn: 37 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "product    tp: 0 - fp: 0 - fn: 12 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2020-03-26 23:32:55,178 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.0,\n",
       " 'dev_score_history': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'train_loss_history': [18.616629340431906,\n",
       "  6.977936441248113,\n",
       "  6.103660691868175,\n",
       "  5.9199065728621045,\n",
       "  5.653064120899547,\n",
       "  5.406765482642434,\n",
       "  5.209765672683716,\n",
       "  5.154167803851041,\n",
       "  5.020516308871183,\n",
       "  5.193517945029519,\n",
       "  5.018847617236051,\n",
       "  4.947862300005826,\n",
       "  5.008049639788541,\n",
       "  4.682003498077393,\n",
       "  4.6183999668468125,\n",
       "  4.758621801029552,\n",
       "  4.851210529153997,\n",
       "  4.823190732435747,\n",
       "  4.818995757536455,\n",
       "  4.657577211206609,\n",
       "  4.733274709094655,\n",
       "  4.691777023402127,\n",
       "  4.714082891290838,\n",
       "  4.77879296649586,\n",
       "  4.851414507085627,\n",
       "  4.7708283771168105,\n",
       "  4.7026496367021045,\n",
       "  4.798913067037409,\n",
       "  4.646338787945834,\n",
       "  4.818568749861284,\n",
       "  4.551341511986473,\n",
       "  4.978998747738925,\n",
       "  4.719470024108887,\n",
       "  4.604441967877475,\n",
       "  4.592240506952459,\n",
       "  4.563080224123868,\n",
       "  4.674780282107267,\n",
       "  4.744065067984841,\n",
       "  4.71554160118103,\n",
       "  4.657735867933794,\n",
       "  4.88631569255482],\n",
       " 'dev_loss_history': [tensor(7.9048),\n",
       "  tensor(7.2770),\n",
       "  tensor(7.2234),\n",
       "  tensor(6.7968),\n",
       "  tensor(6.5182),\n",
       "  tensor(6.5646),\n",
       "  tensor(6.9448),\n",
       "  tensor(6.7348),\n",
       "  tensor(6.7239),\n",
       "  tensor(6.3045),\n",
       "  tensor(6.3675),\n",
       "  tensor(6.5182),\n",
       "  tensor(6.3559),\n",
       "  tensor(6.3748),\n",
       "  tensor(6.3675),\n",
       "  tensor(6.2758),\n",
       "  tensor(6.3189),\n",
       "  tensor(6.3011),\n",
       "  tensor(6.2923),\n",
       "  tensor(6.2518),\n",
       "  tensor(6.2658),\n",
       "  tensor(6.2578),\n",
       "  tensor(6.2443),\n",
       "  tensor(6.2510),\n",
       "  tensor(6.2360),\n",
       "  tensor(6.2356),\n",
       "  tensor(6.2292),\n",
       "  tensor(6.2181),\n",
       "  tensor(6.2111),\n",
       "  tensor(6.2126),\n",
       "  tensor(6.2172),\n",
       "  tensor(6.2176),\n",
       "  tensor(6.2238),\n",
       "  tensor(6.2258),\n",
       "  tensor(6.2275),\n",
       "  tensor(6.2261),\n",
       "  tensor(6.2272),\n",
       "  tensor(6.2269),\n",
       "  tensor(6.2263),\n",
       "  tensor(6.2263),\n",
       "  tensor(6.2245)]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import WNUT_17\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from typing import List\n",
    "# loading the corpus\n",
    "corpus: Corpus = WNUT_17().downsample(0.1)\n",
    "print(corpus)\n",
    "tag_type = 'ner'\n",
    "# make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary)\n",
    "# initialize embeddings to be used \n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "#initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "#initialize trainer\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "# start training\n",
    "trainer.train('resources/taggers/example-ner',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 23:34:22,497 loading file resources/taggers/example-ner/final-model.pt\n",
      "Bob <B-PER> Marley <E-PER> visited Texas <S-LOC>\n"
     ]
    }
   ],
   "source": [
    "model = SequenceTagger.load('resources/taggers/example-ner/final-model.pt')\n",
    "sentence = Sentence('Bob Marley visited Texas')\n",
    "model.predict(sentence)\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References \n",
    "1. FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP - https://www.aclweb.org/anthology/N19-4010/\n",
    "2. Introduction to Flair for NLP: A Simple yet Powerful State-of-the-Art NLP Library:\n",
    "https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/\n",
    "3. Flair Tutrial : https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md\n",
    "4. WNUT dataset : Leon Derczynski, Eric Nichols, Marieke van Erp, Nut Limsopatham (2017) “Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition”, in Proceedings of the 3rd Workshop on Noisy, User-generated Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Dflair-env",
   "language": "python",
   "name": "flair-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
