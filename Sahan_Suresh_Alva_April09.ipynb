{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nolCmlhGQxKI"
   },
   "source": [
    "# CSCE670 Spotlight: Introduction to NLP using Pytorch\n",
    "\n",
    "### Sahan Suresh Alva (UIN: 130004855) \n",
    "PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Facebook's AI Research lab (FAIR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDhO2UPGRCKo"
   },
   "source": [
    "At its core, PyTorch provides two main features:\n",
    "\n",
    "\n",
    "1.   An n-dimensional Tensor, similar to numpy but can run on GPUs\n",
    "2.   Automatic differentiation for building and training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQZw3h2PRSTZ"
   },
   "source": [
    "## Why PyTorch?\n",
    "\n",
    "There are a few reason you might prefer PyTorch to other deep learning libraries:\n",
    "\n",
    "\n",
    "1.   Unlike other libraries like TensorFlow, where you have to first define an entire computational graph before you can run your model, PyTorch allows you to define your graph dynamically.\n",
    "2.   PyTorch is also great for deep learning research and provides maximum flexibility and speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIqDEMD5fDKj"
   },
   "source": [
    "## Basics of PyTorch\n",
    "\n",
    "When it comes to data objects, Pytorch is quite similar to Numpy. The following examples shows how similar they are:\n",
    "\n",
    "In the NumPy library, we have multi-dimensional arrays whereas in PyTorch, we have tensors. So, let’s first understand what tensors are.\n",
    "\n",
    "Tensors are multidimensional arrays. And PyTorch tensors are similar to NumPy’s n-dimensional arrays. We can use these tensors on a GPU as well (this is not the case with NumPy arrays). This is a major advantage of using tensors.\n",
    "\n",
    "PyTorch supports multiple types of tensors, including:\n",
    "*   FloatTensor: 32-bit float\n",
    "*   DoubleTensor: 64-bit float\n",
    "*   HalfTensor: 16-bit float\n",
    "*   IntTensor: 32-bit int\n",
    "*   LongTensor: 64-bit int\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DYDw0NsHhWRq",
    "outputId": "bcbb9cca-4e90-46a7-fda6-6fe0df2a074d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <class 'numpy.ndarray'>\n",
      "tensor(1) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = np.array(1)\n",
    "b = torch.tensor(1)\n",
    "\n",
    "print(a, type(a))\n",
    "print(b, type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ofPP_8c8idGL"
   },
   "source": [
    "### Basic Mathematical Operations\n",
    "\n",
    "We will compare some baisc mathematical operation between NumPy and Pytorch. The operations are very similar in Pytorch. Features like broadcasting are also present in Pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "H1BFp9sBjAZK",
    "outputId": "960a27f0-107d-4043-995d-95b8d7216fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 5 50 2.0\n",
      "15 5 50 2.0\n"
     ]
    }
   ],
   "source": [
    "np_a = np.array(10)\n",
    "np_b = np.array(5)\n",
    "print(np_a + np_b, np_a - np_b, np_a * np_b, np_a / np_b)\n",
    "\n",
    "pt_a = np.array(10)\n",
    "pt_b = np.array(5)\n",
    "print(pt_a + pt_b, pt_a - pt_b, pt_a * pt_b, pt_a / pt_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYPypXnpjcuQ"
   },
   "source": [
    "### Matrix Operations\n",
    "\n",
    "We will compare some matrix operations between NumPy and Pytorch. The operations are very similar in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "GqVKjDdnjcfs",
    "outputId": "ade93685-1f91-4b0d-b924-0de12e47687e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0392742  -0.60168199  0.18195878]\n",
      " [ 1.76499213 -2.14743362 -1.95905479]\n",
      " [ 1.01692529 -0.24539639 -0.15522705]] \n",
      "\n",
      "[[-0.12814468 -0.62164688  0.21069439]\n",
      " [ 0.90133115 -0.02065676 -0.3790019 ]\n",
      " [ 1.30648762 -1.7246546  -2.20677932]] \n",
      "\n",
      "[[ 0.9155008   0.29835784 -1.39069607]\n",
      " [ 6.29449313  0.12238321  0.13573803]\n",
      " [-2.80855031 -0.75771243 -1.49396459]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "np_a = np.random.randn(3,3)\n",
    "np_b = np.random.randn(3,3)\n",
    "\n",
    "print(np.add(np_a,np_b), '\\n')\n",
    "print(np.dot(np_a,np_b), '\\n')\n",
    "print(np.divide(np_a,np_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "5AnfCAcXBl-N",
    "outputId": "8ff1f4ae-3898-4877-e3d3-a7c9f5a1b2bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6040,  0.6637,  1.0438],\n",
      "        [ 1.3406, -2.8127, -1.1753],\n",
      "        [ 3.1662,  0.6841,  1.2788]]) \n",
      "\n",
      "tensor([[ 0.4576,  0.2724,  0.3367],\n",
      "        [-1.3636,  1.7743,  1.1446],\n",
      "        [ 0.3243,  2.8696,  2.7954]]) \n",
      "\n",
      "tensor([[ 1.2594,  0.2408,  0.2897],\n",
      "        [ 0.2075,  0.6645,  0.1884],\n",
      "        [ 2.3051, -0.4826,  0.5649]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "pt_a = torch.randn(3,3)\n",
    "pt_b = torch.randn(3,3)\n",
    "\n",
    "print(torch.add(pt_a,pt_b), '\\n')\n",
    "print(torch.mm(pt_a,pt_b), '\\n')\n",
    "print(torch.div(pt_a,pt_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUTyR_nqCRJs"
   },
   "source": [
    "#### Concatenating Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "CAERSKO8CQN3",
    "outputId": "38af7f2d-e8b1-4a60-f189-599a2f3fb2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "tensor([[5, 6],\n",
      "        [7, 8]]) \n",
      "\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[5,6],[7,8]])\n",
    "c = torch.cat((a,b))\n",
    "print(a, '\\n')\n",
    "print(b, '\\n')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SAr0UEqCjvT"
   },
   "source": [
    "#### Reshaping Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "et_1WR2VCj4M",
    "outputId": "3f4d4601-8cf9-44ac-b716-df98198aa92e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) \n",
      "\n",
      "tensor([[1, 2, 3, 4, 5, 6, 7, 8]]) \n",
      "\n",
      "torch.Size([1, 8]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(c.shape, \"\\n\")\n",
    "d =c.reshape(1,8)\n",
    "print(d, \"\\n\")\n",
    "print(d.shape, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCvOSz_CDwDm"
   },
   "source": [
    "### Important PyTorch Modules\n",
    "\n",
    "#### Autograd Module\n",
    "\n",
    "PyTorch uses a technique called automatic differentiation. It records all the operations that we are performing and replays it backward to compute gradients. This technique helps us to save time on each epoch as we are calculating the gradients on the forward pass itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "eqOwn35JEEXf",
    "outputId": "1e971d3e-a154-4f7e-9b6e-3fca45d5a880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11., 11.],\n",
      "        [11., 11.]], grad_fn=<AddBackward0>) tensor(11., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,2), requires_grad=True)\n",
    "b = a + 10\n",
    "c = b.mean()\n",
    "print(b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtoKXdoGGxA4"
   },
   "source": [
    "We added 10 to all the elements of **a** and then taken the mean it. \n",
    "\n",
    "Now, the derivative of c w.r.t. a will be ¼ and hence the gradient matrix will be 0.25. Let’s verify this using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vzwEClpLGwUx",
    "outputId": "74bb1b0b-fc2d-4a9c-fbb2-0f3cadac7f40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "c.backward()\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZyztUMLzHY2y"
   },
   "source": [
    "#### Optim Module\n",
    "\n",
    "The Optim module in PyTorch has pre-written codes for most of the optimizers that are used while building a neural network. We just have to import them and then they can be used to build models. The below are the examples to get the ADAM and SGD optimizers. We cannot execute the code since we still havent built the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7ujwbb0Hfx5"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "#adam\n",
    "# adam = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# sgd\n",
    "# SGD = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_AKz0-5IFlK"
   },
   "source": [
    "#### nn Module\n",
    "The autograd module in PyTorch helps us define computation graphs as we proceed in the model. But, just using the autograd module can be low-level when we are dealing with a complex neural network.\n",
    "\n",
    "In those cases, we can make use of the nn module. This defines a set of functions, similar to the layers of a neural network, which takes the input from the previous state and produces an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCbsQf9RIKo7"
   },
   "source": [
    "## Building a Sentiment Classifier in Pytorch\n",
    "\n",
    "In this section we will build a deep learning model to detect the sentiment using Pytorch and TorchText. We will use movie review from the IMDB dataset for this tutorial.\n",
    "\n",
    "### Data\n",
    "\n",
    "The following code automatically downloads the IMDb dataset and splits it into the canonical train/test splits as torchtext.datasets objects. It process the data using the Fields we have previously defined. The IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review.\n",
    "\n",
    "The parameters of a Field specify how the data should be processed. We use the TEXT field to define how the review should be processed, and the LABEL field to process the sentiment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzVr2PwG2BrN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "49HSKuwd7KCB",
    "outputId": "ba3890c2-d6bd-4bbc-e86d-f4702f73b0d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz:   0%|          | 131k/84.1M [00:00<01:14, 1.13MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 51.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zT8AtL2_MYv-",
    "outputId": "8da0bb89-eac6-4940-ca08-ed4a918e6941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C0l9mJl0NdSH"
   },
   "source": [
    "We split the train data to train and validation data using a 80/20 ratio. We use the same random.seed to do the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WBU4--W_2Rza",
    "outputId": "e91e3e6b-298e-4791-a866-64e4c8d14a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3gnJsswjNp5"
   },
   "source": [
    "We use of Glove word embeddings as the input to pur deep learning model. Instead of having our word embeddings initialized randomly, they are initialized with Glove pre-trained embedding vectors. We get these vectors simply by specifying which vectors we want and passing it as an argument to build_vocab. TorchText handles downloading the vectors and associating them with the correct words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mYBBFPM1-SFM",
    "outputId": "d253cbd9-e0b2-4586-d818-963d50be18ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:27, 2.23MB/s]                          \n",
      "100%|█████████▉| 398252/400000 [00:15<00:00, 25449.20it/s]"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmiLW2sukFMv"
   },
   "source": [
    "\n",
    "Now we create the iterators, placing the tensors on the GPU if one is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PF948pVn75Ro"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_SSaTm-kiIE"
   },
   "source": [
    "### Model\n",
    "\n",
    "We will be using Bidirectional LSTM architechture for our model. \n",
    "\n",
    "*Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architectureused in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video)*\n",
    "\n",
    "We are using the bi-directional version of the LSTM. We have first RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the last to the first (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$.\n",
    "\n",
    "\n",
    "\n",
    "![title](Deep-Dive-into-Bidirectional-LSTM-i2tutorials.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GzmdVWJB7__o"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                    \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Zp-FjWx2rTl"
   },
   "source": [
    "We introduce regularization by using dropout = 0.5, which removes random hidden states for some perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8s1Q0-og8Ip_"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2g4VR5gV93o2",
    "outputId": "26a3c58c-3765-4764-9ccf-44a86e589ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ieYE-O3BGPW4"
   },
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v29GELIC3pxT"
   },
   "source": [
    "### Training \n",
    "\n",
    "As we had shown in the initial section, we use the optim module to create an Adam optimizer for our model. Adam adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnApstjqGYVm"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOtLB2ta4Kqj"
   },
   "source": [
    "We implement the function to calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmM258zSGdWw"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "   \n",
    "\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5JLt6bS7W_7"
   },
   "source": [
    "We define a function for training our model.\n",
    "\n",
    "As we are now using dropout, we must remember to use model.train() to ensure the dropout is \"turned on\" while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9qkV1VvGjbn"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1L7j7RwE7g6U"
   },
   "source": [
    "We define a function for testing our model.\n",
    "\n",
    "As we are now using dropout, we must remember to use model.eval() to ensure the dropout is \"turned off\" while evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Mp0nn8AGnCp"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFOVdvXuGrNl"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gE5ZEjSBAuPd"
   },
   "source": [
    "At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "ywg22a5NGvwi",
    "outputId": "c6399087-6c17-4590-cb5f-e06674cc0847"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 398252/400000 [00:29<00:00, 25449.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 01 | Iteration Time: 0m 42s | Train Acc: 60.39% |  Val Acc: 72.14%\n",
      "Iteration: 02 | Iteration Time: 0m 41s | Train Acc: 73.51% |  Val Acc: 81.76%\n",
      "Iteration: 03 | Iteration Time: 0m 41s | Train Acc: 80.30% |  Val Acc: 85.12%\n",
      "Iteration: 04 | Iteration Time: 0m 41s | Train Acc: 84.56% |  Val Acc: 86.51%\n",
      "Iteration: 05 | Iteration Time: 0m 41s | Train Acc: 86.39% |  Val Acc: 83.18%\n",
      "Iteration: 06 | Iteration Time: 0m 41s | Train Acc: 88.58% |  Val Acc: 88.75%\n",
      "Iteration: 07 | Iteration Time: 0m 41s | Train Acc: 90.65% |  Val Acc: 88.79%\n",
      "Iteration: 08 | Iteration Time: 0m 41s | Train Acc: 91.08% |  Val Acc: 89.38%\n",
      "Iteration: 09 | Iteration Time: 0m 41s | Train Acc: 92.29% |  Val Acc: 88.85%\n",
      "Iteration: 10 | Iteration Time: 0m 41s | Train Acc: 93.03% |  Val Acc: 90.00%\n",
      "Iteration: 11 | Iteration Time: 0m 41s | Train Acc: 93.89% |  Val Acc: 89.69%\n",
      "Iteration: 12 | Iteration Time: 0m 41s | Train Acc: 94.62% |  Val Acc: 89.80%\n",
      "Iteration: 13 | Iteration Time: 0m 41s | Train Acc: 95.21% |  Val Acc: 89.32%\n",
      "Iteration: 14 | Iteration Time: 0m 41s | Train Acc: 95.21% |  Val Acc: 89.74%\n",
      "Iteration: 15 | Iteration Time: 0m 41s | Train Acc: 95.97% |  Val Acc: 90.12%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Iteration: {epoch+1:02} | Iteration Time: {epoch_mins}m {epoch_secs}s | Train Acc: {train_acc*100:.2f}% |  Val Acc: {valid_acc*100:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWrDZQrIBCkv"
   },
   "source": [
    "Using the best model that we have saved, we calculate the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PEw8a-f43Xe7",
    "outputId": "4716821d-5d30-4fc8-a70f-83a7a636b404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.302 | Test Acc: 88.22%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "spotlight.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
