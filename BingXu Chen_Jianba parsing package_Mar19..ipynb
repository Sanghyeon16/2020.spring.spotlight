{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotlight Jieba Parsing Package Usage in Information Retrieve\n",
    "#### submitted by Chen BingXu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction:\n",
    "\"Jieba\" (Chinese for \"to stutter\") Chinese text segmentation: built to be the best Python Chinese word segmentation module.  \n",
    "`If you're face with some Chinese text to deal with, you can use this package to do something, like finding the keywords or find the top files you want to find.By the way, I'm sorry to say that this package can use to deal with English, but not very good.`  \n",
    "`(there are some Chinese example in this notebook, if you find hard to read about it, you can use some package to convert the example to other languages`\n",
    "\n",
    "## 2 Features:\n",
    "- Support three types of segmentation mode:  \n",
    "1.Accurate Mode attempts to cut the sentence into the most accurate segmentations, which is suitable for text analysis.  \n",
    "2.Full Mode gets all the possible words from the sentence. Fast but not accurate.Search Engine Mode, based on the Accurate Mode, attempts to cut long words into several short words, which can raise  \n",
    "3.the recall rate. Suitable for search engines.  \n",
    "- Supports Traditional Chinese\n",
    "- Supports customized dictionaries\n",
    "\n",
    "## 3 Algorithm:\n",
    "- Based on a prefix dictionary structure to achieve efficient word graph scanning. Build a directed acyclic graph (DAG) for all possible word combinations.\n",
    "- Use dynamic programming to find the most probable combination based on the word frequency.\n",
    "- For unknown words, a HMM-based model is used with the Viterbi algorithm.\n",
    "\n",
    "## 4 Usage\n",
    "- Fully automatic installation: easy_install jieba or pip install jieba\n",
    "- Semi-automatic installation: Download http://pypi.python.org/pypi/jieba/ , run python setup.py install after extracting.\n",
    "- Manual installation: place the jieba directory in the current directory or python site-packages directory.\n",
    "import jieba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Cut (Parsing)\n",
    "- The `jieba.cut` function accepts three input parameters: the first parameter is the string to be cut; the second parameter is `cut_all`, controlling the cut mode; the third parameter is to control whether to use the Hidden Markov Model.\n",
    "- `jieba.cut_for_search` accepts two parameter: the string to be cut; whether to use the Hidden Markov Model. This will cut the sentence into short words suitable for search engines.\n",
    "- The input string can be an unicode/str object, or a str/bytes object which is encoded in UTF-8 or GBK. Note that using GBK encoding is not recommended because it may be unexpectly decoded as UTF-8.\n",
    "- `jieba.cut` and `jieba.cut_for_search` returns an generator, from which you can use a `for` loop to get the segmentation result (in unicode).\n",
    "- `jieba.lcut` and `jieba.lcut_for_search` returns a list.\n",
    "- `jieba.Tokenizer(dictionary=DEFAULT_DICT)` creates a new customized Tokenizer, which enables you to use different dictionaries at the same time. `jieba.dt` is the default Tokenizer, to which almost all global functions are mapped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement(Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "#encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 默认模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement(English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "世界,经济,论坛,也,叫,达沃斯,论坛,。\n",
      "The, ,World, ,Economic, ,Forum, ,is, ,also, ,called, ,the, ,Davos, ,Forum,.\n"
     ]
    }
   ],
   "source": [
    "data=[\n",
    "   \"世界经济论坛也叫达沃斯论坛。\",\n",
    "   \"The World Economic Forum is also called the Davos Forum.\"\n",
    "    ]\n",
    "\n",
    "for d in data:\n",
    "    seg_list = jieba.cut(d)\n",
    "    \n",
    "    print(\",\".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Modify dictionary\n",
    "- Use `add_word(word, freq=None, tag=None)` and `del_word(word)` to modify the dictionary dynamically in programs.\n",
    "\n",
    "- Use `suggest_freq(segment, tune=True)` to adjust the frequency of a single word so that it can (or cannot) be segmented.\n",
    "\n",
    "- Note that HMM may affect the final result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement(Chinese and English) Compare to the result of `Cut` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "世界经济论坛,也,叫,达沃斯论坛,。\n",
      "The, ,World Economic Forum, ,is, ,also, ,called, ,the, ,Davos Forum,.\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('世界经济论坛')\n",
    "jieba.add_word('达沃斯论坛')\n",
    "jieba.add_word('World Economic Forum')\n",
    "jieba.add_word('Davos Forum')\n",
    "\n",
    "data=[\n",
    "   \"世界经济论坛也叫达沃斯论坛。\",\n",
    "   \"The World Economic Forum is also called the Davos Forum.\"\n",
    "    ]\n",
    "\n",
    "for d in data:\n",
    "    seg_list = jieba.cut(d)\n",
    "    \n",
    "    print(\",\".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Keyword Extraction(base on TF-IDF)\n",
    "`import jieba.analyse`\n",
    "\n",
    "- jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n",
    "    * sentence: the text to be extracted\n",
    "    * topK: return how many keywords with the highest TF/IDF weights. The default value is 20\n",
    "    * withWeight: whether return TF/IDF weights with the keywords. The default value is False\n",
    "    * allowPOS: filter words with which POSs are included. Empty for no filtering.\n",
    "- jieba.analyse.TFIDF(idf_path=None) creates a new TFIDF instance, idf_path specifies IDF file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧亚 : 0.7458841454643479\n",
      "吉林 : 0.6733651014252174\n",
      "置业 : 0.49933765769413047\n",
      "万元 : 0.3466477318421739\n",
      "增资 : 0.3431245420230435\n"
     ]
    }
   ],
   "source": [
    "#Keyword Extraction\n",
    "\n",
    "import jieba.analyse\n",
    "\n",
    "kWords = jieba.analyse.extract_tags(\"此外，公司拟对全资子公司吉林欧亚置业有限公司增资4.3亿元，增资后，吉林欧亚置业注册资本由7000万元增加到5亿元。>吉林欧亚置业主要经营范围为房地产开发及百货零售等业务。目前在建吉林欧亚城>市商业综合体项目。2013年，实现营业收入0万元，实现净利润-139.13万元。\",topK=5,withWeight=True)\n",
    "\n",
    "for word,weight in kWords:\n",
    "   # print(word+\":\"+weight)\n",
    "   print(word,\":\",weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword Extraction(base on TextRank) <--------> Compare with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "饮用水 1.062341659448913\n",
      "奚传武 0.5197725001260869\n",
      "讲座 0.3842644616934783\n",
      "美国 0.27108236293434784\n",
      "科学系 0.2672008639021739\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "content = u'会议邀请到美国密歇根大学(University of Michigan, Ann Arbor）环境健康科学系副教授奚传武博士作题为“Multibarrier approach for safe drinking waterin the US : Why it failed in Flint”的学术讲座，介绍美国密歇根Flint市饮用水污染事故的发生发展和处置等方面内容。讲座后各相关单位同志与奚传武教授就生活饮用水在线监测系统、美国水污染事件的处置方式、生活饮用水老旧管网改造、如何有效减少消毒副产物以及美国涉水产品和二次供水单位的监管模式等问题进行了探讨和交流。本次交流会是我市生活饮用水卫生管理工作洽商机制运行以来的又一次新尝试，也为我市卫生计生综合监督部门探索生活饮用水卫生安全管理模式及突发水污染事件的应对措施开拓了眼界和思路。'\n",
    "\n",
    "#base on TF-IDF\n",
    "keywords = jieba.analyse.extract_tags(content,topK = 5,withWeight = True,allowPOS = ('n','nr','ns'))\n",
    "for item in keywords:\n",
    "    print(item[0],item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "饮用水 1.0\n",
      "美国 0.5705647859730464\n",
      "奚传武 0.5107384245087675\n",
      "单位 0.4728418893343898\n",
      "讲座 0.4437707320528082\n"
     ]
    }
   ],
   "source": [
    "#base on TextRank\n",
    "keywords = jieba.analyse.textrank(content,topK = 5,withWeight = True,allowPOS = ('n','nr','ns'))\n",
    "for item in keywords:\n",
    "    print(item[0],item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Part of Speech Tagging\n",
    "- jieba.posseg.POSTokenizer(tokenizer=None) creates a new customized Tokenizer. tokenizer specifies the jieba.Tokenizer to internally use. jieba.posseg.dt is the default POSTokenizer.\n",
    "- Tags the POS of each word after segmentation, using labels compatible with ictclas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "北京 ns\n",
      "天安门 ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱北京天安门\")\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you dealing with an English sequences, it can't help you to tag the word.But when you're doing a sequences contain Chinese and English, it can help you to tag the English word as eng  \n",
    "Here is the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "打篮球 n\n",
      "  x\n",
      "The eng\n",
      "  x\n",
      "World eng\n",
      "  x\n",
      "Economic eng\n",
      "  x\n",
      "Forum eng\n",
      "  x\n",
      "is eng\n",
      "  x\n",
      "also eng\n",
      "  x\n",
      "called eng\n",
      "  x\n",
      "the eng\n",
      "  x\n",
      "Davos eng\n",
      "  x\n",
      "Forum eng\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱打篮球 The World Economic Forum is also called the Davos Forum\")\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Tokenize: return words with position\n",
    "- The input must be unicode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和\t\t start: 0 \t\t end:2\n",
      "word 服装\t\t start: 2 \t\t end:4\n",
      "word 饰品\t\t start: 4 \t\t end:6\n",
      "word 有限公司\t\t start: 6 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "result = jieba.tokenize(u'永和服装饰品有限公司')\n",
    "for tk in result:\n",
    "    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和\t\t start: 0 \t\t end:2\n",
      "word 服装\t\t start: 2 \t\t end:4\n",
      "word 饰品\t\t start: 4 \t\t end:6\n",
      "word 有限\t\t start: 6 \t\t end:8\n",
      "word 公司\t\t start: 8 \t\t end:10\n",
      "word 有限公司\t\t start: 6 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "result = jieba.tokenize(u'永和服装饰品有限公司',mode='search')\n",
    "for tk in result:\n",
    "    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 An Search Engine Whoosh Using jieba package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\": \"这是我们增加的第一篇文档\", \"title\": \"第一篇文档\"}\n",
      "第一篇<b class=\"match term0\">文档</b>\n",
      "0.5945348918918356\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import os\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from jieba.analyse import ChineseAnalyzer\n",
    "import json\n",
    "\n",
    "analyzer = ChineseAnalyzer()\n",
    "\n",
    "# creat schema, stored is True reprsent that it can be searched\n",
    "schema = Schema(title=TEXT(stored=True, analyzer=analyzer), path=ID(stored=False),\n",
    "                content=TEXT(stored=True, analyzer=analyzer))\n",
    "indexdir = 'indexdir/'\n",
    "if not os.path.exists(indexdir):\n",
    "    os.mkdir(indexdir)\n",
    "ix = create_in(indexdir, schema)\n",
    "\n",
    "#String format need to be unicode\n",
    "writer = ix.writer()\n",
    "writer.add_document(title=u\"第一篇文档\", path=u\"/a\",\n",
    "                    content=u\"这是我们增加的第一篇文档\")\n",
    "writer.add_document(title=u\"第二篇文档\", path=u\"/b\",\n",
    "                    content=u\"第二篇文档也很interesting！\")\n",
    "writer.commit()\n",
    "\n",
    "searcher = ix.searcher()\n",
    "\n",
    "results = searcher.find(\"title\", u\"文档\")\n",
    "\n",
    "#The first result data structure is dict{'title':.., 'content':...}\n",
    "firstdoc = results[0].fields()\n",
    "\n",
    "jsondoc = json.dumps(firstdoc, ensure_ascii=False)\n",
    "\n",
    "print(jsondoc) \n",
    "print(results[0].highlights(\"title\"))\n",
    "print(results[0].score)  # bm25 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion\n",
    "\n",
    "From studying, I have studied that there are many python package now for parsing or doing some information function.For example, jieba parsing package is good for parsing and searching key word when facing with a Chinses file;NLTK is good for users doing parsing or other function when facing with english file.So you need to apply different package when facing with different language file or combine/mix dfferent when you facing with the file that contain many languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in learning more about it, here is the document of it:https://github.com/fxsjy/jieba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
