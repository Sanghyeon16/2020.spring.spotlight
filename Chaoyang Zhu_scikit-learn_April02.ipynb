{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning LDA Chinese topic models with scikit-learn\n",
    "\n",
    "Scikit-learn is an important machine learning library in Python. Scikit-learn is short for sklearn. It supports four machine learning algorithms including classification, regression, dimensionality reduction and clustering. It also includes three modules: feature extraction, data processing and model evaluation.\n",
    "\n",
    "## A quick overview of machine learning\n",
    "\n",
    "The general process of traditional machine learning tasks from the beginning to the modeling is: \n",
    "* obtaining data\n",
    "* data preprocessing\n",
    "* training models\n",
    "* model evaluation\n",
    "* prediction and classification.\n",
    "\n",
    "This time we will follow the traditional machine learning process to see what common functions are in each step of the process and how they are used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining data\n",
    "\n",
    "### Import sklearn dataset\n",
    "\n",
    "Sklearn contains a large number of high-quality data sets. In the process of learning machine learning, we can use these data sets to implement different models. At the same time, this process can also deepen the understanding and grasp of theoretical knowledge.\n",
    "\n",
    "First, to use the datasets in sklearn, you must import the datasets module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use iris data as an example to represent the exported data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris() \n",
    "X = iris.data \n",
    "y = iris.target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "\n",
    "In addition to using the dataset contained in sklearn, we can also create training samples ourselves.\n",
    "\n",
    "Specific usage can refer to: https://scikit-learn.org/stable/datasets/\n",
    "\n",
    "Here we take an example of a sample generator for classification problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_classification\n",
    " \n",
    "X, y = make_classification(n_samples=6, n_features=5, n_informative=2,\n",
    "    n_redundant=2, n_classes=2, n_clusters_per_class=2, scale=1.0,\n",
    "    random_state=20)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [-0.6600737  -0.0558978   0.82286793  1.1003977  -0.93493796]\n",
      "1: [ 0.4113583   0.06249216 -0.90760075 -1.41296696  2.059838  ]\n",
      "1: [ 1.52452016 -0.01867812  0.20900899  1.34422289 -1.61299022]\n",
      "0: [-1.25725859  0.02347952 -0.28764782 -1.32091378 -0.88549315]\n",
      "0: [-3.28323172  0.03899168 -0.43251277 -2.86249859 -1.10457948]\n",
      "1: [ 1.68841011  0.06754955 -1.02805579 -0.83132182  0.93286635]\n"
     ]
    }
   ],
   "source": [
    "for x_,y_ in zip(X,y):\n",
    "    print(y_,end=': ')\n",
    "    print(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The data preprocessing stage is an indispensable part of machine learning, which will make the data more effectively recognized by the model or evaluator.\n",
    "\n",
    "### Data standardization\n",
    "\n",
    "Normalization: In machine learning, we may have to deal with different kinds of data, such as pixel values on audio and pictures. These data may be high latitudes. After the data is normalized, the values in each feature will average to 0 ( The value of each feature is subtracted from the average of the feature in the original data), and the standard deviation becomes 1. This method is widely used in many machine learning algorithms (for example: support vector machines, logistic regression, and neural network-like) .\n",
    "\n",
    "StandardScaler calculates the average and standard deviation of the training set in order to test the data and use the same transformation.\n",
    "\n",
    "After the transformation, the features in each dimension have a mean value of 0, and the unit variance is also called z-score normalization (zero mean normalization). The calculation method is to subtract the mean value from the feature value and divide it by the standard deviation.\n",
    "\n",
    "* fit: It is used to calculate the mean and variance of the training data. Later, the mean and variance will be used to convert the training data.\n",
    "\n",
    "* fit_transform: Not only calculate the mean and variance of the training data, but also convert the training data based on the calculated mean and variance, thereby transforming the data into a standard normal distribution.\n",
    "\n",
    "* transform: Obviously, it just transforms, it just transforms the training data into a standard normal distribution. (Generally, the train and test sets are put together for standardization, or after the training set is standardized, the same standardizer is used to standardize the test set. At this time, a scaler can be used.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23118282, -1.71131893,  1.71770304,  1.19857094, -0.51965501],\n",
       "       [ 0.39217125,  0.97024754, -1.00050853, -0.50892817,  1.77777141],\n",
       "       [ 1.03980354, -0.86828257,  0.75345647,  1.36421794, -1.03981919],\n",
       "       [-0.57862217,  0.08660018, -0.02668957, -0.4463902 , -0.4817237 ],\n",
       "       [-1.75732374,  0.437955  , -0.2542427 , -1.49369334, -0.64979461],\n",
       "       [ 1.13515394,  1.08479879, -1.18971871, -0.11377717,  0.91322111]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization\n",
    "\n",
    "The minimum-maximum normalization performs a linear transformation on the original data and transforms it to the interval [0,1] (it can also be other intervals with fixed minimum and maximum values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52762409, 0.        , 1.        , 0.94203914, 0.18461312],\n",
       "       [0.74313278, 0.95903204, 0.06507834, 0.34457514, 1.        ],\n",
       "       [0.96703505, 0.30150246, 0.66834995, 1.        , 0.        ],\n",
       "       [0.40750585, 0.64300551, 0.40002079, 0.36645754, 0.19807544],\n",
       "       [0.        , 0.76866361, 0.32175449, 0.        , 0.13842486],\n",
       "       [1.        , 1.        , 0.        , 0.4828408 , 0.69315972]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(X)\n",
    "scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize\n",
    "\n",
    "An essential operation when you want to calculate the similarity of two samples is regularization. The idea is: first find the p-norm of the sample, and then divide all elements of the sample by the norm, so that the norm of each sample is 1 in the end. Normalization (Normalization) maps the values of different ranges to the same fixed range. Commonly, [0,1] is also normalized.\n",
    "\n",
    "The following example transforms each sample into a unit norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "print(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset splitting\n",
    "\n",
    "When obtaining a training data set, we usually often split the training data into a training set and a validation set, which is helpful to the selection of our model parameters.\n",
    "\n",
    "train_test_split is a commonly used function in cross-validation. Its function is to randomly select train data and testdata from the sample according to the proportion. The example codes are as follows:\n",
    "\n",
    "X_train,X_test, y_train, y_test = cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "In this step, we must first analyze the type of our data and understand what model we want to use. Then we can define the model in sklearn. Sklearn provides very similar interfaces for all models. Familiar with the usage of all models. Before that, let's take a look at the common properties and functions of models.\n",
    "\n",
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "model = naive_bayes.GaussianNB()\n",
    "model = naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "model = naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(C=1.0, kernel='rbf', gamma='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "model = neighbors.KNeighborsClassifier(n_neighbors=5, n_jobs=1)\n",
    "model = neighbors.KNeighborsRegressor(n_neighbors=5, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and selection\n",
    "\n",
    "Evaluation indicators have different indicators for different machine learning tasks, and the same task has different evaluation indicators.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "The original data (dataset) is grouped in a sense, one part is used as a training set and the other is used as a validation set or test set. The classifier is first trained with the training set, and then the validation set is used to test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn LDA topic model overview\n",
    "\n",
    "In scikit-learn, the class of the LDA topic model is in the sklearn.decomposition.LatentDirichletAllocation package, and its algorithm implementation is mainly based on the variational inference EM algorithm.\n",
    "\n",
    "We have three document corpora, which are placed in nlp_test0.txt, nlp_test2.txt and nlp_test4.txt\n",
    "\n",
    "### Parsing\n",
    "\n",
    "First we perform parsing, and save the parsing results in nlp_test1.txt, nlp_test3.txt, and nlp_test5.txt, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.suggest_freq('沙瑞金', True)\n",
    "jieba.suggest_freq('易学习', True)\n",
    "jieba.suggest_freq('王大路', True)\n",
    "jieba.suggest_freq('京州', True)\n",
    "#parsing the first file\n",
    "f = open(\"nlp_test0.txt\", encoding='UTF-8')\n",
    "document = f.read()\n",
    "document_cut = jieba.cut(document)\n",
    "result = ' '.join(document_cut)\n",
    "f2 = open(\"nlp_test1.txt\", 'w', encoding='UTF-8')\n",
    "f2.write(result)\n",
    "f.close()\n",
    "f2.close() \n",
    "\n",
    "f3 = open(\"nlp_test1.txt\", encoding='UTF-8')\n",
    "res1 = f3.read()\n",
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。\n"
     ]
    }
   ],
   "source": [
    "#parsing the second file\n",
    "f = open(\"nlp_test2.txt\", encoding='UTF-8')\n",
    "document = f.read()\n",
    "document_cut = jieba.cut(document)\n",
    "result = ' '.join(document_cut)\n",
    "f2 = open(\"nlp_test3.txt\", 'w', encoding='UTF-8')\n",
    "f2.write(result)\n",
    "f.close()\n",
    "f2.close() \n",
    "\n",
    "f3 = open(\"nlp_test3.txt\", encoding='UTF-8')\n",
    "res2 = f3.read()\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 年 （ 永和 三年 ） 三月 ， 桓 温兵 至 彭模 （ 今 四川 彭山 东南 ） ， 留下 参军 周楚 、 孙盛 看守 辎重 ， 自己 亲率 步兵 直攻 成都 。 同月 ， 成汉 将领 李福 袭击 彭模 ， 结果 被 孙盛 等 人 击退 ； 而桓温 三 战三胜 ， 一直 逼近 成都 。\n"
     ]
    }
   ],
   "source": [
    "#parsing the third file\n",
    "f = open(\"nlp_test4.txt\", encoding='UTF-8')\n",
    "document = f.read()\n",
    "document_cut = jieba.cut(document)\n",
    "result = ' '.join(document_cut)\n",
    "f2 = open(\"nlp_test5.txt\", 'w', encoding='UTF-8')\n",
    "f2.write(result)\n",
    "f.close()\n",
    "f2.close() \n",
    "\n",
    "f3 = open(\"nlp_test5.txt\", encoding='UTF-8')\n",
    "res3 = f3.read()\n",
    "print(res3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stop words from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwrdpath = \"stop_words.txt\"\n",
    "stpwrd_dic = open(stpwrdpath, 'rb')\n",
    "stpwrd_content = stpwrd_dic.read()\n",
    "#convert to list \n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert words into word frequency vectors. \n",
    "Note that because LDA is based on word frequency, TF-IDF is generally not used for document features. Codes are shown as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 47)\t2\n",
      "  (0, 66)\t1\n",
      "  (0, 41)\t4\n",
      "  (0, 59)\t1\n",
      "  (0, 72)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 42)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 68)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 44)\t2\n",
      "  (0, 63)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 12)\t2\n",
      "  (0, 4)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 28)\t2\n",
      "  (0, 2)\t2\n",
      "  (0, 24)\t1\n",
      "  (0, 64)\t1\n",
      "  (0, 73)\t1\n",
      "  (0, 71)\t1\n",
      "  (0, 17)\t1\n",
      "  :\t:\n",
      "  (2, 35)\t2\n",
      "  (2, 25)\t1\n",
      "  (2, 34)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 51)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 23)\t1\n",
      "  (2, 29)\t2\n",
      "  (2, 55)\t1\n",
      "  (2, 67)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 45)\t1\n",
      "  (2, 53)\t1\n",
      "  (2, 38)\t2\n",
      "  (2, 21)\t1\n",
      "  (2, 37)\t1\n",
      "  (2, 32)\t1\n",
      "  (2, 43)\t1\n",
      "  (2, 61)\t1\n",
      "  (2, 57)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 58)\t1\n",
      "  (2, 39)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 70)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "corpus = [res1,res2,res3]\n",
    "cntVector = CountVectorizer(stop_words=stpwrdlst)\n",
    "cntTf = cntVector.fit_transform(corpus)\n",
    "print(cntTf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the LDA topic model\n",
    "The output is the word frequency vector of each word in all documents. With this word frequency vector, we can make the LDA topic model. Since we only have three documents, we choose the number of topics K = 2. Codes are shown as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=2, learning_offset=50., random_state=0)\n",
    "docres = lda.fit_transform(cntTf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the results\n",
    "\n",
    "Through the fit_transform function, we can get the topic model of the document distributed in docres. The topic terms are distributed in lda.components_.\n",
    "\n",
    "#### Document topics are distributed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99176826 0.00823174]\n",
      " [0.01463255 0.98536745]\n",
      " [0.01463255 0.98536745]]\n"
     ]
    }
   ],
   "source": [
    "print(docres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The topics and words are distributed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50064745 0.50064745 2.49967511 2.49967511 2.49967511 0.50064745\n",
      "  0.50064745 2.49967511 0.50064745 1.49968362 1.49968362 0.50064745\n",
      "  2.49967511 1.49968362 0.50064745 1.49968362 1.49968362 1.49968362\n",
      "  1.49968362 0.50064745 1.49968362 0.50064745 1.49968362 0.50064745\n",
      "  1.49968362 0.50064745 1.49968362 1.49968362 2.49967511 0.5006523\n",
      "  1.49968362 2.49967511 0.50064745 1.49968362 0.50064745 0.5006523\n",
      "  1.49968362 0.50064745 0.5006523  0.50064745 1.49968362 4.49967268\n",
      "  1.49968362 0.50064745 2.49967511 0.50064745 0.50064745 2.49967511\n",
      "  1.49968362 0.50064745 5.49967238 0.50064745 1.49968362 0.50064745\n",
      "  1.49968362 0.50064745 1.49968362 0.50064745 0.50064745 1.49968362\n",
      "  1.50023484 0.50064745 2.49967511 1.49968362 1.49968362 1.49968362\n",
      "  1.49968362 0.50064745 1.49968362 1.49968362 0.50064745 1.49968362\n",
      "  1.49968362 1.49968362 1.49968362]\n",
      " [2.49935255 2.49935255 0.50032489 0.50032489 0.50032489 2.49935255\n",
      "  2.49935255 0.50032489 2.49935255 0.50031638 0.50031638 2.49935255\n",
      "  0.50032489 0.50031638 2.49935255 0.50031638 0.50031638 0.50031638\n",
      "  0.50031638 2.49935255 0.50031638 2.49935255 0.50031638 2.49935255\n",
      "  0.50031638 2.49935255 0.50031638 0.50031638 0.50032489 4.4993477\n",
      "  0.50031638 0.50032489 2.49935255 0.50031638 2.49935255 4.4993477\n",
      "  0.50031638 2.49935255 4.4993477  2.49935255 0.50031638 0.50032732\n",
      "  0.50031638 2.49935255 0.50032489 2.49935255 2.49935255 0.50032489\n",
      "  0.50031638 2.49935255 0.50032762 2.49935255 0.50031638 2.49935255\n",
      "  0.50031638 2.49935255 0.50031638 2.49935255 2.49935255 0.50031638\n",
      "  2.49976516 2.49935255 0.50032489 0.50031638 0.50031638 0.50031638\n",
      "  0.50031638 2.49935255 0.50031638 0.50031638 2.49935255 0.50031638\n",
      "  0.50031638 0.50031638 0.50031638]]\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "#### The first document has a higher probability of belonging to topic 1, and the second and third documents belong to topic 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
