{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spotlight.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsA91g7DU8Jd",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification using Neural Networks\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "Text classification is the process of assigning tags or categories to text according to its content. It is an important process in Natural Language processing and Information Retrieval with broad applications in topic labelling and spam detection. Here I demonstrate how to use Machine Learning techniques to classify text. The problem we have is a binary classification problem, where we should assign text blocks to one category or the other. \n",
        "\n",
        "The problem is to classify whether a movie review, written by a critic is positive or negative. The dataset and techniques used for this problem are explained below with the results and discussion to follow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4EQ-cKJv1jZ",
        "colab_type": "text"
      },
      "source": [
        "## 2 The Dataset\n",
        "\n",
        "The Dataset we are using is the IMDB Large Movie Review Dataset. This dataset consists of 25000 highly polar movie reviews for traiing and 25000 movie reviews for testing. We will use this dataset to train a binary classification model to predict whether the given review is positive or negative.\n",
        "\n",
        "Keras provides a convinent way to import the dataset. All the reviews are tokenized and indexed to a number. Each review consists of a series of word indexes that range between 4 and 4999. Index 1 represents the start of a sentence. Index 2 represents unknown and out of vocabulary words. These indexes have been obtained by pre-processing the text data in a pipeline that cleans, normalizes and tokenizes each sentence first and then builds a dictionary indexing each of the tokens by frequency.\n",
        "\n",
        "We shall start by importing the dataset and other essential libraries first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddBZZVDIU4ev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "076ea455-c73c-4e70-aa94-487de848e596"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.python.keras.datasets import imdb\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorboard import summary as summary_lib\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sJ_oIRa_AJU",
        "colab_type": "text"
      },
      "source": [
        "Then we shall proceed by loading the data. Since each sentence is of varying length, we shall pad zeros at the end of each sentence to get a 25000x200 matrix in both the training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PwSThjkmz0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "095b9740-ba79-4f08-94e4-a0c7aefada9a"
      },
      "source": [
        "vocab_size = 5000\n",
        "sentence_size = 200\n",
        "embedding_size = 50\n",
        "model_dir = tempfile.mkdtemp()\n",
        "\n",
        "# we assign the first indices in the vocabulary to special tokens that we use\n",
        "# for padding, as start token, and for indicating unknown words\n",
        "pad_id = 0\n",
        "start_id = 1\n",
        "oov_id = 2\n",
        "index_offset = 2\n",
        "\n",
        "print(\"Loading data...\")\n",
        "(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(\n",
        "    num_words=vocab_size, start_char=start_id, oov_char=oov_id,\n",
        "    index_from=index_offset)\n",
        "print(len(y_train), \"train sequences\")\n",
        "print(len(y_test), \"test sequences\")\n",
        "\n",
        "print(\"Pad sequences (samples x time)\")\n",
        "x_train = sequence.pad_sequences(x_train_variable, \n",
        "                                 maxlen=sentence_size,\n",
        "                                 truncating='post',\n",
        "                                 padding='post',\n",
        "                                 value=pad_id)\n",
        "x_test = sequence.pad_sequences(x_test_variable, \n",
        "                                maxlen=sentence_size,\n",
        "                                truncating='post',\n",
        "                                padding='post', \n",
        "                                value=pad_id)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 200)\n",
            "x_test shape: (25000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5dZFL1q_1ZW",
        "colab_type": "text"
      },
      "source": [
        "We shall use the word index to take a look at how a sample sentence is after padding, before we proceed with building a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW2TZUQHnBlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8afd2302-05b5-48dd-ff89-778dfe5647fc"
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "word_inverted_index = {v + index_offset: k for k, v in word_index.items()}\n",
        "\n",
        "# The first indexes in the map are reserved to represent things other than tokens\n",
        "word_inverted_index[pad_id] = '<PAD>'\n",
        "word_inverted_index[start_id] = '<START>'\n",
        "word_inverted_index[oov_id] = '<OOV>'\n",
        "  \n",
        "def index_to_text(indexes):\n",
        "    return ' '.join([word_inverted_index[i] for i in indexes])\n",
        "\n",
        "print(index_to_text(x_train_variable[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <OOV> is an amazing actor and now the same being director <OOV> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <OOV> and would recommend it to everyone to watch and the fly <OOV> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <OOV> to the two little <OOV> that played the <OOV> of norman and paul they were just brilliant children are often left out of the <OOV> list i think because the stars that play them all grown up are such a big <OOV> for the whole film but these children are amazing and should be <OOV> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <OOV> with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mIBO3UgAPb3",
        "colab_type": "text"
      },
      "source": [
        "There's one more thing we need to do get our data ready for TensorFlow. We need to convert the data from numpy arrays into Tensors. Additionally, We define two input functions: one for processing the training data and one for processing the test data. We shuffle the training data and do not predefine the number of epochs we want to train, while we only need one epoch of the test data for evaluation. We also add an additional \"len\" key to both that captures the length of the original, unpadded sequence, which we will use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z8e525rnClF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])\n",
        "x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])\n",
        "\n",
        "def parser(x, length, y):\n",
        "    features = {\"x\": x, \"len\": length}\n",
        "    return features, y\n",
        "\n",
        "def train_input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))\n",
        "    dataset = dataset.shuffle(buffer_size=len(x_train_variable))\n",
        "    dataset = dataset.batch(100)\n",
        "    dataset = dataset.map(parser)\n",
        "    dataset = dataset.repeat()\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    return iterator.get_next()\n",
        "\n",
        "def eval_input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))\n",
        "    dataset = dataset.batch(100)\n",
        "    dataset = dataset.map(parser)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    return iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmWuGcZpBWya",
        "colab_type": "text"
      },
      "source": [
        "## 3. Linear Classifier\n",
        "\n",
        "It's always a good practice to start any machine learning project trying out a couple of reliable baselines. Simple is always better and it is key to understand exactly how much we are gaining in terms of performance by adding extra complexity. So we shall start approaching this problem by building a basic linear classifier on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkpL9HwknKdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "274d117f-6e78-4fcb-dcfa-360d642f36e6"
      },
      "source": [
        "column = tf.feature_column.categorical_column_with_identity('x', vocab_size)\n",
        "classifier = tf.estimator.LinearClassifier(feature_columns=[column], \n",
        "                                           model_dir=os.path.join(model_dir, 'bow_sparse'),\n",
        "                                           config=tf.estimator.RunConfig(log_step_count_steps=5000))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpt078hbkc/bow_sparse', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa4d4066f28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPlf1CPlKQg5",
        "colab_type": "text"
      },
      "source": [
        "We shall now write routines to train the classifier and evaluate it, which we can use for all the classifiers we are gonna build."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C1NpPb-qMlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_classifier(classifier, steps=25000):\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "    classifier.train(input_fn=train_input_fn, steps=steps)\n",
        "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "def evaluate_classifier(classifier):\n",
        "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
        "    for result in eval_results:\n",
        "      print(result,' : ',eval_results[result])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aysm3i64fWWr",
        "colab_type": "text"
      },
      "source": [
        "Now, we shall train our Linear Classifier on the training Data for 25000 steps and try evaluating it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u3DMcesqR3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "62674c93-b084-4796-9452-275fd70d397f"
      },
      "source": [
        "train_classifier(classifier)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:Entity <function parser at 0x7fa50ecfee18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <function parser at 0x7fa50ecfee18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-5-f1710b5bd6d3>:14: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/feature_column/feature_column_v2.py:305: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/embedding_ops.py:802: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpt078hbkc/bow_sparse/model.ckpt.\n",
            "INFO:tensorflow:loss = 69.31472, step = 1\n",
            "INFO:tensorflow:global_step/sec: 173.362\n",
            "INFO:tensorflow:loss = 10.596842, step = 5001 (28.843 sec)\n",
            "INFO:tensorflow:global_step/sec: 180.661\n",
            "INFO:tensorflow:loss = 16.541143, step = 10001 (27.681 sec)\n",
            "INFO:tensorflow:global_step/sec: 180.374\n",
            "INFO:tensorflow:loss = 18.412226, step = 15001 (27.719 sec)\n",
            "INFO:tensorflow:global_step/sec: 180.469\n",
            "INFO:tensorflow:loss = 14.042293, step = 20001 (27.708 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 25000 into /tmp/tmpt078hbkc/bow_sparse/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 11.801286.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjyvxgMaKBgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9e61f108-73ac-4db9-cfad-6e64116597a6"
      },
      "source": [
        "evaluate_classifier(classifier)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy  :  0.81628\n",
            "accuracy_baseline  :  0.5\n",
            "auc  :  0.88860565\n",
            "auc_precision_recall  :  0.8897927\n",
            "average_loss  :  0.673751\n",
            "label/mean  :  0.5\n",
            "loss  :  67.3751\n",
            "precision  :  0.8195264\n",
            "prediction/mean  :  0.4949526\n",
            "recall  :  0.8112\n",
            "global_step  :  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfK0MD2Df66f",
        "colab_type": "text"
      },
      "source": [
        "Since this is a simpler model, we would be able to inspect the most important tokens, which have more weights in our model. The graph below shows such tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3hIiTYZqTvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "6fd1eaff-235b-4560-f1e7-040f0668010e"
      },
      "source": [
        "weights = classifier.get_variable_value('linear/linear_model/x/weights').flatten()\n",
        "sorted_indexes = np.argsort(weights)\n",
        "extremes = np.concatenate((sorted_indexes[-8:], sorted_indexes[:8]))\n",
        "extreme_weights = sorted([(weights[i], word_inverted_index[i]) for i in extremes])\n",
        "\n",
        "y_pos = np.arange(len(extreme_weights))\n",
        "plt.bar(y_pos, [pair[0] for pair in extreme_weights], align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, [pair[1] for pair in extreme_weights], rotation=45, ha='right')\n",
        "plt.ylabel('Weight')\n",
        "plt.title('Most significant tokens') \n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAE9CAYAAAACk7UUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO2dedymc/XH32fGGMuMJcYyGNNg7Apj\nGZR9G2tF1mmxRRTGEqmoHxEipDSokEiWUGQpKkJGWbIka9qp+JFfZTm/P865uq+5Pc9zX/ez3c8z\n1+f9ej2v576W+3uf63t9r+8533PO93uZuyOEEKJ+jOi0AEIIITqDFIAQQtQUKQAhhKgpUgBCCFFT\npACEEKKmSAEIIURNkQIQcxxm9oqZTeqHcvYys5tL2xua2W+z/J3N7EYz+2Bff2cwMbOJZuZmNlen\nZRGdxzQPQPQGM3sGGA+Md/cXSvt/BbwTeLu7P9OH8h1Ywd2f6KOo/YaZ/Qi4zt3PGsTfvB34lrtf\n0M3xicDTwCh3f71CeW2dL+ZsNAIQfeFpYI9iw8xWB+brnDgDzrLAw50WQoj+QgpA9IVLgA+Utj8I\nXFw+wcwWNLOLzex5M3vWzD5lZiPy2PJm9hMze8nMXjCz7+T+n+bXH0h3y27NP9zdd/OYm9ny+XkR\nM7vezP7XzO41sxPN7I6mcw9M186LZnaumVke+1Bxrpk9CUwCrk+ZRpvZ7Wa2X6ms/c3sUTN72cwe\nMbO1cv8xZvZkaf97St/5kJndYWanm9k/zOxpM9s2j50EvAv4cv7ml7u4B0VdvZjnTDWzEVnPz5rZ\nX7P+F+zqBprZ+8zsGTNbLb9XyPo3M7vCzN6W5xWuow+a2e+yzo8rlbOumc3Kev6LmZ3R1e+JIYa7\n609/bf8BzwBbAL8BVgZGAr8nrGQHJuZ5FwPXAmOBicDjwL557DLgOMIQmQfYqFS+A8v38PuVvgtc\nnn/zAasAzwF3NJ37fWAhYALwPLBNHvtQ07nPAFuUtm8H9svPuwJ/ANYBDFgeWLZ0bHzKuhvwT2DJ\n0m+8BuyfdXgQ8Eca7tn//kY39TAxr2Gu0r59gCcIhTUGuBq4pPl84MN5XlFXhwJ3A0sDo4GvAZc1\nfe98YF7gHcC/gZXz+F3A9Pw8Bli/021Uf63/NAIQfaUYBWwJPEp0ggCY2Uhgd+BYd3/ZIybwRWB6\nnvIaoTDGu/u/3P0OqtPyu/n77wOOd/dX3f0R4KIuyjrF3V90998BtxExjHbZDzjV3e/14Al3fxbA\n3b/r7n909zfd/TvAb4F1S9991t3Pd/c3Ur4lgcV7IUPBXsAZ7v6Uu78CHAvs3hT4PQw4CtjEG3GW\nA4Hj3P337v5v4ARgl6bvfdbd/8/dHwAeIBQBxP1Y3swWdfdX3P3uPsgvBgkpANFXLgH2JCzZi5uO\nLQqMAp4t7XsWWCo/H01Yy78ws4fNbJ82frfKd8cRlu5zpX3PdXHen0ufXyUs2HZZBniyqwNm9gEz\nuz9dTC8CqxF185bfd/dX82NvZCgYz1vrfC5mVypHAee6++9L+5YFrinJ+SjwRtP3uqurfYHJwGPp\natu+D/KLQUKpYKJPuPuzZvY0MI3oBMq8QMNSfyT3TSBHCe7+Z8L1gZltBNxqZj/1Cpk/Fb/7PPA6\n4dJ4PPct0/ZFVuM5YLnmnWa2LOE22Ry4y93fMLP7CeVVhVZpel0d/yNR5wUTiHr4C1EXAFsBPzSz\nP7v7VaVr2Mfd7+ziOib2KIT7b4E9Mr7zXuBKM1vE3f/ZQn7RQTQCEP3BvsBmzQ97ujSuAE4ys7HZ\nGc4AvgVgZruaWdEh/YPozN7M7b8QPuwuafHd8u9fDZxgZvOZ2UrMHrTuTy4AjjSztS1YPq93/pTt\n+ZT7w8QIoCo91kOW+2bTOZcBh5vZ281sDPB54Ds+e9rnw8A2wLlmtmPuO4+4V8umrOPMbKcqQprZ\n3mY2zt3fBF7M3W/29B3ReaQARJ9x9yfdfVY3hz9GBD2fAu4Avg18PY+tA9xjZq8A1wGHuvtTeewE\n4KJ0R7y/i3J7+m6ZQ4AFCdfFJUTn+O82L7El7v5d4CTi+l4Gvge8LeMOXySCpH8BVgfeYmH3wFmE\nH/4fZnZ2F7/7av7unVlX6xP1ewmRIfQ08C/iPjR/9wFge+D8zDw6i6jLm83sZSIgvF5FObcBHs77\ncRawu7v/XxvXKTqAJoKJWmFmXwCWcPdhNYNXiIFAIwAxR2NmK5nZGumWWZdwV13TabmEGAooCCzm\ndMYSbp/xhAvmi8S8BCFqj1xAQghRU+QCEkKImiIFIIQQNWVYxQAWXXRRnzhxYqfFEEKIYcV99933\ngruPa94/rBTAxIkTmTWru3RzIYQQXWFmz3a1Xy4gIYSoKVIAQghRU6QAhBCipkgBCCFETZECEEKI\nmiIFIIQQNUUKQAghaooUgBBC1JRhNRFMCCGGK2fe8njrk3rg8C0n95MkDTQCEEKImiIFIIQQNUUK\nQAghakrHFYCZjTSzX5nZ9zstixBC1ImhEAQ+FHgUWKDTggghhj99CbY2B1r7s6yhSEcVgJktDWwH\nnATM6KQsQojOMBSzY+pCp11AXwKOBt7ssBxCCFE7OqYAzGx74K/ufl+L8w4ws1lmNuv5558fJOmE\nEGLOp5MjgA2BHc3sGeByYDMz+1bzSe4+092nuPuUcePe8kYzIYQQvaRjCsDdj3X3pd19IrA78GN3\n37tT8gghRN3odAxACCFEhxgKaaC4++3A7R0WQwghaoVGAEIIUVOkAIQQoqYMCReQEGJ4MafPkK0L\nGgEIIURNkQIQQoiaIheQEDVBbhvRjEYAQghRUzQCEGIII6tdDCQaAQghRE2RAhBCiJoiBSCEEDVF\nCkAIIWqKFIAQQtQUKQAhhKgpUgBCCFFTpACEEKKmaCKYEP1IXyZugSZvicFFIwAhhKgpUgBCCFFT\npACEEKKmSAEIIURNkQIQQoiaIgUghBA1RQpACCFqihSAEELUFCkAIYSoKVIAQghRU6QAhBCipkgB\nCCFETZECEEKImiIFIIQQNUUKQAghaooUgBBC1BQpACGEqClSAEIIUVOkAIQQoqZ0TAGY2TJmdpuZ\nPWJmD5vZoZ2SRQgh6kgnXwr/OnCEu//SzMYC95nZLe7+SAdlEkKI2tCxEYC7/8ndf5mfXwYeBZbq\nlDxCCFE3hkQMwMwmAmsC93RWEiGEqA8dVwBmNga4CjjM3f+3i+MHmNksM5v1/PPPD76AQggxh9JR\nBWBmo4jO/1J3v7qrc9x9prtPcfcp48aNG1wBhRBiDqaTWUAGXAg86u5ndEoOIYSoK50cAWwITAc2\nM7P7829aB+URQoha0bE0UHe/A7BO/b4QQtSdjgeBhRBCdAYpACGEqClSAEIIUVOkAIQQoqZIAQgh\nRE2RAhBCiJoiBSCEEDVFCkAIIWqKFIAQQtQUKQAhhKgpUgBCCFFTpACEEKKmSAEIIURNkQIQQoia\nIgUghBA1RQpACCFqihSAEELUFCkAIYSoKVIAQghRU6QAhBCipkgBCCFETZECEEKImiIFIIQQNUUK\nQAghaooUgBBC1JRKCsDMvlBlnxBCiOFD1RHAll3s27Y/BRFCCDG4zNXTQTM7CPgoMMnMHiwdGgvc\nOZCCCSGEGFh6VADAt4EbgZOBY0r7X3b3vw+YVEIIIQacHhWAu78EvATsYWYjgcXzO2PMbIy7/24Q\nZBRCCDEAtBoBAGBmhwAnAH8B3szdDqwxMGIJIYQYaCopAOAwYEV3/9tACiOEEGLwqJoF9BzhChJC\nCDGH0CoLaEZ+fAq43cx+APy7OO7uZwygbEIIIQaQVi6gsfn/d/k3d/4JIYQY5rTKAvrsYAkihBBi\ncKmaBXQ9kfVT5iVgFvA1d/9XfwsmhBBiYKkaBH4KeAU4P//+F3gZmJzbvcLMtjGz35jZE2Z2TOtv\nCCGE6C+qpoFu4O7rlLavN7N73X0dM3u4Nz+cE8vOJdYZ+j1wr5ld5+6P9KY8IYQQ7VF1BDDGzCYU\nG/l5TG7+p5e/vS7whLs/5e7/AS4HduplWUIIIdrE3Jtd+12cZDYNOA94EjDg7cQicbcD+7v7l9r+\nYbNdgG3cfb/cng6s5+6HNJ13AHAAwIQJE9Z+9tln2/0pAM685fFefa/g8C0nD0hZfS1vqJbVXF5d\n6kyIoYiZ3efuU5r3V3IBufsNZrYCsFLu+k0p8Nt2598O7j4TmAkwZcqU1tpKCCFEJVpNBNvM3X9s\nZu9tOrScmeHuV/fht/8ALFPaXjr3CSGEGARajQA2Bn4M7NDFMQf6ogDuBVYws7cTHf/uwJ59KE8I\nIUQbtJoIdnz+/3B//7C7v56rjN4EjAS+7u69yigSQgjRPlUngi0OfB4Y7+7bmtkqwFR3v7AvP+7u\nNwA39KUMIYQQvaNqGug3CUt9fG4/TiwRLYQQYphSVQEs6u5XkC+DcffXgTcGTCohhBADTlUF8E8z\nW4RcD8jM1kfvBxBCiGFNqzTQw4CfA0cD1wKTzOxOYByw68CLJ4QQYqBoFQRempjotRLwGHAL8FPg\nMnd/YYBlE0IIMYC0SgM9EsDM5gamABsAmwDHmtmL7r7KgEsohBBiQKi6Gui8wALAgvn3R+ChgRJK\nCCHEwNMqBjATWJVY+/8eIh5whrv/YxBkE0IIMYC0ygKaAIwG/kws1/B74MWBFkoIIcTA0yoGsI2Z\nGTEK2AA4AljNzP4O3FUsFSGEEGL40TIG4PHCgF+b2YtE7v9LwPbEC12kAIQQYpjSKgbwccLy3wB4\njYgB/Bz4OgoCCyHEsKbVCGAi8F3gcHf/08CLI4QQYrBoFQOYMViCCCGEGFyqrgUkhBBiDkMKQAgh\naooUgBBC1BQpACGEqClSAEIIUVOkAIQQoqZIAQghRE2RAhBCiJoiBSCEEDVFCkAIIWqKFIAQQtQU\nKQAhhKgpUgBCCFFTpACEEKKmSAEIIURNkQIQQoiaIgUghBA1RQpACCFqihSAEELUFCkAIYSoKVIA\nQghRU6QAhBCipnREAZjZaWb2mJk9aGbXmNlCnZBDCCHqTKdGALcAq7n7GsDjwLEdkkMIIWpLRxSA\nu9/s7q/n5t3A0p2QQwgh6sxQiAHsA9zYaSGEEKJuzDVQBZvZrcASXRw6zt2vzXOOA14HLu2hnAOA\nAwAmTJgwAJIKIUQ9GTAF4O5b9HTczD4EbA9s7u7eQzkzgZkAU6ZM6fY8IYQQ7TFgCqAnzGwb4Ghg\nY3d/tRMyCCFE3elUDODLwFjgFjO738zO65AcQghRWzoyAnD35Tvxu0IIIRoMhSwgIYQQHUAKQAgh\naooUgBBC1BQpACGEqClSAEIIUVOkAIQQoqZIAQghRE2RAhBCiJoiBSCEEDVFCkAIIWqKFIAQQtQU\nKQAhhKgpUgBCCFFTpACEEKKmSAEIIURNkQIQQoiaIgUghBA1RQpACCFqihSAEELUFCkAIYSoKR15\nKbwQQ4nDt5zcaRGE6AhSAGJYok5biL4jF5AQQtQUjQDEoCCLXYihh0YAQghRU6QAhBCipsgFJLpF\nbhsh5mykAOYw1GkLIaoiF5AQQtQUKQAhhKgpUgBCCFFTpACEEKKmKAg8BFDgVgjRCTQCEEKImqIR\nQC+QxS6EmBPQCEAIIWpKRxWAmR1hZm5mi3ZSDiGEqCMdUwBmtgywFfC7TskghBB1ppMjgDOBowHv\noAxCCFFbOqIAzGwn4A/u/kAnfl8IIcQAZgGZ2a3AEl0cOg74JOH+qVLOAcABABMmTOg3+YQQou4M\nmAJw9y262m9mqwNvBx4wM4ClgV+a2bru/ucuypkJzASYMmWK3EVCCNFPDPo8AHd/CFis2DazZ4Ap\n7v7CYMsihBB1RvMAhBCipnR8JrC7T+y0DEIIUUc0AhBCiJoiBSCEEDVFCkAIIWqKFIAQQtSUjgeB\nBwst4SyEELOjEYAQQtQUKQAhhKgpUgBCCFFTpACEEKKmSAEIIURNkQIQQoiaIgUghBA1RQpACCFq\nirkPn3esmNnzwLMDVPyiQH+9k2ColtXf5dWhrP4ub6iW1d/l1aGs/i6vv2Urs6y7j2veOawUwEBi\nZrPcfcqcXFZ/l1eHsvq7vKFaVn+XV4ey+ru8/patCnIBCSFETZECEEKImiIF0GBmDcrq7/LqUFZ/\nlzdUy+rv8upQVn+X19+ytUQxACGEqCkaAQghRE2RAhBCiJpSCwVgZlb+P6diZvN3WgYhxPBhjlcA\nZmbeCHQs2VFhmjCzEf2llMxsInCnma3RH+X1F83X19frNbMRpc9ztEIfivRnnZfv5VCiq2vsx+d0\n+f4op78YkjegPyk6fzM7ALjWzEb3cyPuVVlmNg+wSRaxvZnt0RcZ3P0Z4FvATDNbsbdlFeX15ftN\nchX1vxU07kcvy5sbeL+ZzWdmqwMH9UXW/lZOWca2ZnZUX8orjVgXNrOF+ihPv7b10v2c2Mey5gXe\nYcHU/jBcurifbfdvTdc42cyWgmi3/WC87A98wszG9KWc/mSOVwAAZrYNsAfwHnf/N9D2jSw9lBua\n2fvMbHMzm6sPHdp/CAVwK3A68OfeFNI0wpkF/A24yMxW6mt5Zraxma1mZpN6U1apnI8Bp5nZcuXf\n6UV5/yHu3W+Aq4Cb+6JQSvJNNbNxwHztltHFdSwHLFguvzdymdkOwPeA883snN6U03QvlzGzBXqQ\nu5Jc+d3DgQvNbJHeyJUsDOwEXABcTh/7ouJa8/n8acr7ZrtKoKnNng980sxuLB/rpXxbA+sDp7r7\nK0Nl9DpHKoBy5ZrZXMAywLrAxhANo90ys3HtCHwJeDvwKWC/3sqXMlwEjAeeAH5uZiOb5a8iV37n\nMOALRMf4CPBtM1u1XdlK5R0JHA98BDjFzN7ZbllZzqbAB4FN3f1JM1vDzBbqw8M0C/gn8Aa5bkre\n4+L32nqwzOwjxMjpG8B+ZSVV4bsLlOprdTNbGbgHWKKLc3uUy2Z3ba0HfBLYC7gL2MrajO+Y2Yim\nDvsm4Etmdgz03qI1sw8CuwG7u/vfzGxcbyxad/8j8CjwfuBaos32mryezYCtgNXM7LbcX0kJ5Iik\n+LwtsDOwPfASMJurtp16s3DzjgL2B9YDJpjZyL4ok/5kjlMATVbPUsD8wNeBjwN75WigalnzlTrl\n+YlRxJbA74F5gWuK4+3Kl9bY01nes8C5wIQ8raVlZWYrmtlapV2rAoe5+9eJxnYV4Q5auapcpc+r\nEx32ZoADI4EHzWx0lbKaeJPotLcys1OJzvYma8OFUBp9rZe7pgFfBm41s1Xc/XUzm1hlRNZ0neOA\n1YF1gC8CSxEuppYjHgtf7iFmNo+ZjQdOBC4FjgP2MbOjzGwvM9scerYezWxxwp1VtKWRwKnABkQH\nuY27/9PacJMURo6ZrQ+sDOxOGBzrmtkJhUwVFFPz8VGE1f5uM/sE8CPgODOb8JYvd13eqFK5PwR2\nAUYDBxf13q6yy++8g2xbhKX9dzO7F1orATNbBdi9VP8vEe1rX2AKsEPW1RZZXjud9+Lu/hpR/z8F\n3gMs3tbFDSTuPkf+AUcRlsUthBU7kbBErwV2rPD9BYCrgR2IB3I08QCdRtzI5fO8rYE125RtGnAn\n8Flg7Sx/JvBVYlTxFLF6X3ffnzevaRFgXO67HDildM7awP3EAzp3C3neCVwPzJPbawBfAz4H/KC0\nf0tggYrXuClwBGFkfCP/tsxjFxIWZDt1ti3wO2Bq0z1+GPhA3ucVWpRhpc8fB04B7izt2yLv7/8A\nb++hnFHA2Kz/dwLLlY6tTCi8D+U9uQSY0EKuFYGViJHDIsBUwiK+o6hvYHPCJbRYG3W2PPAycEZu\njwbWIoyDUyt8v1xfuxBumx3yXt5KdGZb5/1cvkVZS5Y+75hlHACMA1YhOu+PEp3u1cBCbbaPVYDz\nmvbNAm7t6npK+0YAKwCLAZOBScCawD+Au0vnfQi4rGr7z+8cTCi5c4CjgbmzTZwJLNPO9Q3UX8cF\nGJCLikZ6U36+HrgkPy9EdJzfIUYGb2kQ3dzAbXL7AKIT2jq3Nyb80ZUVQD6AVxLD+xOBs4lh6wjg\nWKLj3bmH748ofV4xG9MahJvrQeCYPLYn0cGNryjXDcA12UkYcAXwy1IHdAChtBbuSa68jlHEEPpS\n4INN5+0MPARMbKPOJuR31sntVYnh9EL5YN4ATGujvJ2AXxCK+BHg2tKxaXlfFunmu8tn3UzI7ZOJ\nDrCQbSRwY7aNEeX71UVZSwCfLm2fRSggy7bweN7bPQlFt30b17gLkfX2oSxnlZJ86+W9WbRiWYcS\nxkRh9CwIzFuqr3tp0aEBPyYU2HjgNuDTxCjnG4RLdZVsrz8CdqkgkzX9n5TXuVHpnAPzmbiomzJW\nzN9cjMbI5jNE37ALMdLfNe/Fr4DV2qj/3fI6xwMXA1fk/vmB6/LaR1Ytb6D+OvrjA3ZRYanuS2jd\nHwKjc/+yeaMX7KlhMXsnu3823q0JS+HobAwnEz7Mdh7KZQh//wm5vQRh9ZxNjkqAUeWG3Sxb6fP7\n8rsnZqexJhGAfJCwVJ4AVm0hj5UbIWF5XU8EQ3dMua7KB+DBKg8A2XESI6hpROc4I/dtR1hllR+k\n/N6ChKL7DGFN3UYM9T+Ux8d2V2ddlDWV6Ig+Wtp3N3BNaXu+Hr4/gYi1XJodx+KEv/6rNJTA6cCu\nFWRZnTAGTsnt9YgY0/G5fVSW+20aRkeX19i8HzgG+FKpnIeLeidHtBXrfi0irlHU8fpEZz03sHc+\nC6tXKMcIA+I3ZAdPjJaOIly0Kxftpo17uV3WzUfzPkwD/g5MT9luz3POpqmzTXk2yvo+kRhVr5vb\nnySMi+2z3Z0JrNRmm90129pHgJtpPNfLAPNQ0TAb6L+OC9DnC4C5mir9IGK4/DPCEps7j83Ih61q\nw59QNBqis/0R4SKYj8je2bL0wLdsrKVyPwf8hYZFNg44nIgBLFGxjI2Bn+bn8USw9jTCjTAKeBvt\nuQrGlT5fSiiCscDShPW3L7BiizJGEu6QN4C1ct+ChJvgRuCg3NfyGmlYdcsRnc0IYB/gDGDbPHYI\ncHZ+7snKbu4Y16KRebJWaf9jwGXd3U9mV75LAydlGYvnPfwU4TdeFdisSoeR92oNwv1xckm+cwgL\nef7cV6nN5rk75r1bjrBu35H7Pwn8kexo26ivdQiL9cC8vp8CvyUMjtVo7d5qLu8e4Oel7clZd98i\njIZKzxJhvd9CGCefJ9yzSwHvzmfpiqzbbQlX2thu7uX+ee7xhBJYi1AYx9DGM1Qqb0pex3uB/wV+\n3PRbZ5LKYCj8dVyAPgkfN+sEGn7wzxOpnhCW52mENfAR4AF6sIiJDv9j+XlrYsj73bxh4wi3wc1E\nQKiqfEVHti5hkaxGDAEPA+6joQQWo2eff7nBbp8N/+TSvknZYXwV2KBivU3Jz4fkdZ0H7JT7LiGU\nQI8dT1cPK+Fb/yvwztweTcRdvkEPI68uytmOGHV8PR/gSaVj6xKW59ZV5SM65Y2yrpbItnFCIWee\nM7GrMph9RFgYBQsTluPlef/GEaPCM6g4tC+uiVByl9BQAmsSrsAvEJZ2twquqbzxwJNEh70h4Xq4\nsHT8CErxihb1tWbRJoEjiU7xXbl9GnBwG+1/HeBTpf33ky6R3F6RHmIuec44Mi5AxLeeAXbL7cmE\nEfVNcjRCGA2bEMqqyxFnfucnRF9xA9F/zE88Hxfm8VFdtfNuypsBfL9UbycRz+oqhAK9nxaj8sH+\n67gAfRI+Aru3E8HU+YkOsBhejstKP4UIsK7Soqw1CLfJacSwcmo+REcSbpD5CCv0dir6TrPc7YFf\nE+6L+4G9So3lN901ztL3yw/l3kQHe3E+5BNKD9nywCdoYbVkgz6MGNEcTrjIpmTncBZwQJ53E3B5\nRbl2ynraIrenE0Px9Qn/9bWt5Goqe3nCzbMEYdE+RsYeCAv7e6SyalFOEZc4iIghnAn8ITuG5Qg/\n7GnAGj2UMab0+TDCCr6UUCQTiIDxt1LWRau2DcINcD+NAO2qWc6Jub02Laz1pvKWzf/HEsbFDtn2\nXwU+0+ZzdQihdE8n3DblUfZuhDupx6Bv6fzNiU7xdeArpf13AT+oWMbIvHdlI2AW8IvS9nLZ/i8l\nlPM8RCLC5OY2Syj1hQjDp2hXG+e9/TQxEngHkcFTtc42JVyJC5f2jc37cSXhlh1Snb/7MFUAlKwy\nQgncSPgBzyRcLOsRVse7CMVQ1YJ6J2ER3FHatyRhjW2f2z0OeZvKexth2S2enc4DRaPKRv0JSkGr\nFmVNJQPbuX0pMdRdptSw56pY1mLEqOge4LjcNyY7jW/RcJstXaGsQ4lg8WmEYjo363wfIoPop6Qb\noocy/mvl5ncXBj5GKLt7aFjKm+f/og6784eXO4rlgJ+Tli+hkJ8hlN7KhPHQZadNKJ8L8/PeRCxo\nPiI4+PncvxThN76QNoN62XbvKZW1CmFsfKHKM1D6PJlwgRyf298hjJ8VCMV3FSUXSBdllZXcJkSG\nzxjCLXJr6d5slHVZqSMjLOnH8rqmZP2VM9UeyHOq+PtHZX2dVrTzrLvrSuesQDfJBU31tRShIB4k\nPQa5/xjCWDuuikxN5e8AfKuQtYvjHQ/4dil3pwVoW+DZb2QRmFqacFn8gbDez8uH4FLa91GuQ/jo\nP1nadxZwbH6upEyKsolh5YWExTMx90+jZJlUKGONfJAvIwOU2YAvIjrdKh1183UWLoyngHVL+2+j\nBzcS0aGOLclwSfH7+aAfT7oH8je6DajmOXMR1tOuhNvnfwjr6weEhbxknrdBPpw91hth2V0OnJTb\nI1PGyTQCcYcCZxXX0E05ixCd30qEpf8lohM7mIwt0VC8C1OKo7SQb23CT10OCt5HukgIN2ErhVl2\nSY2m0TleTcQPDqYRAJ4ILNVDWZMJo2nN3F6DiPnMICzkIoGiiL28rYeyliCs6KJeNgGuLB2fCDwH\nfLlq22/aXppUIjRccbcDP2rjmTwk29ahxOj3IhpZfnsBX6E9y3+rbMNTs6xlaSjMPQlDa2TztQyV\nv44L0Jaws3f+hxBujKMIl8GC+QB8hrTounu4m8sjLJv3kdZ4PugPEopkY8JS2byqfPkgLFuS8z5g\nq9yeSqSrrVflOkv7phPD8r2GgSwAAByYSURBVHfRsNDnJdxbPQZWm+ptY8KHXgx9j8qHamci9vFg\nVx0GoYjmIyz8hYpGnZ3EZ0rn7UUGUyve0xGEFfhjIki5ae7fgVAAM4gg4a+pEH/Jh3EdQlkenfsu\nJHzzReriwcA53dV17h+bHcRl2a5Oymu9mkbn/RlyBNVG2ziKSFDYqFROETA8vs3nYX/C0DmdRpbQ\nxwhf+JsV62ttwsD5DBnUzfvwcOmcDxBunB5jOIRSXJFI65w/n4PLiXhCYbgcnfdyRqt2Ufq8MpmE\nQLh2v0eMBAolcA8Z02pR5s7EiHThbG8/IDroRwhD6mnac7sdRLjDlsprn0nESo4kPBKP02JuSqf/\nOi5Ar4SOyP7VxCzJLxKun3cQPtif0bCmqgwtpxG++H0Jf+n03D+FmAxyL6XJRxUb2S+JjvQgwt1y\nTnYkF2Vjq5Q6SnSmJ2RjWoCYiXxLdh6VM0NK5R1C5L+fSIwoiuD5/wD/ygery+wVGlbNKEKJnUJY\nnxsRVtM+eXwXIqti/gryFJ3iAvlgfj/rrJh4tinRWR8HbNLGPZ1OrOPyINFJjspr+zaRAVQpFZXo\nrF4hOu1xhDtjb8ItuDuhoCpl1TB7ptUBRIylMDg2IgK+m7RxL99PdKQbEAryHGC/PLYkMRLrVjZm\nNwrWJuIhJxKd9gbEaPrQLOc+KqR6lsqbSTyHcxFxk0uJCY67E6Oq6XmvuxwdZl0XGV4bZr3fm9e5\nODE6uzKvubJrhZgI+l5ibkTZnTqViFW0495dO9vRMqV9owmFcmI+Ez3GHYfCX8cFqFjZa9LogNYj\nXDRFts/KhO/uDMKHvwgVZ9nlg/ILwmrZkggCPwUcUvrddSqUUzzkI7JRrkG4Dh4nfOFjsqydaGTH\nVJmEdnc+hGcRLqS3ER3Q3bShlLK8bYiA3vxE7OFPWY9L5PH9qeZKWopwW9xGY/T1XsIKu4aYG9Fj\nZ8Fb/bEj8m8dIt5STGZbmB6yVrop+wPZYWxIBLmvIJTKCKKj3Y1SjKBFWcsSqb+PEcP5dxEW9+WE\nG6jSfAYi7nATYakX7XZfwgI9l4gpbNhTu8gOZzMa/u8ZwIH5eSxheHyTHnz93dR/0XYnEMbUiXlP\n1iIMgxNokdJaKqP8HHyZCP6PJIyCU4lMm7XyOm6ie/fbckT218VERz+JcC9+mVBIixPG3vWtZGsq\nd2MiS+pnpX0ziNFdj7Plu7jWdWn4/EfQGJUX92fIpHr2eD2dFqBipZ/D7MHTqwi3TGEprpCN9ZSe\nbiTR+a2Un9cglMV4ouO5L/fvQAyfp7cp47R8gK6k4etfnRgiHlvh+4WCKxrYeczum/8kcEF+Ppj2\nYxuL57V+GLgl911DrHtSNXNlbWBmfl4xH+IjCAt+LJHJ0qP/lPDjbpKftyM61ytppEBuSSiBywgL\nvt1lNg4DPpyfx2Z5PyfdQb1sf2sTxsEe2f56nEzY9N1Nsq0un+32DuDjeWwK0XFvUqGcg/M6Ns7t\n3QmjoHCNGGFdV7Y6iSD7BcTIaArhuvkCERivlOVTKmvrfAbPJJTJQsRzezWN53Q0EVh/iB4yr/Lc\n5Qml8RSN9M81CRfLyYTx1lYnSxhiZxCKeBPCWLiP9mb4Fm7EhYgRwH6lYx+hMalvSPr833I9nRag\nRWU3D1NvKW1/jfDJFpNllqOb6ful70zMBnkmYcEWD880GstFrEdYFhtWlY8YhfwiH4DvEYpgYh57\nB2F1TKrSKMhgJTFEPrq0fw3g6xXrrew/XYZSCmY+PPvn54MIl1IlKzsfwIdKdT6ZsGJPoZRJ0qKM\n6VlX7yeCs5tkx/MzGoHLYnLQNlXbR2lfsWzCUrk9D6HovkMb6btdlPsOYvXRj1Y4t9xuD8z2UcyC\nPohQnIdTYb2bpnt5Rt6vrbIDOobowNcnOta7qBjAJEZ8txLW/m3kcgl5necSBsc8FdvseoT/fGdi\nxHUu4VIZnff4RzQMnGl0n5dfPE+rEu6jiXnfvkkj8WBtwjjqlW+dUBwfyXZ7Me25tvYljJVTiZHk\n+sSo5izC8JjVTnlD4a/jAvRQ2YuRHReN9L9bCGvF8u/c7Ex6zDRpKncGYfX+T9HoyOEcMez873oi\nFRv/ukTq6N65vQExpDyVRvpit51jnr97fv4YjbkIJxGugcK3vhcRuFqoJ7mIIfa6+flwwvq8jkYW\n02GEj/ZL2WFUmZm7B9Fxv4eIY4ylMeSdRJudK2F5/TzvZaFMFs16vLDp3Cr34MOE6+KjxMjkyHzA\nVyEUzfVUzNJp8TurUV1ZbpS/vRXRyX6FRmLAD7OtVXJF5Xc+mp3PjdlG3k0olsOyXVxHaVJbhfIO\nIzrDw4lObO78m4ewvivN2yA66RMpBbCJ2Mk1+XkU7VnY2xKr4xazyVegkWa7YO5r6eaq8DujaGME\nQRgWDxEuwYMIxXY04SY8g4hTDXmf/1uuq9MC9FDhGxKWw8lEsK3ImLiBsDKM8L2dQQ/uECJzpZjh\nuhYxcjiIGPrtXTpvy3xgt2wh17w03FHLE375ByhNmiIsgzOy4c5DD4EqwjJ8mvC1XkCMZA4i/PTf\nJDIyziECfi3zr4mJLLcSaZWXZXlrpowz8iGfTlgtXVorvNV9dDRhbV5KuMd+SIxQPkV0RO2kxi6d\n925nIpaxNQ1lMo5QSqs3y9BDeYcSHeCOWd7RROzg08QI8Tba6Bj72GYLC3YDIhZyMTED+mpi7sF+\nhHV7By1SPZvKXZHZ55B8nBgxFRPv5qWHjLeu6pIwMh6nlLFFKJmj26j7xYjR30eJoPvypWO3VWmv\nTeWtQmlxRcJlWawW+mXC+BjZTnvrp/u6QD6Te+b2mGz3l1Ah2WEo/3VcgC4quzzk/Qrx5qytm875\nPjGMrtpQv0m4Be6hsejUNEKx7ES4SU6kyQ/fTVnrES6ejxOZEmMJ//qvmH2Sywa0WD+ndO6WhHVx\nfm6PJiyOT+QDNonW7q1yvR1OWNjlmZcr5fX2OCuU2d0XUwmrt8gWGkMoqeMIS+ijVFjVk0bHuGY+\nxIUPfA9CyW9FQwn0aJU1yTeGGCkZkeF0E2EUFD7nuWiRCjwA7Xfd7PzWz+1JxMjuSiI77G7gvW2U\ntzCR4nwFs69d9BXgeSpOJMzv7E2MJFch4mE/J3PyCffGw7QXVB1BGBk3ErGDYv38lYj4Tcu20VTe\navlsvY8wiO4jRnJTCTfhoM+kzTZ+aD6LD9BIMZ+f6IeGdJpny+vrtABNlV1+uA/KRnBKNqa1m879\nOi2yVkodz4pE1su9Tce3IdIyfwNs14ac5xNpk/uW9i1B+ADP7uW170SsoVO4g4oF0D5PC19xU70V\n1tMhhEtlbRqZCasRmUCL0kJ5ZsN/KOv5KRprB+1OI9DVjuW/Q/72DwgL+KOl8u7Oe9FKpvJ1foCw\n+i8i3IDXlu73/lXKG6A2vCWxIN4nc3tU3tvPEwqrcGu2XNWTUCZnEqPIr2RHVEy625UYDXe7qmRT\nWTtnW/8a4U7ZjUiCuJlwf/6Yii4MwjIvloZehLCETyViYDcTk7NaKrnS/VqMUHRFFt13U76FiZH0\nQYN9H1Ouj2TbKuJJnyMC+ROy/u+k9J6D4fjXcQF6qPh7ShV/HGE9LUMohmPaaFxjCG09KTufm0vn\nzJ3HW87KbXqY3k1YKlcQGUTFJJclCaul8mSSpt8oFkArK4HK/s7sIG4sOoWst+tTxsrpaYRv+R4a\na94fSPidl0sZiyyinkZKC5Q+L0LEb9bI7T2JdZuKNMYPkBZzxeucSlhfc+e13Ukj8+eDhPulrfTR\nfm6/O2V97ZHbG2e7aCdOMhfheribsKyXIEYRRXrkr3q6xqb2uhQxKiwy4N5PGDHlZRAqtbN8ls4h\nFO6HCeV0NPkOC8K4WKZV+yiVt31e4+VEcHcuZjdY7icznwb5Hs5LJHRsne33QMJL8Ke8DzfRIpNp\nOPx1XIAWFV+sWXM84UO9JhtLJZ8uYQVeSQzfipmStxMBs/WIoXo7i5RtnI2+WFf9KKKDnUxYuPvR\nx/xfIgj2HBVeitH0vU2JyTLjmvYfQ/iL1+rhu80+/2INo/E0Yi9foBE4b/Xyj3mzjhcvbf+I0hr5\nhDX1S0pxmArXaHnf/g6cnvsWI6yxWYTl+CuGwKJb2R7+N9vfFVR4C13pu/vld9Yi3IvXE1lg44is\nqY/Qg+uB2d2BRxAxoT8CH8l9CxFK4HLyhT3NbaCFfAsQcyJ+QSx2dhOR6daW4UO4BO8h4kL7ZxlF\nts+7CMXectG/AbyHB2R7uj7b/4H5f2EG2bU4YNfYaQFaVHzx5pwDs8OYTA9rkTSVsSHh01ybGCp/\nu3Ss8FtWeTVkMZJYjxhCX0VYYEV2ztH5ID0GvL+frn9LWmSI0DTfgVAclxYyU1JExKipy06b2S3F\nxQkLby5C2R5FY7r9DOBzFWQvsnoWIFxvH8rtfQl//Qa5vQGNt5D1tL5MVwHMY7O+J5b2LZAdSa9T\nPQegHe9IWLBHle5LFat4Z2IEcVfW2cFUGPV2Uc607LwWzGfqMRoZbgsTE/gqvYOim/LHE7Ggs4kZ\n090aGc3Xn21tWUIRFW7At+exdxCj3//OcejQ/ZuHGGG+Lbf3JIzGylmHQ/2v4wJUrPi9suLnrfDd\nooEVPuJ1CMu4cGcUa+AsVD6/i3LmLX1ej1AYxVrjexLD4EIJLFlqvAPeWAm31YGEpb4zYTG+k8jS\nWbN03nRyeeee6io/z8jO6uv5UC5CLM8wk0i3vY/WbxhbgOjQC3fApsDfiKDeUsR6Mz8k3D+PE0P8\ny+kmkNkk33sIN8Z22TkclfIu2+n22qJOtiLSeav4xHcjLP0xNBbHOzPb3psVOtipNGaxTyRGET8q\nHT+IcI9t1t9tldaL9JXv5RbEiHCPlOe/qdzECOcmevEylgG8hyMIA+Yh2nyb3VD/67gA/VXxNDr+\noiFtTVg8D9LIYtmBGEm0ekn6goQbqsg7nkYE9g7L7VGE1XI+EWztRLBxbyIL5KGsp9FEXOJkwve/\nDzECapmlQAQbv0mMlrbNB3AvwnWzHaFsqq7/vl92WNNye2MiiLxz1ts62RGtkh3Wo/SwWmWWcRjh\nKjiVyEK6NK/3mCx7SLxguwf5uxzRNbcbYv7CdwllfE5e9ygi9jKT1m9lm0AYIxNzeytiBHBE6ZzD\nCWU+b3+0W5oSAboqM+91sez4ZGJkX4wEjyZcnu8m3KuVFv0b5Ps3X8rWq9jeUP7ruAD9UfHM7qaZ\nReSRz5udxamEa2A9QhlUXYhtScKF8e7cfh/hAnp/bs+VneSgTf5gditqHaLzf4hG0HdpwnL8IpE1\nUuWdwOsRVnrh35+HmMR0Q7njqCBb4SpalMgGuYXGOxQ2JSz+D5fO35Dw8b4lkEYpHZfI+/4WjUlU\nS2T5xXpBn6PF26SG4l/Tvdwk78NiRKLD9Kz/V4DTKpS1GI241M7EjOXP5fa2RObPjNL5LWcg9+N1\nLpj3eStiVPlpYuRWVkoziJHOBTRWzR1SSykMNXn67bo6LUB/VTwR8L0gG9dzhKWxJhFA/gVhle7U\nqkxmD6AdSFjZxavwdqLNwOVA1AOzv8j94JSpePfrms3nlMugCz804VN/lEb2RrFG/3fzoa16D4qV\nUJehMXO5GAlsAfyORmbX/LRO451IzLP4JbMrj/eR6yIN9z8abyv7crbbTXL/ZMLt+X1Kb5nqpowV\niPTL4lWeG+V3P5PHtyZGTYV7aNA6MyJO8EMicH0e4d8/Ijv87ZrOHdQJXvobBgqg0kWE5fskjWHl\nCcS7QIt37i5KpiW26PytdH7xebcsu3kksGQnGixhLV1AZJcUk1KOIOYyfCY73e7ecFVeunY64Sra\nibCyZ/OppxJoGXMplffO/P5Kpe9/KeUsRgJFTKfLmdG8dVmM+7OMW4mAaKHA9yJiDWMGszMbgHs5\nKa+riB/tQmS7FYHaRan+opnTiayj4r3WqxKW96dze0v6EPDt43WeAbxGw4U6njA6TqWDWT76c+Zi\nmGJm5u5uZqsRyxrfRnTUuPsJZjYJuMnMprr7781sRB7z7srM8rYnrLI3zOwnRGf7OjDTzA5y96vM\n7Gfu/tcBvsS3YGYfJ/KmdyL8uNeb2Qfd/Ytm9jxhZR/i7i908d2FgO+b2ZdorPV+K+Hj341wtY0E\nbjezd7v7c8R1V+XfZM62me1K+HSfJwLVR5rZL3Ibd3+jmzIWBk42s1UJv/f7iE7y90Qc5kIz24FI\nEXyvu7/Shnwdp2izpV1/JbJ9RpjZKHe/0szeDrzPzO7s6j72wHnETNXDzezv7n6pme0M3GtmL7j7\nV/vvStrmPmJC54Fmdr+7325m3yBG2Jv04lpFPzFsFUB21u8mrIgjCD/oe4gHAWLIuypwXXZoLTsL\nM1uRsFb2JNwPKxAv6f6YmS0KfMPM1iQ7ssHEzEYRFuF0Imf6QWJSyg/MbEd3v9jMLnP317r4rrn7\ni2Z2BOESm5uYpPRbM1uSmPV7pLufZGbz0bt28RwRf/kgYY1eRXTUTwMPVlGY7v4DM/sP4R54wN2f\nNLPfEwpgcSIw+iBh1f6pFzJ2lKLzN7MJwEvu/pKZORE4PyFP+xfwRk+GSjdlPwE8YWYvAieZ2UvA\ny8RyD7f20yVUomScrUtkf/0yFdKTwAVm9iF3v8PMvkYkbajz7xSdHoL09o8I0F4G7JXb6xBzBz5P\nBJp+RbglvkrrNerLr4a8qrR/dWKly6m5XfmNQf18rRsQQdNRROrkT0vHnk0Zu5yAxlvfH7sWocA+\nW9q/A/DNfpK1WNNnHcJV1vJVml2UsRPxNrbdSvuupY01dIbSX7arIkYzg4i3XERYwPMQM9QvITJ9\nKr2trMXvbUMoyll0KHMl29SvCcX2II2F1PYF/ky6VPXX2b9hOwIgArzjgS3M7CZ3v9fM9iKCl0sR\n8wAWJTrON7sqoDQkn5twYTwMLJIWyjfd/SEz+zuhCO4iXCcDTtlVkK6rrQmr8E4zewF43sw2JUY9\nNxJr87zF8gdw9zeznAOJVUqfJfyvx5nZH9x9JjEzdJKZLQC8XPx2L3nDzNYmgprHuvuP2i3A3a81\ns+nA2Wa2MuFamkS4OIYjGwCHmNlhxDIbuxLuriOJtle4tZYBvuDuT/blx9z9h2b2y/jonRitrky0\n2c0Ig+U9hIsWd7/QzOYiUpdFh7G+PeuDh5mNdPc3zGw80Um9nJ3grkTHcIW7/6N0/gbEjN2d3f3X\nPZS7NaEsfkNYTPMSueuvEDnU3yCmy98zQJfWLWa2CbHswXNEWuXX3P18MzuWmDC0NhFEe7RFOcXq\nitOJ+QFPE+l5BxNxk98SHU+39dSm3PMTE3me7sLv3U45OxOupO8Dh7v7U/0h32BhZiNKCvgEov6v\ndfcZZjaayI//DPCou3+yc5L2nZLbZwMi7fSnxOhmFcLd+EzG1x7zcFd1FRMRg8yQVwBmtiwh5zNm\nNo0IJv2WmHW6C9FZb5H7Lnb3l/J7SxGLSj3bQ9lTiXXRzyesfIiXxdxIrB/0N+BGd792IK6tC3mK\nh8iIUcxdwD+JCU/PEDnvHyOG0KMJ/+mfK5T7SeA/7n66mc1NKIHliEyaTxOrLT7T/1fUd8xsY+DZ\noSpfdzSN4g4CXiXu6f7EEiQPmdlIwiV3KKHgBt1a70/MbD3CBXs6keb7KSK2dGs+a98EPtAJY0p0\nQ6d9UD39EYHYxwgf6mQira1I9ZxJBLfmJrJFzqWNGaHErMm7aMxQXJjwPZ9HphfS8GcPaqohufY/\n8Vq97xAL2J2Q/w/tRXk7Ez70VUr7bifiKKM7fZ/n5D8i/30Wb13ZtkhRHjmn3ANistcbxOz4+YjJ\niJcTy4s8QsVJmPobvL8h64dLK3gKMfx/ivCTPkcoBNz9ACLv+Sh3vwo4ySN1sSr/ISbg7GNmkz3c\nRzcQneIqHvwnf2tAh0l5rVjwTuDS9NlDuKBOIhbL+j8ipXLeNn/iduJa9zKzLc1sR8JC+5u7/7s/\nrkG8lbxP2xKd/mtm9hEiw2pBIoV3NXd/Y065B+5+M2GMfZww2g4mZqV/n0jW+H7R1sXQYEi7gDJ3\n/VHCL78nkS53sbt/L49/mJgoc2qFsgr3yurEsroPEIGo/YhA8fHAi4Tffzd3f2gALqlbufLzKHd/\nLYfS6xK57yOAu9z9hDxnCa/g9unid8YTqz/uSMQ3PuvuwzWoOmwwswOIeSWF8fIUMdp8Hbja+xjw\nHYqkr/9zxJLd3+60PKJ7hroCGEFYD6sRQ+l/EzNDLyc6sf2JEcAPK5a3NeHieZDIEDqbGJruSwSC\n7yHy/u8cjABVU+c/g7Ca/kGsy/OMma1P+FM3INZ2OaGvcmWev7n7P/vhEkQLzGweIr70pLv/3cz2\nJmIw2xQjzDkRM9uJWJhwC+DPnsFwMbQY0gqgIAPBPyLSF58g0suWAm7wCDC17BQtJnmdQriKZmWa\n4UaEf/IhYtg6hXhL1QuDmaFgMaHts8QoZFNiOeZd3f0Ri5mhOxDZI90GtMXQJo2ZDxMrfO7h/ZRx\nNZQxs3E+zAPbczrDQgEAWMzA/TZwjrt/pY3vjSDS0T5BrD9+ukfuO2b2OWKCzk6ZNfQxYo2f/QfL\nOktLaToxuevs3PeJ3LeHR7bICFlQw5scee0G3O0t0naFGCyGjQIAMLO1iMXFNgee6clCL/n8i/kD\nCxPZCYsQI4ebMzXtMGC6u//HYlmE13yQpqab2RSio1+DiHWc4Llkgpl9lgggbpQyDZ8bJbpEee9i\nqDGsFACAmY1195dbnFN0/tsQL055iFgl8zFiKv7m+XlNwt9+3QCL3SzXCHd/08z2IV6LN4rw898I\nXFQEec1sEXf/22DIJoSoH0M2DbQHWi7qlp3stkQQ6hLCwj6beOnGyYQyGA2cO1idfyFXflwu/19E\nzEB+mQhAbw4cbGaL5fnq/IUQA8awUwBVhtCZebEO4XMdQazq+S0iyLoRsYLoE8DaZrbRwEnbpWwT\ngFvMbLrHsshXEDN7lyHWInonMZlGCCEGlGHnAuqOkntlErFswkKElf8dIoX0t8SCVHPTWDDuvcA3\nfJDX9rdY0/6zxOv+Lst9txAjkwsHWx4hRD0ZzquB/peST317IpPnCHf/dU5++huxnvyaxCSck939\nVeC3ZvZFd2/npSf9grtfb2ZvAKfkbNEX89BF6vyFEIPFsFYAZjaPu/8rO/8pxMJu78/Of353/6PF\nC0W+Q7wc5jB3f6wYLXSi8y9w9xvM7J/ESOBVYtGsP3ZKHiFE/Ri2CiBTNrczsyvd/UUisHovULz5\nalszG0Ms8zCBeDHKk0MpFc/df2Kxwqm7+/91Wh4hRL0YdkFgiPRIwn//S8DN7B3EQm5LE+vHv0rM\nunwc2Njdny7WXBkqnX+Bu7+qzl8I0QmG3QggVxOcRqR2/ovw+f8NuMDdNzOzMe7+ipmtQSzt8MXO\nSSuEEEOXYZsFlG6eccBfiXcF/BW4jnh94PpEjv1h7n59x4QUQoghzLAbAcB/V/XckXBhPU9MoppM\nvAx7LiLlc7dc9G3I+PyFEGIoMexiADlLtniN4buIVULHEks8rwhsB7zq7rNg6Pn8hRBiqDDsFADw\nGmHlL5rbMwlX0A7E2v7fbbVWkBBCiGGoADxe3XgFsEm+Uu814Goi8+c77v5gRwUUQohhwrAMApvZ\n0sCBxGsT7wV2AQ5291s7KpgQQgwjhqUCgFgWGphKvC7yPnf/SYdFEkKIYcWwVQBCCCH6xrCLAQgh\nhOgfpACEEKKmSAEIIURNkQIQQoiaIgUghBA1RQpACCFqihSAEELUFCkAIYSoKf8PA5Q5rgVYUVUA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80wiQLnCgZ9d",
        "colab_type": "text"
      },
      "source": [
        "## 4. Neural Networks Classifier.\n",
        "\n",
        "We saw that the linear classifier performed quite well on the dataset. We shall move ahead and try getting a better accuracy from other classifiers. To make the dataset more complex, we shall also try adding word embeddings. Embeddings are a dense low-dimensional representation of sparse high-dimensional data. This allows our model to learn a more meaningful representation of each token, rather than just an index. We can add word embeddings by converting our existing feature column into an embedding_column. The representation seen by the model is the mean of the embeddings for each token "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MybxYEiBIs1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embedding_column = tf.feature_column.embedding_column(column, dimension=embedding_size)\n",
        "neural_classifier = tf.estimator.DNNClassifier(\n",
        "    hidden_units=[100],\n",
        "    feature_columns=[word_embedding_column], \n",
        "    model_dir=os.path.join(model_dir, 'bow_embeddings'),\n",
        "    config=tf.estimator.RunConfig(log_step_count_steps=5000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWf-jqTorMsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "2cc928b5-7307-435c-fdcd-47a0dd492936"
      },
      "source": [
        "train_classifier(neural_classifier)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7fa4ba71b438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7fa4ba71b438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/feature_column/feature_column_v2.py:3079: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpt078hbkc/bow_embeddings/model.ckpt.\n",
            "INFO:tensorflow:loss = 69.34512, step = 1\n",
            "INFO:tensorflow:global_step/sec: 133.614\n",
            "INFO:tensorflow:loss = 16.770742, step = 5001 (37.426 sec)\n",
            "INFO:tensorflow:global_step/sec: 138.552\n",
            "INFO:tensorflow:loss = 3.8213756, step = 10001 (36.089 sec)\n",
            "INFO:tensorflow:global_step/sec: 132.694\n",
            "INFO:tensorflow:loss = 1.1921632, step = 15001 (37.678 sec)\n",
            "INFO:tensorflow:global_step/sec: 131.668\n",
            "INFO:tensorflow:loss = 0.54301333, step = 20001 (37.976 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 25000 into /tmp/tmpt078hbkc/bow_embeddings/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.33573103.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHEpsmA_r48b",
        "colab_type": "text"
      },
      "source": [
        "Now we shall see how this new classifier fared against the Linear Classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjJrvE52r0yg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bb663795-0d15-4bf0-a0ba-2c8b45d9258e"
      },
      "source": [
        "evaluate_classifier(classifier)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy  :  0.81628\n",
            "accuracy_baseline  :  0.5\n",
            "auc  :  0.88860565\n",
            "auc_precision_recall  :  0.8897927\n",
            "average_loss  :  0.673751\n",
            "label/mean  :  0.5\n",
            "loss  :  67.3751\n",
            "precision  :  0.8195264\n",
            "prediction/mean  :  0.4949526\n",
            "recall  :  0.8112\n",
            "global_step  :  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEnWzb1xk00k",
        "colab_type": "text"
      },
      "source": [
        "## 5. Connvolutional Neural Networks\n",
        "\n",
        "We saw that neural network had a better accuracy than the linear classifier. We shall make our model more complex by building convlutional neural networks for this problem. Convolutions are one way to take advantage of this structure, similar to how we can model salient clusters of pixels for image classification. The intuition is that certain sequences of words, or n-grams, usually have the same meaning regardless of their overall position in the sentence. Introducing a structural prior via the convolution operation allows us to model the interaction between neighboring words and consequently gives us a better way to represent such meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPsshT7_qfQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "head = tf.contrib.estimator.binary_classification_head()\n",
        "\n",
        "def cnn_model_fn(features, labels, mode, params):    \n",
        "    input_layer = tf.contrib.layers.embed_sequence(\n",
        "        features['x'], vocab_size, embedding_size,\n",
        "        initializer=params['embedding_initializer'])\n",
        "    \n",
        "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
        "    dropout_emb = tf.layers.dropout(inputs=input_layer, \n",
        "                                    rate=0.2, \n",
        "                                    training=training)\n",
        "\n",
        "    conv = tf.layers.conv1d(\n",
        "        inputs=dropout_emb,\n",
        "        filters=32,\n",
        "        kernel_size=3,\n",
        "        padding=\"same\",\n",
        "        activation=tf.nn.relu)\n",
        "    \n",
        "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
        "    \n",
        "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
        "    \n",
        "    dropout_hidden = tf.layers.dropout(inputs=hidden, \n",
        "                                       rate=0.2, \n",
        "                                       training=training)\n",
        "    \n",
        "    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
        "    \n",
        "    if labels is not None:\n",
        "        labels = tf.reshape(labels, [-1, 1])\n",
        "        \n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    \n",
        "    def _train_op_fn(loss):\n",
        "        return optimizer.minimize(\n",
        "            loss=loss,\n",
        "            global_step=tf.train.get_global_step())\n",
        "\n",
        "    return head.create_estimator_spec(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        mode=mode,\n",
        "        logits=logits, \n",
        "        train_op_fn=_train_op_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0XnpC6Eq_9e",
        "colab_type": "text"
      },
      "source": [
        "Now that we have written the classifier object, we shall proceed by intializing an instance of the classifier and passing it to the training function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDLJhd0mqoj9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "76b3cb90-56ea-43d0-ab0b-cd796f111927"
      },
      "source": [
        "params = {'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0)}\n",
        "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
        "                                        model_dir=os.path.join(model_dir, 'cnn'),\n",
        "                                        params=params,\n",
        "                                        config=tf.estimator.RunConfig(log_step_count_steps=5000))\n",
        "train_classifier(cnn_classifier)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From <ipython-input-18-b7034db0ed8e>:11: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-18-b7034db0ed8e>:18: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv1D` instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpt078hbkc/cnn/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.99569696, step = 1\n",
            "INFO:tensorflow:global_step/sec: 20.5512\n",
            "INFO:tensorflow:loss = 0.09419124, step = 5001 (243.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 20.7033\n",
            "INFO:tensorflow:loss = 0.04770914, step = 10001 (241.507 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 12359 into /tmp/tmpt078hbkc/cnn/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 20.4608\n",
            "INFO:tensorflow:loss = 0.04602832, step = 15001 (244.369 sec)\n",
            "INFO:tensorflow:global_step/sec: 20.6281\n",
            "INFO:tensorflow:loss = 0.013398403, step = 20001 (242.391 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 24671 into /tmp/tmpt078hbkc/cnn/model.ckpt.\n",
            "INFO:tensorflow:Saving checkpoints for 25000 into /tmp/tmpt078hbkc/cnn/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.013949459.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80j_kRfVq02t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3240992a-f8c3-460c-965d-357215d732b3"
      },
      "source": [
        "evaluate_classifier(cnn_classifier)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy  :  0.84596\n",
            "accuracy_baseline  :  0.5\n",
            "auc  :  0.9012032\n",
            "auc_precision_recall  :  0.9104663\n",
            "average_loss  :  0.86630476\n",
            "label/mean  :  0.5\n",
            "loss  :  0.86630476\n",
            "precision  :  0.8429693\n",
            "prediction/mean  :  0.503754\n",
            "recall  :  0.85032\n",
            "global_step  :  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzOzY2nGsFdg",
        "colab_type": "text"
      },
      "source": [
        "## 6 LSTM Network\n",
        "\n",
        "In our efforts to get even better accuracy on the dataset, we shall try building a Long Short-Term Memory (LSTM) cells Network. An LSTM processes the entire document sequentially, recursing over the sequence with its cell while storing the current state of the sequence in its memory. LSTM is a Recurrent Neural Network (RNN). One of the drawbacks of recurrent models is that, because of the nature of recursion, models are deeper and more complex, which usually results in slower training time and worse convergence. LSTMs can suffer convergence issues like vanishing or exploding gradients.\n",
        "\n",
        "In the beginning, we padded all documents up to 200 tokens, which is necessary to build a proper tensor. However, when a document contains fewer than 200 words, we don't want the LSTM to continue processing padding tokens as it does not add information and degrades performance. For this reason, we additionally want to provide our network with the length of the original sequence before it was padded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FtmDsMSsM5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "head = tf.contrib.estimator.binary_classification_head()\n",
        "\n",
        "def lstm_model_fn(features, labels, mode):    \n",
        "    # [batch_size x sentence_size x embedding_size]\n",
        "    inputs = tf.contrib.layers.embed_sequence(\n",
        "        features['x'], vocab_size, embedding_size,\n",
        "        initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
        "\n",
        "    # create an LSTM cell of size 100\n",
        "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)\n",
        "    \n",
        "    # create the complete LSTM\n",
        "    _, final_states = tf.nn.dynamic_rnn(\n",
        "        lstm_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
        "\n",
        "    # get the final hidden states of dimensionality [batch_size x sentence_size]\n",
        "    outputs = final_states.h\n",
        "\n",
        "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
        "\n",
        "    if labels is not None:\n",
        "        labels = tf.reshape(labels, [-1, 1])\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "    def _train_op_fn(loss):\n",
        "        return optimizer.minimize(\n",
        "            loss=loss,\n",
        "            global_step=tf.train.get_global_step())\n",
        "\n",
        "    return head.create_estimator_spec(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        mode=mode,\n",
        "        logits=logits,\n",
        "        train_op_fn=_train_op_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WnGsYoGKvQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_classifier = tf.estimator.Estimator(model_fn=lstm_model_fn,\n",
        "                                         model_dir=os.path.join(model_dir, 'lstm'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "849Jb89PKyhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94704f7a-f3b8-4453-cd6e-77fab4e61e21"
      },
      "source": [
        "train_classifier(lstm_classifier)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From <ipython-input-14-66949bb143c8>:10: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-14-66949bb143c8>:14: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-14-66949bb143c8>:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.7076198, step = 1\n",
            "INFO:tensorflow:global_step/sec: 2.47515\n",
            "INFO:tensorflow:loss = 0.7425122, step = 101 (40.407 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48924\n",
            "INFO:tensorflow:loss = 0.5470848, step = 201 (40.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49018\n",
            "INFO:tensorflow:loss = 0.5880917, step = 301 (40.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46896\n",
            "INFO:tensorflow:loss = 0.3970359, step = 401 (40.503 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45782\n",
            "INFO:tensorflow:loss = 0.45159426, step = 501 (40.686 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44031\n",
            "INFO:tensorflow:loss = 0.34689546, step = 601 (40.981 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46154\n",
            "INFO:tensorflow:loss = 0.3863623, step = 701 (40.627 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46618\n",
            "INFO:tensorflow:loss = 0.36065066, step = 801 (40.544 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4525\n",
            "INFO:tensorflow:loss = 0.3622551, step = 901 (40.775 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46717\n",
            "INFO:tensorflow:loss = 0.22952613, step = 1001 (40.532 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47359\n",
            "INFO:tensorflow:loss = 0.30774277, step = 1101 (40.427 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4753\n",
            "INFO:tensorflow:loss = 0.23067878, step = 1201 (40.399 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45999\n",
            "INFO:tensorflow:loss = 0.2991316, step = 1301 (40.653 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48248\n",
            "INFO:tensorflow:loss = 0.32243615, step = 1401 (40.284 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1481 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.45839\n",
            "INFO:tensorflow:loss = 0.24598733, step = 1501 (40.672 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48036\n",
            "INFO:tensorflow:loss = 0.21953197, step = 1601 (40.320 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48393\n",
            "INFO:tensorflow:loss = 0.21378899, step = 1701 (40.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44304\n",
            "INFO:tensorflow:loss = 0.23467131, step = 1801 (40.931 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43778\n",
            "INFO:tensorflow:loss = 0.20551053, step = 1901 (41.021 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44674\n",
            "INFO:tensorflow:loss = 0.1121484, step = 2001 (40.871 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4329\n",
            "INFO:tensorflow:loss = 0.21478172, step = 2101 (41.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44801\n",
            "INFO:tensorflow:loss = 0.23003723, step = 2201 (40.849 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47227\n",
            "INFO:tensorflow:loss = 0.15532249, step = 2301 (40.449 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47224\n",
            "INFO:tensorflow:loss = 0.19073911, step = 2401 (40.449 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49263\n",
            "INFO:tensorflow:loss = 0.14607829, step = 2501 (40.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49092\n",
            "INFO:tensorflow:loss = 0.18944925, step = 2601 (40.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48373\n",
            "INFO:tensorflow:loss = 0.12618862, step = 2701 (40.259 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48187\n",
            "INFO:tensorflow:loss = 0.115360096, step = 2801 (40.292 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43681\n",
            "INFO:tensorflow:loss = 0.13943048, step = 2901 (41.037 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2959 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.4261\n",
            "INFO:tensorflow:loss = 0.18479696, step = 3001 (41.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46599\n",
            "INFO:tensorflow:loss = 0.11729857, step = 3101 (40.551 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4889\n",
            "INFO:tensorflow:loss = 0.14973287, step = 3201 (40.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48818\n",
            "INFO:tensorflow:loss = 0.09194274, step = 3301 (40.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48699\n",
            "INFO:tensorflow:loss = 0.11223269, step = 3401 (40.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49315\n",
            "INFO:tensorflow:loss = 0.083649136, step = 3501 (40.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48122\n",
            "INFO:tensorflow:loss = 0.13925336, step = 3601 (40.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47324\n",
            "INFO:tensorflow:loss = 0.1729661, step = 3701 (40.436 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44153\n",
            "INFO:tensorflow:loss = 0.075825535, step = 3801 (40.956 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44314\n",
            "INFO:tensorflow:loss = 0.058767427, step = 3901 (40.930 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44656\n",
            "INFO:tensorflow:loss = 0.055978037, step = 4001 (40.873 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44456\n",
            "INFO:tensorflow:loss = 0.046467166, step = 4101 (40.907 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44012\n",
            "INFO:tensorflow:loss = 0.14181922, step = 4201 (40.981 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45341\n",
            "INFO:tensorflow:loss = 0.026507387, step = 4301 (40.764 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45411\n",
            "INFO:tensorflow:loss = 0.040757827, step = 4401 (40.744 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4437 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.48083\n",
            "INFO:tensorflow:loss = 0.058588587, step = 4501 (40.309 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48591\n",
            "INFO:tensorflow:loss = 0.030171946, step = 4601 (40.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46923\n",
            "INFO:tensorflow:loss = 0.057292033, step = 4701 (40.501 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46581\n",
            "INFO:tensorflow:loss = 0.06631984, step = 4801 (40.552 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46171\n",
            "INFO:tensorflow:loss = 0.018308075, step = 4901 (40.626 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45358\n",
            "INFO:tensorflow:loss = 0.014008304, step = 5001 (40.753 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45166\n",
            "INFO:tensorflow:loss = 0.060268097, step = 5101 (40.792 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43917\n",
            "INFO:tensorflow:loss = 0.046560984, step = 5201 (40.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45225\n",
            "INFO:tensorflow:loss = 0.036192838, step = 5301 (40.783 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4581\n",
            "INFO:tensorflow:loss = 0.010195819, step = 5401 (40.677 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45093\n",
            "INFO:tensorflow:loss = 0.0306886, step = 5501 (40.802 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44476\n",
            "INFO:tensorflow:loss = 0.046013325, step = 5601 (40.905 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46411\n",
            "INFO:tensorflow:loss = 0.069471374, step = 5701 (40.580 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46261\n",
            "INFO:tensorflow:loss = 0.007971866, step = 5801 (40.607 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46141\n",
            "INFO:tensorflow:loss = 0.022482863, step = 5901 (40.627 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5913 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.45145\n",
            "INFO:tensorflow:loss = 0.047856264, step = 6001 (40.792 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45901\n",
            "INFO:tensorflow:loss = 0.011574383, step = 6101 (40.672 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46039\n",
            "INFO:tensorflow:loss = 0.03412968, step = 6201 (40.639 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45736\n",
            "INFO:tensorflow:loss = 0.0230061, step = 6301 (40.695 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45443\n",
            "INFO:tensorflow:loss = 0.0041737864, step = 6401 (40.742 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46684\n",
            "INFO:tensorflow:loss = 0.011349258, step = 6501 (40.538 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47551\n",
            "INFO:tensorflow:loss = 0.006215686, step = 6601 (40.395 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45974\n",
            "INFO:tensorflow:loss = 0.011401521, step = 6701 (40.658 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44856\n",
            "INFO:tensorflow:loss = 0.0022760627, step = 6801 (40.842 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.464\n",
            "INFO:tensorflow:loss = 0.0026611432, step = 6901 (40.580 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45083\n",
            "INFO:tensorflow:loss = 0.051623993, step = 7001 (40.802 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45518\n",
            "INFO:tensorflow:loss = 0.013234343, step = 7101 (40.734 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44132\n",
            "INFO:tensorflow:loss = 0.006117618, step = 7201 (40.958 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45075\n",
            "INFO:tensorflow:loss = 0.0062585133, step = 7301 (40.804 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7387 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:global_step/sec: 2.42591\n",
            "INFO:tensorflow:loss = 0.0039606676, step = 7401 (41.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44929\n",
            "INFO:tensorflow:loss = 0.0009949484, step = 7501 (40.824 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4599\n",
            "INFO:tensorflow:loss = 0.0032988414, step = 7601 (40.653 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44699\n",
            "INFO:tensorflow:loss = 0.032405425, step = 7701 (40.866 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44958\n",
            "INFO:tensorflow:loss = 0.011759782, step = 7801 (40.823 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45461\n",
            "INFO:tensorflow:loss = 0.02254511, step = 7901 (40.744 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46392\n",
            "INFO:tensorflow:loss = 0.013961483, step = 8001 (40.586 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47448\n",
            "INFO:tensorflow:loss = 0.010188877, step = 8101 (40.408 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43034\n",
            "INFO:tensorflow:loss = 0.015384767, step = 8201 (41.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43367\n",
            "INFO:tensorflow:loss = 0.003488777, step = 8301 (41.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45714\n",
            "INFO:tensorflow:loss = 0.14221671, step = 8401 (40.692 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.42506\n",
            "INFO:tensorflow:loss = 0.044543844, step = 8501 (41.236 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45085\n",
            "INFO:tensorflow:loss = 0.004535718, step = 8601 (40.807 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47412\n",
            "INFO:tensorflow:loss = 0.019078502, step = 8701 (40.418 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48841\n",
            "INFO:tensorflow:loss = 0.009123727, step = 8801 (40.182 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8860 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.47764\n",
            "INFO:tensorflow:loss = 0.0024561246, step = 8901 (40.364 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46057\n",
            "INFO:tensorflow:loss = 0.0042997845, step = 9001 (40.639 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48626\n",
            "INFO:tensorflow:loss = 0.0057453983, step = 9101 (40.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48194\n",
            "INFO:tensorflow:loss = 0.0037152297, step = 9201 (40.288 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49365\n",
            "INFO:tensorflow:loss = 0.0015412173, step = 9301 (40.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46688\n",
            "INFO:tensorflow:loss = 0.0018292831, step = 9401 (40.536 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45202\n",
            "INFO:tensorflow:loss = 0.0011873477, step = 9501 (40.784 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45661\n",
            "INFO:tensorflow:loss = 0.0010742435, step = 9601 (40.700 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44946\n",
            "INFO:tensorflow:loss = 0.00068298454, step = 9701 (40.826 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.42778\n",
            "INFO:tensorflow:loss = 0.0005975583, step = 9801 (41.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44662\n",
            "INFO:tensorflow:loss = 0.00080744294, step = 9901 (40.873 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45127\n",
            "INFO:tensorflow:loss = 0.00090433704, step = 10001 (40.795 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46758\n",
            "INFO:tensorflow:loss = 0.00025731462, step = 10101 (40.531 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44094\n",
            "INFO:tensorflow:loss = 0.00021593485, step = 10201 (40.967 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46474\n",
            "INFO:tensorflow:loss = 0.0002730553, step = 10301 (40.573 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10337 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.43192\n",
            "INFO:tensorflow:loss = 0.00011049061, step = 10401 (41.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45802\n",
            "INFO:tensorflow:loss = 0.0003660325, step = 10501 (40.683 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47987\n",
            "INFO:tensorflow:loss = 0.0003264403, step = 10601 (40.330 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.51361\n",
            "INFO:tensorflow:loss = 0.00052079244, step = 10701 (39.782 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48066\n",
            "INFO:tensorflow:loss = 0.000510853, step = 10801 (40.308 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49091\n",
            "INFO:tensorflow:loss = 0.00017537529, step = 10901 (40.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44454\n",
            "INFO:tensorflow:loss = 0.00026968232, step = 11001 (40.909 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49128\n",
            "INFO:tensorflow:loss = 0.00019277747, step = 11101 (40.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48693\n",
            "INFO:tensorflow:loss = 0.00024012556, step = 11201 (40.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46621\n",
            "INFO:tensorflow:loss = 0.00021827879, step = 11301 (40.553 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45992\n",
            "INFO:tensorflow:loss = 0.00015914913, step = 11401 (40.647 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47173\n",
            "INFO:tensorflow:loss = 0.0002586105, step = 11501 (40.457 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49076\n",
            "INFO:tensorflow:loss = 0.00027961016, step = 11601 (40.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48039\n",
            "INFO:tensorflow:loss = 9.4286865e-05, step = 11701 (40.311 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46958\n",
            "INFO:tensorflow:loss = 8.2985665e-05, step = 11801 (40.492 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 11822 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.42749\n",
            "INFO:tensorflow:loss = 0.00027939343, step = 11901 (41.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45782\n",
            "INFO:tensorflow:loss = 0.00019912295, step = 12001 (40.687 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47978\n",
            "INFO:tensorflow:loss = 0.00011043084, step = 12101 (40.327 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47923\n",
            "INFO:tensorflow:loss = 0.00028941184, step = 12201 (40.335 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46392\n",
            "INFO:tensorflow:loss = 3.5166016e-05, step = 12301 (40.586 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49706\n",
            "INFO:tensorflow:loss = 6.192193e-05, step = 12401 (40.047 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48783\n",
            "INFO:tensorflow:loss = 0.00011866685, step = 12501 (40.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46653\n",
            "INFO:tensorflow:loss = 7.488851e-05, step = 12601 (40.543 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49022\n",
            "INFO:tensorflow:loss = 4.7975507e-05, step = 12701 (40.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48118\n",
            "INFO:tensorflow:loss = 7.59484e-05, step = 12801 (40.304 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49588\n",
            "INFO:tensorflow:loss = 0.0001290434, step = 12901 (40.062 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45834\n",
            "INFO:tensorflow:loss = 0.000101502184, step = 13001 (40.680 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44199\n",
            "INFO:tensorflow:loss = 6.081683e-05, step = 13101 (40.951 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43952\n",
            "INFO:tensorflow:loss = 5.9984428e-05, step = 13201 (40.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45711\n",
            "INFO:tensorflow:loss = 7.203457e-05, step = 13301 (40.695 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 13304 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.43401\n",
            "INFO:tensorflow:loss = 5.248623e-05, step = 13401 (41.083 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4608\n",
            "INFO:tensorflow:loss = 4.4327135e-05, step = 13501 (40.635 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45679\n",
            "INFO:tensorflow:loss = 4.3518823e-05, step = 13601 (40.711 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49521\n",
            "INFO:tensorflow:loss = 3.1714855e-05, step = 13701 (40.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47978\n",
            "INFO:tensorflow:loss = 1.2978848e-05, step = 13801 (40.327 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49138\n",
            "INFO:tensorflow:loss = 3.8785358e-05, step = 13901 (40.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47025\n",
            "INFO:tensorflow:loss = 4.147966e-05, step = 14001 (40.481 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48753\n",
            "INFO:tensorflow:loss = 2.7150752e-05, step = 14101 (40.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46794\n",
            "INFO:tensorflow:loss = 6.806782e-05, step = 14201 (40.519 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47076\n",
            "INFO:tensorflow:loss = 1.7158984e-05, step = 14301 (40.473 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45136\n",
            "INFO:tensorflow:loss = 1.712201e-05, step = 14401 (40.794 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.51857\n",
            "INFO:tensorflow:loss = 1.3262045e-05, step = 14501 (39.705 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47162\n",
            "INFO:tensorflow:loss = 1.9761603e-05, step = 14601 (40.459 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48474\n",
            "INFO:tensorflow:loss = 1.598955e-05, step = 14701 (40.246 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 14789 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.44376\n",
            "INFO:tensorflow:loss = 8.521403e-06, step = 14801 (40.920 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46196\n",
            "INFO:tensorflow:loss = 2.2393346e-05, step = 14901 (40.618 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4766\n",
            "INFO:tensorflow:loss = 3.085622e-05, step = 15001 (40.378 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48246\n",
            "INFO:tensorflow:loss = 2.2419448e-05, step = 15101 (40.287 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50934\n",
            "INFO:tensorflow:loss = 1.7886452e-05, step = 15201 (39.846 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.38754\n",
            "INFO:tensorflow:loss = 1.5078133e-05, step = 15301 (41.884 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.51276\n",
            "INFO:tensorflow:loss = 1.18334565e-05, step = 15401 (39.797 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49936\n",
            "INFO:tensorflow:loss = 1.0586597e-05, step = 15501 (40.010 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4975\n",
            "INFO:tensorflow:loss = 7.706847e-06, step = 15601 (40.040 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48982\n",
            "INFO:tensorflow:loss = 2.5927e-05, step = 15701 (40.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4651\n",
            "INFO:tensorflow:loss = 9.803343e-06, step = 15801 (40.562 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43508\n",
            "INFO:tensorflow:loss = 1.1750447e-05, step = 15901 (41.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47201\n",
            "INFO:tensorflow:loss = 9.226846e-06, step = 16001 (40.452 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50354\n",
            "INFO:tensorflow:loss = 6.091453e-06, step = 16101 (39.943 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49331\n",
            "INFO:tensorflow:loss = 5.6027334e-06, step = 16201 (40.108 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 16275 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.46786\n",
            "INFO:tensorflow:loss = 8.462088e-06, step = 16301 (40.527 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50533\n",
            "INFO:tensorflow:loss = 5.219649e-06, step = 16401 (39.909 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.52427\n",
            "INFO:tensorflow:loss = 4.0614027e-06, step = 16501 (39.615 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.508\n",
            "INFO:tensorflow:loss = 6.4767983e-06, step = 16601 (39.877 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.51414\n",
            "INFO:tensorflow:loss = 5.3129893e-06, step = 16701 (39.770 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50608\n",
            "INFO:tensorflow:loss = 4.1322355e-06, step = 16801 (39.903 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.51077\n",
            "INFO:tensorflow:loss = 8.748528e-06, step = 16901 (39.833 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49753\n",
            "INFO:tensorflow:loss = 8.8134875e-06, step = 17001 (40.035 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49472\n",
            "INFO:tensorflow:loss = 6.4399173e-06, step = 17101 (40.089 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50116\n",
            "INFO:tensorflow:loss = 2.0470306e-06, step = 17201 (39.981 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49456\n",
            "INFO:tensorflow:loss = 4.328399e-06, step = 17301 (40.089 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50866\n",
            "INFO:tensorflow:loss = 3.2644548e-06, step = 17401 (39.857 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49278\n",
            "INFO:tensorflow:loss = 2.758435e-06, step = 17501 (40.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4846\n",
            "INFO:tensorflow:loss = 3.6612612e-06, step = 17601 (40.248 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49481\n",
            "INFO:tensorflow:loss = 2.6666585e-06, step = 17701 (40.084 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 17775 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.46782\n",
            "INFO:tensorflow:loss = 3.0158326e-06, step = 17801 (40.524 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48019\n",
            "INFO:tensorflow:loss = 5.5261935e-06, step = 17901 (40.324 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50039\n",
            "INFO:tensorflow:loss = 1.4851996e-06, step = 18001 (39.987 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50062\n",
            "INFO:tensorflow:loss = 4.0155246e-06, step = 18101 (39.990 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49035\n",
            "INFO:tensorflow:loss = 2.213777e-06, step = 18201 (40.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50066\n",
            "INFO:tensorflow:loss = 2.2474735e-06, step = 18301 (39.990 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50035\n",
            "INFO:tensorflow:loss = 2.6125665e-06, step = 18401 (39.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50677\n",
            "INFO:tensorflow:loss = 2.473002e-06, step = 18501 (39.892 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48198\n",
            "INFO:tensorflow:loss = 2.1533701e-06, step = 18601 (40.290 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50434\n",
            "INFO:tensorflow:loss = 1.6133736e-06, step = 18701 (39.931 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49954\n",
            "INFO:tensorflow:loss = 1.8319369e-06, step = 18801 (40.007 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50699\n",
            "INFO:tensorflow:loss = 1.4473713e-06, step = 18901 (39.892 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46424\n",
            "INFO:tensorflow:loss = 1.3087919e-06, step = 19001 (40.577 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46441\n",
            "INFO:tensorflow:loss = 1.580932e-06, step = 19101 (40.577 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45982\n",
            "INFO:tensorflow:loss = 1.372059e-06, step = 19201 (40.658 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 19268 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.42618\n",
            "INFO:tensorflow:loss = 2.223363e-06, step = 19301 (41.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49199\n",
            "INFO:tensorflow:loss = 1.0416798e-06, step = 19401 (40.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46473\n",
            "INFO:tensorflow:loss = 1.1667446e-06, step = 19501 (40.574 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45537\n",
            "INFO:tensorflow:loss = 6.50205e-07, step = 19601 (40.722 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45855\n",
            "INFO:tensorflow:loss = 1.4513757e-06, step = 19701 (40.674 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43192\n",
            "INFO:tensorflow:loss = 1.1493363e-06, step = 19801 (41.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46903\n",
            "INFO:tensorflow:loss = 7.989409e-07, step = 19901 (40.504 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45878\n",
            "INFO:tensorflow:loss = 1.2290126e-06, step = 20001 (40.673 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44032\n",
            "INFO:tensorflow:loss = 1.2766493e-06, step = 20101 (40.978 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44358\n",
            "INFO:tensorflow:loss = 4.6890432e-07, step = 20201 (40.919 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44941\n",
            "INFO:tensorflow:loss = 1.039353e-06, step = 20301 (40.825 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45384\n",
            "INFO:tensorflow:loss = 3.8674946e-07, step = 20401 (40.753 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45168\n",
            "INFO:tensorflow:loss = 5.702111e-07, step = 20501 (40.793 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43778\n",
            "INFO:tensorflow:loss = 7.160204e-07, step = 20601 (41.016 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45065\n",
            "INFO:tensorflow:loss = 6.649695e-07, step = 20701 (40.806 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 20740 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.41298\n",
            "INFO:tensorflow:loss = 5.012156e-07, step = 20801 (41.443 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45181\n",
            "INFO:tensorflow:loss = 6.710865e-07, step = 20901 (40.791 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45982\n",
            "INFO:tensorflow:loss = 2.9420676e-07, step = 21001 (40.652 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45985\n",
            "INFO:tensorflow:loss = 2.4269636e-07, step = 21101 (40.650 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4573\n",
            "INFO:tensorflow:loss = 4.495783e-07, step = 21201 (40.699 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44601\n",
            "INFO:tensorflow:loss = 1.7152298e-07, step = 21301 (40.883 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45315\n",
            "INFO:tensorflow:loss = 2.6042753e-07, step = 21401 (40.760 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44681\n",
            "INFO:tensorflow:loss = 2.4183635e-07, step = 21501 (40.874 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43759\n",
            "INFO:tensorflow:loss = 1.9130066e-07, step = 21601 (41.024 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46284\n",
            "INFO:tensorflow:loss = 8.1182384e-07, step = 21701 (40.599 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.43944\n",
            "INFO:tensorflow:loss = 4.578127e-07, step = 21801 (40.993 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47423\n",
            "INFO:tensorflow:loss = 1.5404737e-07, step = 21901 (40.417 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47076\n",
            "INFO:tensorflow:loss = 3.8463168e-07, step = 22001 (40.473 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44492\n",
            "INFO:tensorflow:loss = 3.279659e-07, step = 22101 (40.901 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.46345\n",
            "INFO:tensorflow:loss = 9.139971e-08, step = 22201 (40.597 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 22210 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.35964\n",
            "INFO:tensorflow:loss = 2.7002136e-07, step = 22301 (42.377 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45832\n",
            "INFO:tensorflow:loss = 3.1167093e-07, step = 22401 (40.677 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4765\n",
            "INFO:tensorflow:loss = 9.606399e-08, step = 22501 (40.382 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4777\n",
            "INFO:tensorflow:loss = 2.1026038e-07, step = 22601 (40.362 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47755\n",
            "INFO:tensorflow:loss = 3.922182e-07, step = 22701 (40.359 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.45431\n",
            "INFO:tensorflow:loss = 1.3607155e-07, step = 22801 (40.749 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4972\n",
            "INFO:tensorflow:loss = 2.2594733e-07, step = 22901 (40.043 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50048\n",
            "INFO:tensorflow:loss = 2.1644883e-07, step = 23001 (39.992 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47576\n",
            "INFO:tensorflow:loss = 1.648541e-07, step = 23101 (40.389 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48064\n",
            "INFO:tensorflow:loss = 1.2389813e-07, step = 23201 (40.318 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47949\n",
            "INFO:tensorflow:loss = 1.08398375e-07, step = 23301 (40.330 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50629\n",
            "INFO:tensorflow:loss = 1.0001986e-07, step = 23401 (39.895 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.51223\n",
            "INFO:tensorflow:loss = 2.0222298e-07, step = 23501 (39.805 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48578\n",
            "INFO:tensorflow:loss = 9.175658e-08, step = 23601 (40.229 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 23698 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.47776\n",
            "INFO:tensorflow:loss = 1.2815282e-07, step = 23701 (40.359 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49568\n",
            "INFO:tensorflow:loss = 6.161692e-08, step = 23801 (40.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48979\n",
            "INFO:tensorflow:loss = 1.6449142e-07, step = 23901 (40.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4935\n",
            "INFO:tensorflow:loss = 5.7860223e-08, step = 24001 (40.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49273\n",
            "INFO:tensorflow:loss = 1.0698776e-07, step = 24101 (40.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49335\n",
            "INFO:tensorflow:loss = 5.3461626e-08, step = 24201 (40.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.50217\n",
            "INFO:tensorflow:loss = 7.614558e-08, step = 24301 (39.965 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48791\n",
            "INFO:tensorflow:loss = 1.0117433e-07, step = 24401 (40.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49235\n",
            "INFO:tensorflow:loss = 4.331174e-08, step = 24501 (40.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.47367\n",
            "INFO:tensorflow:loss = 8.822588e-08, step = 24601 (40.422 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49449\n",
            "INFO:tensorflow:loss = 9.353289e-08, step = 24701 (40.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.48713\n",
            "INFO:tensorflow:loss = 3.681649e-08, step = 24801 (40.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49478\n",
            "INFO:tensorflow:loss = 4.0621643e-08, step = 24901 (40.082 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 25000 into /tmp/tmpt078hbkc/lstm/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 5.373673e-08.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nXE8UuK3qo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b8d085e0-f8cc-4f9b-dbbd-d3c7526611f6"
      },
      "source": [
        "evaluate_classifier(lstm_classifier)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy  :  0.83924\n",
            "accuracy_baseline  :  0.5\n",
            "auc  :  0.85572875\n",
            "auc_precision_recall  :  0.88701934\n",
            "average_loss  :  2.7127547\n",
            "label/mean  :  0.5\n",
            "loss  :  2.7127552\n",
            "precision  :  0.8427775\n",
            "prediction/mean  :  0.49515826\n",
            "recall  :  0.83408\n",
            "global_step  :  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE-ivW3S7RPp",
        "colab_type": "text"
      },
      "source": [
        "## 7. Discussion\n",
        "\n",
        "We can see that the CNNs and LSTM work well with the dataset. Due to limitations in computing, I was not able to train these networks for longer period. With more training, these models might perform even better. Now, we are getting an accuracy of closer to 85%, which is really great considering the size of the dataset. Similarly, we can also perform multiclass classification using these neural networks."
      ]
    }
  ]
}