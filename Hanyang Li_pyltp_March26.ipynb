{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Spotlight: pyLTP\n",
    "\n",
    "### Submitted by: Hanyang Li\n",
    "\n",
    "### Due: March 26, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Technology Platform (LTP) is a Chinese language processing library developed by HIT-SCIR, pyLTP is the python implementation of LTP. LTP provides sufficient and efficient Chinese language processing modules.\n",
    "\n",
    "With LTP, you can process text in Chinese. First, splitting the sentences in the text. Then splitting Chinese words in each sentence, this is different from splitting words in English, because there is no spaces between Chinese words. After that, you can determine the pos of each words. With the pos tags, you can do more analyses like name entitity recognizing and sementic role labelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "#### Support and dependency\n",
    "\n",
    "|       |Python 2.6|Python 2.7|Python 3.4|Python 3.5|Python 3.6|Conda Python|\n",
    "|:-----:|:--------:|:--------:|:--------:|:--------:|:--------:|:----------:|\n",
    "|Linux  |Support   |Support   |Support   |Support   |Support   |Nonsupport  |\n",
    "|Mac OS |Support   |Support   |Support   |Support   |Support   |Nonsupport  |\n",
    "|Windows|Nonsupport|Nonsupport|Nonsupport|Support   |Support   |Nonsupport  |\n",
    "\n",
    "#### Setup\n",
    "\n",
    "Install pyLTP using pip by running in your terminal:\n",
    "\n",
    "```\n",
    "$ pip install pyltp\n",
    "```\n",
    "\n",
    "#### Model files downloading\n",
    "\n",
    "pyLTP has trained models for different language processing modules. You can download the models in http://ltp.ai/download.html. This notes uses model version 3.4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Begin\n",
    "\n",
    "Before using pyLTP processing Chinese language, we need to load the directory of models downloaded in last parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LTP_DATA_DIR='./ltp_data_v3.4.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Sentences\n",
    "\n",
    "pyLTP can split the sentences in the text base on the punctuations. This is basically identical to the English language processing. LTP will split the text when detecting punctuations like `。`, `！`, `？`, `……`, `；`.\n",
    "\n",
    "Take a paragraph of Joker Wikipedia (https://zh.wikipedia.org/wiki/小丑_(電影)) for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0: 1981年，哥谭市充满失业、犯罪，导致许多人穷困潦倒、失去基本权利。\n",
      "\n",
      "Sentence 1: 身为社会局外人的亚瑟·佛莱克立志成为一位单口喜剧演员，做起派对小丑来供养他的年迈母亲潘妮……\n",
      "\n",
      "Sentence 2: 亚瑟本身患有一种罕见精神疾病，导致他在不合时宜的时候会大笑，只能接受社福机构人员的治疗以获取药物；\n",
      "\n",
      "Sentence 3: 一次工作时，亚瑟被一群小孩偷走招牌，被他们引入小巷里暴打，他的同事蓝道送他一把自保用的左轮手枪！\n",
      "\n",
      "Sentence 4: 回家时，亚瑟开始对他的邻居苏菲·杜蒙感兴趣，邀请她前来观看他的单口喜剧表演，随即顺利跟她开始交往？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyltp import SentenceSplitter\n",
    "sentences = SentenceSplitter.split('1981年，哥谭市充满失业、犯罪，导致许多人穷困潦倒、失去基本权利。身为社会局外人的亚瑟·佛莱克立志成为一位单口喜剧演员，做起派对小丑来供养他的年迈母亲潘妮……亚瑟本身患有一种罕见精神疾病，导致他在不合时宜的时候会大笑，只能接受社福机构人员的治疗以获取药物；一次工作时，亚瑟被一群小孩偷走招牌，被他们引入小巷里暴打，他的同事蓝道送他一把自保用的左轮手枪！回家时，亚瑟开始对他的邻居苏菲·杜蒙感兴趣，邀请她前来观看他的单口喜剧表演，随即顺利跟她开始交往？')\n",
    "for i in range(len(sentences)):\n",
    "    print(\"Sentence \" + str(i) + \": \" + sentences[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the text is splitted into sentences by the puncruations above, and other puntuations are ignored by the sentence splitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Segment\n",
    "\n",
    "After splitting the sentences, we can split the words in sentences. pyLTP can segment words base on trained model. \n",
    "\n",
    "Take the `Sentence 0` for example, to make it easier to understand, I will show you the English version of this sentence and the true segments of it:\n",
    "\n",
    "* Chinese: 1981年，哥谭市充满失业、犯罪，导致许多人穷困潦倒、失去基本权利。\n",
    "* English: In 1981, Gotham is rife with crime and unemployment, leaving swathes of the population lost of basic rights and impoverished.\n",
    "\n",
    "The true segments are:\n",
    "\n",
    "|Chinese|English|\n",
    "|:-:|:-:|\n",
    "|1981年|In 1981|\n",
    "|，|,|\n",
    "|哥谭市|Gotham|\n",
    "|充满|is rife with|\n",
    "|失业|unemployment|\n",
    "|、|and|\n",
    "|犯罪|crime|\n",
    "|导致|leaving|\n",
    "|许多|swathes of|\n",
    "|人|the population|\n",
    "|穷困潦倒|impoverished|\n",
    "|、|and|\n",
    "|失去|lost of|\n",
    "|基本|basic|\n",
    "|权利|rights|\n",
    "\n",
    "And the words splitted by pyLTP are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  0: 1981年\n",
      "Word  1: ，\n",
      "Word  2: 哥\n",
      "Word  3: 谭市\n",
      "Word  4: 充满\n",
      "Word  5: 失业\n",
      "Word  6: 、\n",
      "Word  7: 犯罪\n",
      "Word  8: ，\n",
      "Word  9: 导致\n",
      "Word 10: 许多\n",
      "Word 11: 人\n",
      "Word 12: 穷困\n",
      "Word 13: 潦倒\n",
      "Word 14: 、\n",
      "Word 15: 失去\n",
      "Word 16: 基本\n",
      "Word 17: 权利\n",
      "Word 18: 。\n"
     ]
    }
   ],
   "source": [
    "from pyltp import Segmentor\n",
    "\n",
    "cws_model_path=os.path.join(LTP_DATA_DIR,'cws.model')\n",
    "\n",
    "segmentor=Segmentor()\n",
    "segmentor.load(cws_model_path)\n",
    "\n",
    "words=segmentor.segment('1981年，哥谭市充满失业、犯罪，导致许多人穷困潦倒、失去基本权利。')\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(\"Word \" + str(i).rjust(2) + \": \" + words[i])\n",
    "\n",
    "segmentor.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, most of the words are splitted very well except word Gotham and impoverished.\n",
    "\n",
    "The model of pyLTP is trained by common word segments, so it cannot detect the words, which are in specific areas, well.\n",
    "\n",
    "The word impoverished in Chinese is an idiom, it is composed by two common words, so pyLTP make mistake when splitting this word.\n",
    "\n",
    "To solve the problem above, we can segment words base on lexicon. Once we add Gotham and impoverished into the lexicon and load it to pyLTP word segmentor, it will not split the words in lexicon apart anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  0: 1981年\n",
      "Word  1: ，\n",
      "Word  2: 哥谭市\n",
      "Word  3: 充满\n",
      "Word  4: 失业\n",
      "Word  5: 、\n",
      "Word  6: 犯罪\n",
      "Word  7: ，\n",
      "Word  8: 导致\n",
      "Word  9: 许多\n",
      "Word 10: 人\n",
      "Word 11: 穷困潦倒\n",
      "Word 12: 、\n",
      "Word 13: 失去\n",
      "Word 14: 基本\n",
      "Word 15: 权利\n",
      "Word 16: 。\n"
     ]
    }
   ],
   "source": [
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')\n",
    "\n",
    "from pyltp import Segmentor\n",
    "\n",
    "segmentor = Segmentor()\n",
    "segmentor.load_with_lexicon(cws_model_path, './lexicon.txt')\n",
    "\n",
    "words = segmentor.segment('1981年，哥谭市充满失业、犯罪，导致许多人穷困潦倒、失去基本权利。')\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(\"Word \" + str(i).rjust(2) + \": \" + words[i])\n",
    "\n",
    "segmentor.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have got the right word segments. Let's translate these words into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  0: 1981年  \t(In 1981)\n",
      "Word  1: ，      \t(,)\n",
      "Word  2: 哥谭市    \t(Gotham)\n",
      "Word  3: 充满     \t(is rife with)\n",
      "Word  4: 失业     \t(unemployment)\n",
      "Word  5: 、      \t(and)\n",
      "Word  6: 犯罪     \t(crime)\n",
      "Word  7: ，      \t(,)\n",
      "Word  8: 导致     \t(leaving)\n",
      "Word  9: 许多     \t(swathes of)\n",
      "Word 10: 人      \t(the population)\n",
      "Word 11: 穷困潦倒   \t(impoverished)\n",
      "Word 12: 、      \t(and)\n",
      "Word 13: 失去     \t(lost of)\n",
      "Word 14: 基本     \t(basic)\n",
      "Word 15: 权利     \t(rights)\n",
      "Word 16: 。      \t(.)\n"
     ]
    }
   ],
   "source": [
    "en_words = ['In 1981',',','Gotham','is rife with','unemployment','and','crime',',','leaving',\n",
    "            'swathes of','the population','impoverished','and','lost of','basic','rights','.']\n",
    "for i in range(len(words)):\n",
    "    print(\"Word \" + str(i).rjust(2) + \": \" + words[i].ljust(7) + \"\\t(\" + en_words[i] + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Pos\n",
    "\n",
    "pyLTP can determine the pos of the words base on trained model and the word segments.\n",
    "\n",
    "The pos tag table is shown as below (some of the examples have no English version):\n",
    "\n",
    "|Tag|Description|Example|\n",
    "|:-:|:-:|:-:|\n",
    "|a|adjective|美丽 (beautiful)|\n",
    "|b|other noun-modifier|大型 (large size)|\n",
    "|c|conjunction|虽然 (although)|\n",
    "|d|adverb|很 (very)|\n",
    "|e|exclamation|嗯 (Emmm)|\n",
    "|g|morpheme|茨|\n",
    "|h|prefix|阿|\n",
    "|i|idiom|百花齐放|\n",
    "|j|abbreviation|公检法|\n",
    "|k|suffix|率|\n",
    "|m|number|第一 (the first)|\n",
    "|n|general noun|苹果 (apple)|\n",
    "|nd|direction noun|右侧 (right)|\n",
    "|nh|person name|汤姆 (Tom)|\n",
    "|ni|organization name|保险公司 (insurance company)|\n",
    "|nl|location noun|城市 (city)|\n",
    "|ns|geographical name|北京 (Beijing)|\n",
    "|nt|temporal noun|近日 (recently)|\n",
    "|nz|other proper noun|诺贝尔奖 (Nobel Prize)|\n",
    "|o|onomatopoeia|当啷 (clank)|\n",
    "|p|preposition|让 (let)|\n",
    "|q|quantity|张 (piece)|\n",
    "|r|pronoun|我们 (we)|\n",
    "|u|auxiliary|的 ('s)|\n",
    "|v|verb|学习 (study)|\n",
    "|wp|punctuation|。|\n",
    "|ws|foreign words|CPU|\n",
    "|x|non-lexeme|萄|\n",
    "|z|descriptive words|瑟瑟|\n",
    "\n",
    "And the true pos tags of the Chinese words (may different from English) are:\n",
    "\n",
    "|Chinese|English|Tag|\n",
    "|:-:|:-:|:-:|\n",
    "|1981年|In 1981|nt|\n",
    "|，|,|wp|\n",
    "|哥谭市|Gotham|ns|\n",
    "|充满|is rife with|v|\n",
    "|失业|unemployment|v|\n",
    "|、|and|wp|\n",
    "|犯罪|crime|v|\n",
    "|，|,|wp|\n",
    "|导致|leaving|v|\n",
    "|许多|swathes of|m|\n",
    "|人|the population|n|\n",
    "|穷困潦倒|impoverished|i|\n",
    "|、|and|wp|\n",
    "|失去|lost of|v|\n",
    "|基本|basic|a|\n",
    "|权利|rights|n|\n",
    "|。|.|wp|\n",
    "\n",
    "The pos tags given by pyLTP are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981年\t:nt\n",
      "，\t:wp\n",
      "哥谭市\t:ns\n",
      "充满\t:v\n",
      "失业\t:v\n",
      "、\t:wp\n",
      "犯罪\t:v\n",
      "，\t:wp\n",
      "导致\t:v\n",
      "许多\t:m\n",
      "人\t:n\n",
      "穷困潦倒\t:i\n",
      "、\t:wp\n",
      "失去\t:v\n",
      "基本\t:a\n",
      "权利\t:n\n",
      "。\t:wp\n"
     ]
    }
   ],
   "source": [
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')\n",
    "\n",
    "from pyltp import Postagger\n",
    "\n",
    "postagger = Postagger()\n",
    "postagger.load(pos_model_path)\n",
    "\n",
    "postags = postagger.postag(words)\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(words[i] + \"\\t:\" + postags[i])\n",
    "\n",
    "postagger.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the pos tagger works pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax Dependency\n",
    "\n",
    "Once we got the word segments and the pos tags, we can use pyLTP detect the syntax dependencies base on trained model.\n",
    "\n",
    "The tags table of syntax dependencies is shown as below:\n",
    "\n",
    "|Tag|Description|Example|\n",
    "|:-:|:-:|:-:|\n",
    "|SBV|subject-verb|I <-- give|\n",
    "|VOB|verb-object|give --> apple|\n",
    "|IOB|indirect-object|give --> you|\n",
    "|FOB|fronting-object|book <-- read|\n",
    "|DBL|double|buy --> me|\n",
    "|ATT|attribute|red <-- apple|\n",
    "|ADV|adverbial|very <-- beautiful|\n",
    "|CMP|complement|finish --> all|\n",
    "|COO|coordinate|sea --> ocean|\n",
    "|POB|preposition-object|on --> top|\n",
    "|LAD|left adjunct|and <-- ocean|\n",
    "|RAD|right adjunct|child --> ren|\n",
    "|IS|independent structure|independent sentences|\n",
    "|HED|head|core of the sentence|\n",
    "\n",
    "And the syntax dependencies given by pyLTP are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981年\t(In 1981)       \t---ADV-->\t充满\t(is rife with)     \n",
      "，\t(,)                 \t---WP -->\t1981年\t(In 1981)       \n",
      "哥谭市\t(Gotham)          \t---SBV-->\t充满\t(is rife with)     \n",
      "充满\t(is rife with)     \t---HED-->\t。\t(.)                 \n",
      "失业\t(unemployment)     \t---VOB-->\t充满\t(is rife with)     \n",
      "、\t(and)               \t---WP -->\t犯罪\t(crime)            \n",
      "犯罪\t(crime)            \t---COO-->\t失业\t(unemployment)     \n",
      "，\t(,)                 \t---WP -->\t充满\t(is rife with)     \n",
      "导致\t(leaving)          \t---COO-->\t充满\t(is rife with)     \n",
      "许多\t(swathes of)       \t---ATT-->\t人\t(the population)    \n",
      "人\t(the population)    \t---SBV-->\t穷困潦倒\t(impoverished)   \n",
      "穷困潦倒\t(impoverished)   \t---VOB-->\t导致\t(leaving)          \n",
      "、\t(and)               \t---WP -->\t失去\t(lost of)          \n",
      "失去\t(lost of)          \t---COO-->\t穷困潦倒\t(impoverished)   \n",
      "基本\t(basic)            \t---ATT-->\t权利\t(rights)           \n",
      "权利\t(rights)           \t---VOB-->\t失去\t(lost of)          \n",
      "。\t(.)                 \t---WP -->\t充满\t(is rife with)     \n"
     ]
    }
   ],
   "source": [
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')\n",
    "\n",
    "from pyltp import Parser\n",
    "\n",
    "parser = Parser()\n",
    "parser.load(par_model_path)\n",
    "\n",
    "arcs = parser.parse(words, postags)\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print((words[i] + \"\\t(\" + en_words[i] + \")\").ljust(22),end='')\n",
    "    print(\"\\t---\" + arcs[i].relation.ljust(3) + \"-->\", end='\\t')\n",
    "    print((words[arcs[i].head-1] + \"\\t(\" + en_words[arcs[i].head-1] + \")\").ljust(22))\n",
    "\n",
    "parser.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sementic Role Label\n",
    "\n",
    "Based on the trained model, word segments, pos tags and syntac dependencies, pyLTP can recognize the sementic roles in the sentence.\n",
    "\n",
    "The labels of sementic roles are shown in table below:\n",
    "\n",
    "|Sementic Role Label|Description|\n",
    "|:-:|:-:|\n",
    "|ADV|adverbial, default tag|\n",
    "|BNE|beneﬁciary|\n",
    "|CND|condition|\n",
    "|DIR|direction|\n",
    "|DGR|degree|\n",
    "|EXT|extent|\n",
    "|FRQ|frequency|\n",
    "|LOC|locative|\n",
    "|MNR|manner|\n",
    "|PRP|purpose or reason|\n",
    "|TMP|temporal|\n",
    "|TPC|topic|\n",
    "|CRD|coordinated arguments|\n",
    "|PRD|predicate|\n",
    "|PSR|possessor|\n",
    "|PSE|possessee|\n",
    "\n",
    "And the sementic roles detected by pyLTP from the sentence of Gotham are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: 充满 (is rife with)\n",
      "\t TMP: 1981年， (In 1981 ,)\n",
      "\t  A0: 哥谭市 (Gotham)\n",
      "\t  A1: 失业、犯罪 (unemployment and crime)\n",
      "\n",
      "Role: 导致 (leaving)\n",
      "\t TMP: 1981年， (In 1981 ,)\n",
      "\t  A0: 哥谭市 (Gotham)\n",
      "\n",
      "Role: 穷困潦倒 (impoverished)\n",
      "\t  A0: 许多人 (swathes of the population)\n",
      "\n",
      "Role: 失去 (lost of)\n",
      "\t  A0: 许多人 (swathes of the population)\n",
      "\t  A1: 基本权利 (basic rights)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl.model')\n",
    "\n",
    "from pyltp import SementicRoleLabeller\n",
    "\n",
    "labeller = SementicRoleLabeller()\n",
    "labeller.load(srl_model_path)\n",
    "\n",
    "roles = labeller.label(words, postags, arcs)\n",
    "\n",
    "for role in roles:\n",
    "    print(\"Role: \" + words[role.index] + \" (\" + en_words[role.index] + \")\")\n",
    "    for arg in role.arguments:\n",
    "        print(\"\\t\" + (arg.name + \":\").rjust(5),end=' ')\n",
    "        for i in range(arg.range.start, arg.range.end + 1):\n",
    "            print(words[i],end='')\n",
    "        print(\" (\",end='')\n",
    "        for i in range(arg.range.start, arg.range.end + 1):\n",
    "            print(en_words[i],end=' ')\n",
    "        print(\"\\b)\")\n",
    "    print()\n",
    "\n",
    "labeller.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
