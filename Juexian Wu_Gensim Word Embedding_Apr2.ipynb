{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim - Fast Building Framework for Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding can be regarded as an efficient way to represent word in machines. In machines, words are usually represented as vectors.\n",
    "\n",
    "In order to represent words in machines, the simplest way is by using one-hot-vector, that is, for each word, it can be a high-dimensional vector with only one \"1\" in the vector. For example, if there are only 4 words \"man\", \"king\", \"woman\" and \"queen\", they can be represented as [1,0,0,0], [0,1,0,0], [0,0,1,0] and [0,0,0,1]. However, there are millions of vocabularies in the dictionary and representing all of the words by one-hot-vector requires high-dimensional vectors, which may result in out-of-memory problem in machines. In addition, we cannot make machine know the meaning of the words and relationship between two words. To solve these problems, low-dimensional word vectors are needed. In this example, if we manually choose features to represent a word, we can make machines recognize a word according to these features. For example, if there are four features \"royal\", \"normal\", \"mascular\", \"feminine\", the word \"king\" can be represented as [0.99, 0.01, 0.8, 0.1] and \"queen\" as [0.98, 0.01, 0.2, 0.8]. Still it is troublesome to select features manually, but making machines learn such kind of features may work. \n",
    "\n",
    "Before word embeddings, scholars trained neural networks to represent words in word vectors on CBOW and Skip-gram. The problem is that computing cost in output layer of a deep neural network is non-trivial. An optimal way is using word2vec model to represent words. Using large amounts of unannotated plain text, word2vec learns relationships between words automatically. The output are low-dimensional vectors with remarkable linear relationships. For example, vec(\"king\") - vec(\"man\") + vec(\"woman) = vec(\"queen\").\n",
    "\n",
    "With the help of word embedding, words are represented in an efficient way and make it possible for machines to understand the meaning of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a free Python library with following features: \n",
    "\n",
    "- Scalable statistical semantics\n",
    "\n",
    "- Analyze plain-text documents for semantic structure\n",
    "\n",
    "- Retrieve semantically similar documents\n",
    "\n",
    "Gensim is a convenient library that allows to do following tasks:\n",
    "\n",
    "- Training word embedding model\n",
    "\n",
    "- Similarity Queires\n",
    "\n",
    "- Text Summarization\n",
    "\n",
    "- Topic Modelling\n",
    "\n",
    "Gensim is a powerful tool to allow us to do many IR or NLP tasks. In this spotlight, we mainly focus on training word embedding model - including Word2Vec model and FastText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module depends on NumPy and Scipy, two Python packages for scientific computing. Before installing Gensim make sure they are also installed. Gensim depends on Python with version 2.7 or >=3.5.\n",
    "\n",
    "Gensim can be installed in one of the following ways:\n",
    "\n",
    "1. pip\n",
    "\n",
    "`pip install -U gensim`\n",
    "\n",
    "2. Anaconda\n",
    "\n",
    "`conda install -c conda-forge gensim`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to download and preprocess the dataset. The dataset can be downloaded from Kaggle https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset. It contains a list of fake and real news. In the directory \"fake-and-real-news-dataset\", there are two files - \"Fake.csv\" and \"True.csv\". The dataset used to train Word2Vec model is called **Corpus**.\n",
    "\n",
    "First, read the files and check how many rows they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23481, 4)\n",
      "(21417, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fake_df = pd.read_csv('./fake-and-real-news-dataset/Fake.csv')\n",
    "real_df = pd.read_csv('./fake-and-real-news-dataset/True.csv')\n",
    "print(fake_df.shape)\n",
    "print(real_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four columns, including title, text, subject and date. In this example, we extract sentences from the text column and form the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "2  December 30, 2017  \n",
      "3  December 29, 2017  \n",
      "4  December 25, 2017  \n",
      "                                               title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3  FBI Russia probe helped by Australian diplomat...   \n",
      "4  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "2  December 31, 2017   \n",
      "3  December 30, 2017   \n",
      "4  December 29, 2017   \n"
     ]
    }
   ],
   "source": [
    "print(fake_df.head())\n",
    "print(real_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the following steps will remove noise from the text. Let's check if there is missing value in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "dtype: int64\n",
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(fake_df.isnull().sum())\n",
    "print(real_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no missing value, we are not required to remove any missing value. \n",
    "\n",
    "The next step is preprocessing the corpus.\n",
    "\n",
    "Gensim also supports a bunch of methods of parsing and preprocessing strings and tokenizing. Usually preprocessing data needs to remove punctuations, tags, numeric, whitespaces and so on. After cleaning the data, tokenize it. Gensim provides a method **preprocess_documents** which can do the preprocessing above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "raw_dataset = []\n",
    "for row in fake_df['text']:\n",
    "    raw_dataset.append(str(row))\n",
    "for row in real_df['text']:\n",
    "    raw_dataset.append(str(row))\n",
    "sentences = preprocess_documents(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44898\n",
      "['donald', 'trump', 'couldn', 'wish', 'american', 'happi', 'new', 'year', 'leav', 'instead', 'shout', 'enemi', 'hater', 'dishonest', 'fake', 'new', 'media', 'realiti', 'star', 'job', 'couldn', 'countri', 'rapidli', 'grow', 'stronger', 'smarter', 'want', 'wish', 'friend', 'support', 'enemi', 'hater', 'dishonest', 'fake', 'new', 'media', 'happi', 'healthi', 'new', 'year', 'presid', 'angri', 'pant', 'tweet', 'great', 'year', 'america', 'countri', 'rapidli', 'grow', 'stronger', 'smarter', 'want', 'wish', 'friend', 'support', 'enemi', 'hater', 'dishonest', 'fake', 'new', 'media', 'happi', 'healthi', 'new', 'year', 'great', 'year', 'america', 'donald', 'trump', 'realdonaldtrump', 'decemb', 'trump', 'tweet', 'went', 'welll', 'expect', 'kind', 'presid', 'send', 'new', 'year', 'greet', 'like', 'despic', 'petti', 'infantil', 'gibberish', 'trump', 'lack', 'decenc', 'won', 'allow', 'rise', 'gutter', 'long', 'wish', 'american', 'citizen', 'happi', 'new', 'year', 'bishop', 'talbert', 'swan', 'talbertswan', 'decemb', 'like', 'calvin', 'calvinstowel', 'decemb', 'impeach', 'great', 'year', 'america', 'accept', 'regain', 'control', 'congress', 'miranda', 'yaver', 'mirandayav', 'decemb', 'hear', 'talk', 'includ', 'peopl', 'hate', 'wonder', 'hate', 'alan', 'sandov', 'alansandov', 'decemb', 'us', 'word', 'hater', 'new', 'year', 'wish', 'marlen', 'marlen', 'decemb', 'happi', 'new', 'year', 'koren', 'pollitt', 'korencarpent', 'decemb', 'trump', 'new', 'year', 'ev', 'tweet', 'happi', 'new', 'year', 'includ', 'enemi', 'fought', 'lost', 'badli', 'know', 'love', 'donald', 'trump', 'realdonaldtrump', 'decemb', 'new', 'trump', 'year', 'trump', 'direct', 'messag', 'enemi', 'hater', 'new', 'year', 'easter', 'thanksgiv', 'anniversari', 'pic', 'twitter', 'com', 'fpaekypa', 'daniel', 'dale', 'ddale', 'decemb', 'trump', 'holidai', 'tweet', 'clearli', 'presidenti', 'long', 'work', 'hallmark', 'presid', 'steven', 'goodin', 'sgoodin', 'decemb', 'like', 'differ', 'year', 'filter', 'break', 'roi', 'schulz', 'thbthttt', 'decemb', 'apart', 'teenag', 'us', 'term', 'hater', 'wendi', 'wendywhistl', 'decemb', 'fuck', 'year', 'old', 'know', 'rainydai', 'decemb', 'peopl', 'vote', 'hole', 'think', 'chang', 'got', 'power', 'wrong', 'year', 'old', 'men', 'chang', 'year', 'older', 'photo', 'andrew', 'burton', 'getti', 'imag']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing for the dataset, we are going to train the word2vec model and FastText model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Word2Vec Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train Word2Vec model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 34.75360059738159s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "w2v_model = Word2Vec(sentences=sentences)\n",
    "end = time.time()\n",
    "print(\"Time cost: \"+str(end-start)+\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we train our model, we can do following things: obtain the word vectors for words, retrieve the vocabulary of a model and so on. For example, we can obtain the word vector for the word \"trump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90112185 -0.06394381 -0.54695094  0.5451448   3.9546165   1.4378763\n",
      "  1.0281826  -1.2107242   1.1606402  -0.7177944   0.58483505  0.328974\n",
      " -2.9239256  -1.7333181  -0.7306151  -1.1336333   1.9647046  -0.17106111\n",
      "  0.26653388 -1.2920225  -0.85093015  0.4205638  -0.7908393  -0.65425694\n",
      "  0.84474224 -0.5225834  -2.5841691  -0.58454174 -0.275691   -0.13299435\n",
      " -2.2737842  -0.67487735 -1.6917374  -0.95548    -1.908789   -0.9495424\n",
      "  1.6894523  -0.2289263   1.6128705   0.33084643  1.6679616  -2.4691775\n",
      " -0.8925712   0.78315806  0.60559136  0.47666115  0.69272935  3.3046427\n",
      "  0.32235762 -0.6916537  -1.1403162  -1.034852   -0.6216303   0.34747735\n",
      " -1.5019106   1.6279614  -0.5610111   0.9031772   2.5842667  -3.317516\n",
      "  0.4004678   0.16276014  0.96519786 -0.97517735  1.0138454  -1.3710843\n",
      " -0.4389914   2.9887648  -0.3712641  -2.5208259  -0.49166197  1.0353457\n",
      "  1.5504236   0.8116096  -0.1808369   1.7955558  -2.576719   -0.31995437\n",
      "  1.9097854  -1.4624949  -1.2789673  -0.40529037  0.78812593 -0.87856984\n",
      " -0.18614233 -0.7792587   1.7823915  -0.05882051  3.1993148  -1.527999\n",
      "  0.5318005   0.04708174  0.5430608   1.4028046  -1.4808218   0.31356877\n",
      " -0.5301649  -2.3735425  -0.1354338   0.7514387 ]\n"
     ]
    }
   ],
   "source": [
    "vec_trump = w2v_model.wv['trump']\n",
    "print(vec_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve the vocabulary of the model and here are ten of the words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald\n",
      "trump\n",
      "couldn\n",
      "wish\n",
      "american\n",
      "happi\n",
      "new\n",
      "year\n",
      "leav\n",
      "instead\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(w2v_model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, Word2Vec supports the function of calculating similarity between two words. Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'trump'\t'american'\t0.16\n",
      "'trump'\t'donald'\t0.25\n",
      "'new'\t'tweet'\t0.05\n",
      "'dishonest'\t'fake'\t0.53\n",
      "'fake'\t'real'\t0.36\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('trump', 'american'),   \n",
    "    ('trump', 'donald'),   \n",
    "    ('new', 'tweet'),  \n",
    "    ('dishonest', 'fake'),    \n",
    "    ('fake', 'real'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, w2v_model.wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can achieve the top n words similar to a word \"trump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trump’', 0.8477027416229248), ('rumsfeld', 0.5220260620117188), ('president’', 0.5177746415138245), ('presumpt', 0.463289737701416), ('candid', 0.4583457112312317)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.most_similar(positive=['trump'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several parameters will affect the training speed and the quality of the model.\n",
    "\n",
    "**min_count**\n",
    "\n",
    "min_count is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:\n",
    "\n",
    "default value of min_count=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**size**\n",
    "\n",
    "size is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\n",
    "\n",
    "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences, size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**workers**\n",
    "\n",
    "workers are the number of threads to training the model. In Python there is only one worker due to GIL, unless Cython is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to modify the parameters and train another model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 37.1169114112854s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = Word2Vec(sentences=sentences, min_count=10, size=200)\n",
    "end = time.time()\n",
    "print(\"Time cost: \"+str(end-start)+\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now obtain the word vector of \"trump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.1083409e+00  2.8412825e-01 -4.0333152e-01  4.2425072e-01\n",
      "  1.2265630e+00  1.3978356e+00  1.9230708e+00 -7.0238733e-01\n",
      "  1.8900141e+00 -7.5629696e-02  6.2934566e-01  1.6841220e+00\n",
      " -1.0395401e+00 -8.9119613e-01  4.1590594e-02 -2.5856951e-01\n",
      "  7.0791280e-01 -1.4080112e+00  4.6715644e-01  1.3355272e-01\n",
      " -1.7386986e-01  2.5997877e-01 -2.0607309e-01 -7.9682446e-01\n",
      " -1.4383588e+00 -9.3288469e-01 -2.0245752e+00 -1.1665574e+00\n",
      " -1.3097171e-01 -6.1054868e-01  7.3613666e-02 -1.5768647e+00\n",
      " -8.1336451e-01  2.2355423e+00 -2.2096384e+00 -8.5814375e-01\n",
      "  5.5592966e-01  7.4210280e-01  1.1821085e+00  7.4717379e-01\n",
      "  4.7867921e-01 -1.3896639e+00 -6.0631406e-01 -7.3368832e-02\n",
      "  1.1755929e+00  5.3119159e-01 -3.4468132e-01  2.7831409e+00\n",
      "  2.6191559e-01 -1.0885777e+00 -3.8575473e-01  1.6791103e+00\n",
      "  3.5325789e-01  8.7553543e-01 -7.3722500e-01  9.6064168e-01\n",
      " -1.5864632e+00 -3.3833343e-01 -3.3062354e-02 -4.5033127e-01\n",
      " -8.7628447e-02 -2.3689389e-01  1.5903516e+00 -1.4993639e-01\n",
      "  2.9186693e-01  1.7612221e+00  9.1350585e-01  3.9815679e-01\n",
      " -4.3809658e-01 -1.7154714e+00 -1.5547864e+00  1.4586744e+00\n",
      "  6.8261653e-02  1.9826683e+00 -5.7839599e-02 -1.4946461e+00\n",
      " -3.3116372e+00  3.4391567e-01  3.3944345e-01 -5.6428456e-01\n",
      " -2.2447658e+00 -1.3679866e+00  7.4900991e-01  2.7321213e-01\n",
      " -3.9718997e-01 -1.5151465e+00 -2.1225424e-01  1.0713305e+00\n",
      "  1.6616769e+00 -1.6192060e+00  9.1865651e-02 -1.4471647e-01\n",
      " -6.7316510e-02  2.0231202e+00 -1.9414172e+00 -2.1446301e-01\n",
      " -2.9214469e-01 -1.9021966e+00  8.2285799e-02  1.9790741e-02\n",
      "  5.9031707e-01  2.1678264e-01 -1.1067210e+00 -7.4785537e-01\n",
      "  1.3833972e+00  1.6246892e+00  1.9140580e+00  5.3261185e-01\n",
      "  1.0233271e+00 -6.3954628e-01  1.2234451e+00  1.4133251e+00\n",
      "  1.9006845e-02  1.3839239e-01 -1.3857847e+00  4.1588953e-01\n",
      " -3.6592627e-01  4.3502957e-01  2.0645764e+00  6.7287105e-01\n",
      "  2.3398352e-01 -1.6532631e+00  1.7268619e-01 -1.1017084e+00\n",
      "  3.6054084e-01 -1.3849846e+00 -6.3869911e-01 -8.0684853e-01\n",
      " -3.7977478e-01 -4.0229818e-01 -1.9894624e+00  2.7375114e-01\n",
      "  7.3471099e-01 -4.7553545e-01 -7.3458630e-01 -3.7046161e-02\n",
      "  4.9595234e-01  7.9116291e-01  1.2027981e+00 -1.0299289e+00\n",
      " -2.3301573e+00 -1.8980543e-01  6.1403668e-01  6.7515564e-01\n",
      " -4.1119665e-01 -2.5743628e-02 -8.5024601e-01 -2.9673669e-03\n",
      "  4.2630285e-01  2.4215748e+00  9.5502764e-02 -6.0631478e-01\n",
      "  9.7606266e-01  1.4375007e+00  3.6930221e-01 -4.2285275e-01\n",
      " -1.7162032e+00 -3.1240401e-01 -8.3078963e-01  1.0640467e+00\n",
      " -5.2599568e-02 -5.9994549e-01 -1.3903886e+00 -8.4023461e-02\n",
      "  5.7115096e-01  1.0562363e+00 -1.5486649e+00  3.1378773e-01\n",
      " -4.8617163e-01  3.9281043e-01 -1.1480170e+00 -1.1131741e-01\n",
      "  1.2377791e-01  7.2095597e-01 -4.7235233e-01  4.4941339e-01\n",
      "  7.8066450e-01 -1.1052520e+00  2.6342937e-01  1.4590381e+00\n",
      " -4.3016413e-01 -6.1540227e-02  7.3870605e-01 -1.1261488e-01\n",
      "  1.2143703e-01 -2.0614305e+00  2.9612267e-01  1.1612858e+00\n",
      "  3.9218286e-01 -8.3999223e-01 -3.1546339e-01 -6.2858212e-01\n",
      "  9.0573698e-01  1.2778182e+00  1.8534099e+00  1.5923206e+00\n",
      "  5.4256177e-01 -3.5171035e-01  4.8566163e-01 -1.1821914e+00]\n"
     ]
    }
   ],
   "source": [
    "vec_trump = model.wv['trump']\n",
    "print(vec_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check ten of the vocabulary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald\n",
      "trump\n",
      "couldn\n",
      "wish\n",
      "american\n",
      "happi\n",
      "new\n",
      "year\n",
      "leav\n",
      "instead\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 FastText Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from Word2Vec, FastText does significantly better on syntactic tasks as compared to Word2Vec, especially when the size of training corpus is small. But the training time for FastText is higher than Word2Vec. FastText can be used to obtain vectors of out-of-vocabulary words, by summing up vectors for its component char-ngrams, provided at least one of the char-ngrams was present in the training data.\n",
    "\n",
    "To train FastText model, we will the Lee Corpus provided by Gensim to train our model. We are going to train FastText model in the following steps. Be careful, training on a large corpus may make the memory crash. In this spotlight, we are not going to train with the previous corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Time cost: 0.8102421760559082s'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as print\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Set file names for train and test data\n",
    "corpus_file = datapath('lee_background.cor')\n",
    "\n",
    "model = FT_gensim(size=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# train the model\n",
    "start = time.time()\n",
    "model.train(\n",
    "    corpus_file=corpus_file, epochs=model.epochs,\n",
    "    total_examples=model.corpus_count, total_words=model.corpus_total_words\n",
    ")\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time cost: \"+str(end-start)+\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train the model, gensim allows us to do following things with the model such as word vector lookup and similarity queries.\n",
    "\n",
    "Different to Word2Vec model, FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.\n",
    "\n",
    "For example, here we want to look up the word vectors of \"car\" and \"cars\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('night' in model.wv.vocab)\n",
    "print('nights' in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 0.1941219 , -0.15062791, -0.5355393 ,  0.5484641 ,  0.54986024,\n",
      "       -0.21399635, -0.19775148, -0.01883192,  0.35155442,  0.38947713,\n",
      "       -0.54005367,  0.03728401, -0.71751434,  0.42678547,  0.19532633,\n",
      "       -0.02315019, -0.22061744,  0.26909608,  0.18943693, -0.39142597,\n",
      "       -0.25171676,  0.36400837, -0.3321525 ,  0.07453673, -0.6866999 ,\n",
      "        0.73278326,  0.19264132,  0.2135463 ,  0.50038254,  0.10496826,\n",
      "       -0.6929901 ,  0.22178094,  0.05497402, -0.5014203 ,  0.37060383,\n",
      "       -0.0215224 , -0.15111026, -0.08223297,  0.46393153,  0.27434742,\n",
      "       -0.01615404, -0.04809566,  0.3160232 , -0.01038613,  0.14094114,\n",
      "        0.24010792, -0.12259277,  0.22934291, -0.04977904, -0.46853322,\n",
      "       -0.5398909 , -0.6163394 , -0.01623671, -0.01325615,  0.46008006,\n",
      "       -0.7649052 , -0.15356226, -0.238984  ,  0.09233637, -0.11875038,\n",
      "        0.22653954, -0.08755895, -0.4213021 , -0.09683564, -0.5114078 ,\n",
      "        0.26933733,  0.08372572,  0.17297511,  0.01307578,  0.50528246,\n",
      "       -0.6011253 , -0.4917206 ,  0.0550482 , -0.09982002, -0.40875623,\n",
      "        0.19871749,  0.32394505,  0.13634636, -0.09173625,  0.11228773,\n",
      "        0.4346371 , -0.04731154, -0.10175098,  0.5923649 , -0.2709999 ,\n",
      "       -0.37494823,  0.23971681,  0.22132248,  0.39942253,  0.32433394,\n",
      "       -0.15217493,  0.12114611, -0.08823058, -0.1597967 , -0.24573125,\n",
      "        0.6034518 ,  0.32011032,  0.54513013,  0.58743864,  0.4175128 ],\n",
      "      dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\cs670\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['night'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 0.16976325, -0.13037507, -0.4659069 ,  0.4763106 ,  0.47740814,\n",
      "       -0.18761593, -0.17260599, -0.01553334,  0.3055244 ,  0.33936536,\n",
      "       -0.47178084,  0.03113239, -0.62432075,  0.3725923 ,  0.16590124,\n",
      "       -0.01995055, -0.19381092,  0.23225938,  0.16306035, -0.340871  ,\n",
      "       -0.21879405,  0.317462  , -0.28945407,  0.06545433, -0.5977323 ,\n",
      "        0.63712573,  0.16729671,  0.1860668 ,  0.43515775,  0.09081806,\n",
      "       -0.6005572 ,  0.19395107,  0.0465325 , -0.43817773,  0.32224125,\n",
      "       -0.01889641, -0.12995604, -0.07268477,  0.4036368 ,  0.23933122,\n",
      "       -0.01307502, -0.0421077 ,  0.27434275, -0.0073134 ,  0.12151323,\n",
      "        0.2090702 , -0.10995457,  0.20143966, -0.04402449, -0.4061034 ,\n",
      "       -0.47224367, -0.5351996 , -0.01290479, -0.01055295,  0.40079945,\n",
      "       -0.66518825, -0.13369225, -0.20678155,  0.0778923 , -0.10292069,\n",
      "        0.19665094, -0.07701752, -0.3661156 , -0.08469696, -0.4453588 ,\n",
      "        0.23374492,  0.07293572,  0.15031497,  0.01318435,  0.44027388,\n",
      "       -0.5228024 , -0.42714784,  0.04785497, -0.08756307, -0.35538766,\n",
      "        0.17201902,  0.2819258 ,  0.11877273, -0.07771286,  0.09693872,\n",
      "        0.37875748, -0.04127169, -0.08928138,  0.51431125, -0.23475212,\n",
      "       -0.32544443,  0.21053955,  0.19374241,  0.3467881 ,  0.28291744,\n",
      "       -0.13262786,  0.10512583, -0.07809771, -0.13859871, -0.21342897,\n",
      "        0.5249243 ,  0.2774188 ,  0.4725984 ,  0.5105651 ,  0.36324888],\n",
      "      dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\cs670\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['nights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity operations work as the same way as Word2Vec. But out-of-vocabulary words can also be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\cs670\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity(\"night\",\"nights\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for evaluation, since this task is an unsupervised learning task, there is no suitable method to evaluate the model, but it is depend on the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this spotlight we introduce the definition of word embedding and the benefits of it. Then we focus on how to train Word2Vec model and FastText model using a python library Gensim and show some usages of the model. I hope this spotlight will give you a deep understanding of word embedding and Gensim. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[Fake news and real news corpora](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)\n",
    "\n",
    "[gensim: Word2Vec Documentation](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py)\n",
    "\n",
    "[Gensim: Word2Vec Tutorial](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial/notebook)\n",
    "\n",
    "[Gensim: FastText Model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
