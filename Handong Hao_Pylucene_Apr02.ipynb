{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CSCE 670 Information Storage and Retrieval Spring 2020\n",
    "---\n",
    "# Spotlight - Guideline for Pylucene\n",
    "--Handong Hao\n",
    "\n",
    "## 1.Introduction\n",
    "PyLucene is a Python extension for accessing Java Lucene. Its goal is to allow you to use Lucene's text indexing and searching capabilities from Python.PyLucene is not a Lucene port but a Python wrapper around Java Lucene. PyLucene embeds a Java VM with Lucene into a Python process. \n",
    "\n",
    "## 2.Requirements\n",
    "PyLucene is a Python extension built with JCC.To build PyLucene, JCC needs to be built first. Also, To build PyLucene, a Java Development Kit (JDK) and Ant are required; use of the resulting PyLucene binaries requires only a Java Runtime Environment (JRE).\n",
    "\n",
    "## 3.Implementation\n",
    "Here, we will use Pylucene to implement a simple search system. This work is separate into two parts, one is to create Lucene index by using the class in Pylucene, the other is searching.\n",
    "\n",
    "### 3.1 Creating a Lucene index (IndexFiles.py)\n",
    "This part is loosely based on the Lucene (java implementation) demo class  *org.apache.lucene.demo.IndexFiles*.  It will take a directory as an argument and will index all of the files in that directory and downward recursively. It will index the file path, the file name, and the file contents.  The resulting Lucene index will be placed in the current directory and called 'index'. \n",
    "\n",
    "In the beginning, let me introduce the main class we will use in Pylucene.\n",
    ">**Directory**: A Directory provides an abstraction layer for storing a list of files. A directory contains only files (no sub-folder hierarchy).\n",
    "\n",
    ">**Analyzer**: An Analyzer builds TokenStreams, which analyze text. It thus represents a policy for extracting index terms from text.\n",
    "\n",
    ">**IndexWriter**: An IndexWriter creates and maintains an index.\n",
    "\n",
    ">**Document**: Documents are the unit of indexing and search. A Document is a set of fields. Each field has a name and a textual value. A field may be stored with the document, in which case it is returned with search hits on the document. Thus each document should typically contain one or more stored fields that uniquely identify it.\n",
    "\n",
    ">**Field**: A field is a section of a Document. Each field has three parts: name, type, and value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, PyLucene, threading, time  \n",
    "from datetime import datetime  \n",
    "class Ticker(object):  \n",
    "  \n",
    "    def __init__(self):  \n",
    "        self.tick = True  \n",
    "  \n",
    "    def run(self):  \n",
    "        while self.tick:  \n",
    "            sys.stdout.write('.')  \n",
    "            sys.stdout.flush()  \n",
    "            time.sleep(1.0)  \n",
    "  \n",
    "class IndexFiles(object):  \n",
    "  \n",
    "    def __init__(self, root, storeDir, analyzer):  \n",
    "  \n",
    "        if not os.path.exists(storeDir):  \n",
    "            os.mkdir(storeDir)  \n",
    "        store = PyLucene.FSDirectory.getDirectory(storeDir, False)  \n",
    "        writer = PyLucene.IndexWriter(store, analyzer, False)  \n",
    "        writer.setMaxFieldLength(1048576)  \n",
    "        self.indexDocs(root, writer)  \n",
    "        ticker = Ticker()  \n",
    "        print 'optimizing index',  \n",
    "        threading.Thread(target=ticker.run).start()  \n",
    "        writer.optimize()  \n",
    "        writer.close()  \n",
    "        ticker.tick = False  \n",
    "        print 'done'  \n",
    "  \n",
    "    def indexDocs(self, root, writer):  \n",
    "        for root, dirnames, filenames in os.walk(root):  \n",
    "            print root  \n",
    "            try:  \n",
    "                sroot = unicode(root, 'GBK')  \n",
    "                print sroot  \n",
    "            except:  \n",
    "                print \"***************************unicode error**********************************\"  \n",
    "                print root  \n",
    "                continue  \n",
    "  \n",
    "            #add dir  \n",
    "            doc = PyLucene.Document()  \n",
    "            doc.add(PyLucene.Field(\"path\", sroot,  \n",
    "                                   PyLucene.Field.Store.YES,  \n",
    "                                   PyLucene.Field.Index.UN_TOKENIZED))  \n",
    "  \n",
    "            doc.add(PyLucene.Field(\"name\", sroot,  \n",
    "                                   PyLucene.Field.Store.YES,  \n",
    "                                   PyLucene.Field.Index.TOKENIZED))  \n",
    "            writer.addDocument(doc)  \n",
    "              \n",
    "            for filename in filenames:  \n",
    "                try:  \n",
    "                    filename = unicode(filename, 'GBK')  \n",
    "                except:  \n",
    "                    print \"***************************unicode error******************************\"  \n",
    "                    print filename  \n",
    "                    continue  \n",
    "                print \"adding\", filename  \n",
    "                try: \n",
    "                    path =os.path.join(sroot, filename)  \n",
    "                    doc = PyLucene.Document()  \n",
    "                    doc.add(PyLucene.Field(\"path\", path,  \n",
    "                                           PyLucene.Field.Store.YES,  \n",
    "                                           PyLucene.Field.Index.UN_TOKENIZED))  \n",
    "                    doc.add(PyLucene.Field(\"name\", filename,  \n",
    "                                           PyLucene.Field.Store.YES,  \n",
    "                                           PyLucene.Field.Index.TOKENIZED))  \n",
    "                    writer.addDocument(doc)  \n",
    "                except Exception, e:  \n",
    "                    print \"Failed in indexDocs:\", e  \n",
    "__debug = 0  \n",
    "if __name__ == '__main__':  \n",
    "    if __debug != 1:  \n",
    "        if len(sys.argv) < 2:  \n",
    "            print IndexFiles.__doc__  \n",
    "            sys.exit(1)  \n",
    "  \n",
    "    print 'PyLucene', PyLucene.VERSION, 'Lucene', PyLucene.LUCENE_VERSION  \n",
    "    start = datetime.now()  \n",
    "    try:  \n",
    "        if __debug != 1:  \n",
    "            IndexFiles(sys.argv[1], \"index\", PyLucene.StandardAnalyzer())  \n",
    "        else:  \n",
    "            IndexFiles(r'c:/test', \"index\", PyLucene.StandardAnalyzer())  \n",
    "        end = datetime.now()  \n",
    "        print end - start  \n",
    "    except Exception, e:  \n",
    "        print \"Failed: \", e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how it works. \n",
    "\n",
    "The main method parses the command-line parameters, then in preparation for instantiating *IndexWriter*, opens a *Directory*, and instantiates *StandardAnalyzer*. Lucene *Directory*s are used by the IndexWriter to store information in the index. In addition to the *FSDirectory* implementation we are using, there are several other Directory subclasses that can write to RAM, to a database, etc.\n",
    "\n",
    "Lucene *Analyzers* are processing pipelines that break up text into indexed tokens, also known as terms, and optionally perform other operations on these tokens, for example, downcasing, synonym insertion, filtering out unwanted tokens, etc. The Analyzer we are using is *StandardAnalyzer*, which creates tokens using the Word Break rules from the Unicode Text Segmentation algorithm specified in Unicode Standard Annex #29; converts tokens to lowercase; and then filters out stopwords. Stopwords are common language words such as articles (a, an, the) and other tokens that may have less value for searching. \n",
    "\n",
    "After *IndexWriter* is instantiated, Let us see the *indexDocs()* code. This recursive function crawls the directories and creates *Document* objects. The *Document* is simply a data object to represent the text content from the file as well as its creation time and location. These instances are added to the IndexWriter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Searching Files (SearchFiles.py)\n",
    "This script is loosely based on the Lucene (java implementation) demo class *org.apache.lucene.demo.SearchFiles*.  It will prompt for a search query, then it will search the Lucene index in the current directory called 'index' for the search query entered against the 'contents' field.  It will then display the 'path' and 'name' fields for each of the hits it finds in the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyLucene import QueryParser, IndexSearcher, StandardAnalyzer, FSDirectory  \n",
    "from PyLucene import VERSION, LUCENE_VERSION  \n",
    "  \n",
    "def run(searcher, analyzer):  \n",
    "    while True:  \n",
    "        print  \n",
    "        print \"Hit enter with no input to quit.\"  \n",
    "        command = raw_input(\"Query:\")  \n",
    "        command = unicode(command, 'GBK')  \n",
    "        if command == '':  \n",
    "            return  \n",
    "  \n",
    "        print  \n",
    "        print \"Searching for:\", command  \n",
    "        query = QueryParser(\"name\", analyzer).parse(command)  \n",
    "        hits = searcher.search(query)  \n",
    "        print \"%s total matching documents.\" % hits.length()  \n",
    "  \n",
    "        for i, doc in hits:  \n",
    "            print 'path:', doc.get(\"path\"), 'name:', doc.get(\"name\")  \n",
    "  \n",
    "  \n",
    "if __name__ == '__main__':  \n",
    "    STORE_DIR = \"index\"  \n",
    "    print 'PyLucene', VERSION, 'Lucene', LUCENE_VERSION  \n",
    "    directory = FSDirectory.getDirectory(STORE_DIR, False)  \n",
    "    searcher = IndexSearcher(directory)  \n",
    "    analyzer = StandardAnalyzer()  \n",
    "    run(searcher, analyzer)  \n",
    "    searcher.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It primarily collaborates with an *IndexSearcher*, *StandardAnalyzer*, (which is used in the 3.1 as well) and a *QueryParser*. The query parser is constructed with an analyzer used to interpret query text in the same way the documents are interpreted: finding word boundaries, downcasing, and removing stop words. The *Query* object contains the results from the *QueryParser* which is passed to the searcher. Note that it's also possible to programmatically construct a rich *Query* object without using the query parser. The query parser just enables decoding the *Lucene query syntax* into the corresponding *Query* object.\n",
    "\n",
    "Now, in the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python IndexFiles.py c:/\n",
    "python SearchFiles.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to find only one keyword, inpu it directly; if you want to find two keywords at the same time, such as \"Python network\", input: \"Python AND network\"; if you want to find \"Python\" or \"\"network\", input: \"Python network\"(or \"Python OR network\").\n",
    "\n",
    "\n",
    "## Reference\n",
    "[https://freethreads.net/2012/09/17/pylucene-part-i-creating-index/](https://freethreads.net/2012/09/17/pylucene-part-i-creating-index/)\n",
    "\n",
    "[https://medium.com/@michaelaalcorn/how-to-use-pylucene-e2e2f540024c](https://medium.com/@michaelaalcorn/how-to-use-pylucene-e2e2f540024c)\n",
    "\n",
    "[https://svn.apache.org/viewvc/lucene/pylucene/trunk/test3/](https://svn.apache.org/viewvc/lucene/pylucene/trunk/test3/)\n",
    "\n",
    "[https://lucene.apache.org/pylucene/features.html](https://lucene.apache.org/pylucene/features.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
