{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shravani Sridhar_NimbusML_April09.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQS1ys-H9ZqQ",
        "colab_type": "text"
      },
      "source": [
        "# **CSCE 670 (Information Storage and Retrieval) Spotlight**\n",
        "\n",
        "*Submitted by: Shravani Sridhar (430000439)*\n",
        "\n",
        "*On: Apr 9th, 2020*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Y21aSg9uXk",
        "colab_type": "text"
      },
      "source": [
        "# **NimbusML**\n",
        "\n",
        "NimbusML is a Python machine learning package from Microsoft that provides ML algorithms, transforms, and components. It is a Python bindings framework for ML.NET and provides simple interoperability between ML.NET and Scikit-learn components. \n",
        "\n",
        "In this Spotlight notebook, I will showcase NimbusML's NLP features, particularly in sentiment analysis and text processing. These NLP tasks are especially important in search and recommender systems; therefore they are relevant to our course studies.\n",
        "\n",
        "**Contents:**\n",
        "\n",
        "*   Installation\n",
        "      - We will see how to install NimbusML in Google Colab as well as in the terminal using pip.\n",
        "*   I. Sentiment Analysis\n",
        "      - We will see how we can use various transforms in NimbusML to perform sentiment analysis with the help of a classifier.\n",
        "*   II. Removing Stopwords\n",
        "      - We will explore NimbusML's classes for stopword removal.\n",
        "*   III. Working with N-grams\n",
        "      - We will explore in detail transforms in NimbusML that help in feature extraction and working with n-grams.\n",
        "*   IV. Word Embedding\n",
        "      - We will see how we can perform word embedding using NimbusML's WordEmbedding class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5pAY_-V_ijf",
        "colab_type": "text"
      },
      "source": [
        "**Installation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owl5BJknlY3w",
        "colab_type": "text"
      },
      "source": [
        "The version of NimbusML I am using in my notebook is nimbusml 1.7.0.\n",
        "\n",
        "To install nimbusml 1.7.0:\n",
        "\n",
        "(i) in Google Colab:\n",
        "\n",
        "*!pip install nimbusml*\n",
        "\n",
        "(ii) using pip in terminal:\n",
        "\n",
        "*pip install nimbusml*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6CqdUj9591f",
        "colab_type": "text"
      },
      "source": [
        "**I. Sentiment Analysis**\n",
        "\n",
        "We will see how we can use NimbusML's various transforms to perform sentiment analysis using classifiers. \n",
        "\n",
        "(i) First, we will detect sentiments of tweets by building a binary classifier using manually generated Twitter text data. We will do this with the help of the Pandas library.\n",
        "\n",
        "We will use text as the input (text column) and the sentiments as the output (sentiment column as the label)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Q7Sxw3yUco",
        "colab_type": "code",
        "outputId": "ab04fe44-f1a0-451e-9611-b4cbf8e603f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from IPython.display import display\n",
        "from nimbusml.datasets import get_dataset\n",
        "from nimbusml.feature_extraction.text import NGramFeaturizer\n",
        "from nimbusml.feature_extraction.text.extractor import Ngram\n",
        "from nimbusml.linear_model import AveragedPerceptronBinaryClassifier\n",
        "from nimbusml.decomposition import PcaTransformer\n",
        "from nimbusml import Pipeline\n",
        "\n",
        "#Loading training and test data from package\n",
        "#This Twitter data contains text\n",
        "trainDataFile = get_dataset('gen_twittertrain').as_filepath()\n",
        "testDataFile = get_dataset('gen_twittertest').as_filepath()\n",
        "\n",
        "print(\"Train data file path: \" + str(os.path.basename(trainDataFile)))\n",
        "print(\"Test data file path: \" + str(os.path.basename(testDataFile)))\n",
        "\n",
        "trainData = pd.read_csv(trainDataFile, sep = \"\\t\")\n",
        "testData = pd.read_csv(testDataFile, sep = \"\\t\")\n",
        "\n",
        "trainData.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data file path: train-twitter.gen-sample.tsv\n",
            "Test data file path: test-twitter.gen-sample.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Negative</td>\n",
              "      <td>Oh you are hurting me</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Positive</td>\n",
              "      <td>So long</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Ths sofa is comfortable</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Negative</td>\n",
              "      <td>The place suck.    No?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Positive</td>\n",
              "      <td>@fakeid &amp;quot;Chillin&amp;quot; I love it!!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Sentiment                                     Text  Label\n",
              "0  Negative                    Oh you are hurting me      0\n",
              "1  Positive                                  So long      1\n",
              "2  Positive                  Ths sofa is comfortable      1\n",
              "3  Negative                   The place suck.    No?      0\n",
              "4  Positive  @fakeid &quot;Chillin&quot; I love it!!      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk3wPz_Onh_7",
        "colab_type": "text"
      },
      "source": [
        "The above output contains the first 5 entries of the training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmRD2an27Hml",
        "colab_type": "text"
      },
      "source": [
        "We need to preprocess the data.\n",
        "\n",
        "NGramFeaturizer is a transform in NimbusML that outputs a bag of counts of sequences of consecutive words, called n-grams, from a given corpus of text. The word counts can then be normalized using the term frequency-inverse document frequency (TF-IDF) method by passing the appropriate parameter to the feature extractor.\n",
        "\n",
        "For all the examples unless otherwise mentioned, we will use the NGram feature extractor. A feature extractor extracts n-grams from text and converts them to vector form using a dictionary. We will explore this in more detail in subsequent sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PFP11MEHJXK",
        "colab_type": "code",
        "outputId": "7543c338-ae16-40a2-b8b9-0db77f0879c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "featurizer = NGramFeaturizer(word_feature_extractor=Ngram(weighting = 'TfIdf')) #Passing the TF-IDF weighting parameter to the feature extractor\n",
        "\n",
        "#Training the featurizer \n",
        "text_transformed = featurizer.fit_transform(trainData[\"Text\"].to_frame()) #The text column is our input\n",
        "\n",
        "print(text_transformed.shape)\n",
        "text_transformed.head(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(71, 1007)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text.Char.&lt;␂&gt;|o|h</th>\n",
              "      <th>Text.Char.o|h|&lt;␠&gt;</th>\n",
              "      <th>Text.Char.h|&lt;␠&gt;|y</th>\n",
              "      <th>Text.Char.&lt;␠&gt;|y|o</th>\n",
              "      <th>Text.Char.y|o|u</th>\n",
              "      <th>Text.Char.o|u|&lt;␠&gt;</th>\n",
              "      <th>Text.Char.u|&lt;␠&gt;|a</th>\n",
              "      <th>Text.Char.&lt;␠&gt;|a|r</th>\n",
              "      <th>Text.Char.a|r|e</th>\n",
              "      <th>Text.Char.r|e|&lt;␠&gt;</th>\n",
              "      <th>Text.Char.e|&lt;␠&gt;|h</th>\n",
              "      <th>Text.Char.&lt;␠&gt;|h|u</th>\n",
              "      <th>Text.Char.h|u|r</th>\n",
              "      <th>Text.Char.u|r|t</th>\n",
              "      <th>Text.Char.r|t|i</th>\n",
              "      <th>Text.Char.t|i|n</th>\n",
              "      <th>Text.Char.i|n|g</th>\n",
              "      <th>Text.Char.n|g|&lt;␠&gt;</th>\n",
              "      <th>Text.Char.g|&lt;␠&gt;|m</th>\n",
              "      <th>Text.Char.&lt;␠&gt;|m|e</th>\n",
              "      <th>Text.Char.m|e|&lt;␃&gt;</th>\n",
              "      <th>Text.Char.&lt;␂&gt;|s|o</th>\n",
              "      <th>Text.Char.s|o|&lt;␠&gt;</th>\n",
              "      <th>Text.Char.o|&lt;␠&gt;|l</th>\n",
              "      <th>Text.Char.&lt;␠&gt;|l|o</th>\n",
              "      <th>Text.Char.l|o|n</th>\n",
              "      <th>Text.Char.o|n|g</th>\n",
              "      <th>Text.Char.n|g|&lt;␃&gt;</th>\n",
              "      <th>Text.Char.&lt;␂&gt;|t|h</th>\n",
              "      <th>Text.Char.t|h|s</th>\n",
              "      <th>Text.Char.h|s|&lt;␠&gt;</th>\n",
              "      <th>Text.Char.s|&lt;␠&gt;|s</th>\n",
              "      <th>Text.Char.&lt;␠&gt;|s|o</th>\n",
              "      <th>Text.Char.s|o|f</th>\n",
              "      <th>Text.Char.o|f|a</th>\n",
              "      <th>Text.Char.f|a|&lt;␠&gt;</th>\n",
              "      <th>Text.Char.a|&lt;␠&gt;|i</th>\n",
              "      <th>Text.Char.&lt;␠&gt;|i|s</th>\n",
              "      <th>Text.Char.i|s|&lt;␠&gt;</th>\n",
              "      <th>Text.Char.s|&lt;␠&gt;|c</th>\n",
              "      <th>...</th>\n",
              "      <th>Text.Word.her</th>\n",
              "      <th>Text.Word.demo?</th>\n",
              "      <th>Text.Word.up</th>\n",
              "      <th>Text.Word.all</th>\n",
              "      <th>Text.Word.night</th>\n",
              "      <th>Text.Word.visit</th>\n",
              "      <th>Text.Word.everyday</th>\n",
              "      <th>Text.Word.icecream</th>\n",
              "      <th>Text.Word.store</th>\n",
              "      <th>Text.Word.nearby</th>\n",
              "      <th>Text.Word.b</th>\n",
              "      <th>Text.Word.subject</th>\n",
              "      <th>Text.Word.ranked</th>\n",
              "      <th>Text.Word.out</th>\n",
              "      <th>Text.Word.week</th>\n",
              "      <th>Text.Word.beautiful</th>\n",
              "      <th>Text.Word.isn't</th>\n",
              "      <th>Text.Word.friday!</th>\n",
              "      <th>Text.Word.tgif</th>\n",
              "      <th>Text.Word.nonononononononono..</th>\n",
              "      <th>Text.Word.we</th>\n",
              "      <th>Text.Word.proud</th>\n",
              "      <th>Text.Word.haven't</th>\n",
              "      <th>Text.Word.prepared</th>\n",
              "      <th>Text.Word.lunch?</th>\n",
              "      <th>Text.Word.trust</th>\n",
              "      <th>Text.Word.try</th>\n",
              "      <th>Text.Word.as</th>\n",
              "      <th>Text.Word.replacement.</th>\n",
              "      <th>Text.Word.should</th>\n",
              "      <th>Text.Word.upset</th>\n",
              "      <th>Text.Word.vocation</th>\n",
              "      <th>Text.Word.flight</th>\n",
              "      <th>Text.Word.late</th>\n",
              "      <th>Text.Word.again</th>\n",
              "      <th>Text.Word.commute</th>\n",
              "      <th>Text.Word.died</th>\n",
              "      <th>Text.Word.cancer</th>\n",
              "      <th>Text.Word.oh,</th>\n",
              "      <th>Text.Word.finally</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.218218</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.377964</td>\n",
              "      <td>0.377964</td>\n",
              "      <td>0.377964</td>\n",
              "      <td>0.377964</td>\n",
              "      <td>0.377964</td>\n",
              "      <td>0.377964</td>\n",
              "      <td>0.377964</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.145865</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1007 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Text.Char.<␂>|o|h  Text.Char.o|h|<␠>  ...  Text.Word.oh,  Text.Word.finally\n",
              "0           0.218218           0.218218  ...            0.0                0.0\n",
              "1           0.000000           0.000000  ...            0.0                0.0\n",
              "2           0.000000           0.000000  ...            0.0                0.0\n",
              "3           0.000000           0.000000  ...            0.0                0.0\n",
              "4           0.000000           0.000000  ...            0.0                0.0\n",
              "\n",
              "[5 rows x 1007 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DhguA8b9wXb",
        "colab_type": "text"
      },
      "source": [
        "In the above output, all the columns are features from the first 5 entries of the input training text data. We can then train our binary classifier using these features.\n",
        "\n",
        "Our binary classifier will be an AveragedPerceptron classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytLCSfmvHXhn",
        "colab_type": "code",
        "outputId": "36bf1aa7-93f5-40de-e781-7ced0923b524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "ag = AveragedPerceptronBinaryClassifier()\n",
        "ag.fit(text_transformed, 1 * (trainData[\"Sentiment\"] == \"Positive\")) #Using the transformed data from above as input to the classifier"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.\n",
            "Training calibrator.\n",
            "Elapsed time: 00:00:00.6856655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AveragedPerceptronBinaryClassifier(averaged=True, averaged_tolerance=0.01,\n",
              "                                   caching='Auto', decrease_learning_rate=False,\n",
              "                                   feature=None, initial_weights=None,\n",
              "                                   initial_weights_diameter=0.0,\n",
              "                                   l2_regularization=0.0, label=None,\n",
              "                                   lazy_update=True, learning_rate=1.0,\n",
              "                                   loss='hinge', normalize='Auto',\n",
              "                                   number_of_iterations=1, recency_gain=0.0,\n",
              "                                   recency_gain_multiplicative=False,\n",
              "                                   reset_weights_after_x_examples=None,\n",
              "                                   shuffle=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKkk83lO-hX-",
        "colab_type": "text"
      },
      "source": [
        "Instead of training the featurizer and the classifier separately like we did above, we can also use the NimbusML Pipeline to train them together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLAxO9ZKHgw2",
        "colab_type": "code",
        "outputId": "dd83ee5a-04c1-432d-c2e0-1c1589d47697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "#Training the featurizer and the classifier simultaneously using a pipeline\n",
        "\n",
        "#We are employing TF-IDF normalization and an L2 regularization parameter of 0.4\n",
        "ppl = Pipeline([NGramFeaturizer(word_feature_extractor=Ngram(weighting = 'Tf')), PcaTransformer(rank = 100), \n",
        "                AveragedPerceptronBinaryClassifier(l2_regularization=0.4, number_of_iterations=5),])\n",
        "\n",
        "#Fitting the pipeline model using the training dataframe\n",
        "ppl.fit(trainData[\"Text\"], trainData[\"Label\"]) \n",
        "\n",
        "print(\"Training time: \"  + str(round(time.time() - t0, 2)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.\n",
            "Training calibrator.\n",
            "Elapsed time: 00:00:00.2393713\n",
            "Training time: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y79PsVkv_XLf",
        "colab_type": "text"
      },
      "source": [
        "We can then test the above pipeline-trained model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouepuFCdIN6v",
        "colab_type": "code",
        "outputId": "5cb046e0-0107-4cfb-8ec2-f611ad0eda2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "#Testing the model\n",
        "metrics, scores = ppl.test(testData[\"Text\"], testData[\"Label\"], output_scores = True)  \n",
        "\n",
        "print(\"Performance metrics: \")\n",
        "display(metrics)\n",
        "print(\"\\nIndividual scores: \")\n",
        "\n",
        "#Appending origin text to the score\n",
        "scores[\"OriginText\"] = testData[\"Text\"]\n",
        "scores[\"Sentiment\"] = testData[\"Sentiment\"]\n",
        "\n",
        "display(scores[0:5]) #Displaying the first 5 scores\n",
        "print(\"Total runtime: \"  + str(round(time.time() - t0, 2)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance metrics: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AUC</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Positive precision</th>\n",
              "      <th>Positive recall</th>\n",
              "      <th>Negative precision</th>\n",
              "      <th>Negative recall</th>\n",
              "      <th>Log-loss</th>\n",
              "      <th>Log-loss reduction</th>\n",
              "      <th>Test-set entropy (prior Log-Loss/instance)</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>AUPRC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.580762</td>\n",
              "      <td>0.662791</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.662791</td>\n",
              "      <td>1</td>\n",
              "      <td>1.284392</td>\n",
              "      <td>-0.392864</td>\n",
              "      <td>0.922123</td>\n",
              "      <td>0</td>\n",
              "      <td>0.480434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        AUC  Accuracy  ...  F1 Score     AUPRC\n",
              "0  0.580762  0.662791  ...         0  0.480434\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Individual scores: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PredictedLabel</th>\n",
              "      <th>Score</th>\n",
              "      <th>Probability</th>\n",
              "      <th>OriginText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.233300</td>\n",
              "      <td>0.255333</td>\n",
              "      <td>@faketwitterid I am sad</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.270544</td>\n",
              "      <td>0.106594</td>\n",
              "      <td>@wakeup_you  It is a very simple twit I created</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.245938</td>\n",
              "      <td>0.193317</td>\n",
              "      <td>@anotherfakeid I would love to see the latest ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.269664</td>\n",
              "      <td>0.108991</td>\n",
              "      <td>Oh my ladygaga! I haven't played tennis for 2 ...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.150562</td>\n",
              "      <td>0.781553</td>\n",
              "      <td>I am heading on a road trip and taking a few d...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PredictedLabel  ...  Sentiment\n",
              "0               0  ...   Negative\n",
              "1               0  ...   Negative\n",
              "2               0  ...   Positive\n",
              "3               0  ...   Negative\n",
              "4               0  ...   Positive\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Total runtime: 129.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLjoUQnJ_uTk",
        "colab_type": "text"
      },
      "source": [
        "Therefore, we have obtained predicted sentiment labels for the first 5 entries of input Twitter test text data using a binary classifier.\n",
        "\n",
        "The test accuracy obtained is 0.662791 and the AUC (Area Under the Curve) is 0.580762."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-JvmOlXPqQe",
        "colab_type": "text"
      },
      "source": [
        "(ii) Now we will see how we can do the same Sentiment Analysis using nimbusml to load the data instead of Pandas.\n",
        "\n",
        "While Pandas saves the entire dataset in memory, nimbusml processes the data by passing a DataFileStream in the training/testing process, thus attaining exponential speed up.\n",
        "\n",
        "We will again use the manually generated Twitter text data like we did in (i)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7m4xbt8Pqp8",
        "colab_type": "code",
        "outputId": "12a672b1-4591-4e75-a03e-4b20388d3079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Getting the input file path from package\n",
        "import os\n",
        "from nimbusml.datasets import get_dataset\n",
        "from nimbusml import FileDataStream \n",
        "\n",
        "train_file = get_dataset('gen_twittertrain').as_filepath()\n",
        "test_file = get_dataset('gen_twittertest').as_filepath()\n",
        "\n",
        "print(os.path.basename(train_file))\n",
        "print(os.path.basename(test_file))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train-twitter.gen-sample.tsv\n",
            "test-twitter.gen-sample.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAG2pIVwPyHY",
        "colab_type": "code",
        "outputId": "a6a5d4d3-0787-4793-d52b-cf1a0def180f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Generating file schema\n",
        "data_stream_train = FileDataStream.read_csv(train_file, sep='\\t')\n",
        "data_stream_test = FileDataStream.read_csv(test_file, sep='\\t')\n",
        "data_stream_train.schema"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataSchema([DataColumn(name='Sentiment', type='TX', pos=0),\n",
              "    DataColumn(name='Text', type='TX', pos=1),\n",
              "    DataColumn(name='Label', type='I8', pos=2)], header=True,\n",
              "    sep='\\t')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "abd68529-26af-4683-fc71-997974eed2e7",
        "id": "Gwkgz8vNBIP8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import time\n",
        "from IPython.display import display\n",
        "from nimbusml.feature_extraction.text import NGramFeaturizer\n",
        "from nimbusml.feature_extraction.text.extractor import Ngram\n",
        "from nimbusml.linear_model import AveragedPerceptronBinaryClassifier\n",
        "from nimbusml.decomposition import PcaTransformer\n",
        "from nimbusml import Pipeline, FileDataStream\n",
        "\n",
        "columns = {\"features\":\"Text\"}\n",
        "columns = [\"features\"]\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "#Using a pipeline to train the NGramFeaturizer and the binary classifier simultaneously\n",
        " \n",
        "#We are employing TF-IDF normalization and an L2 regularization parameter of 0.3\n",
        "ppl = Pipeline([NGramFeaturizer(word_feature_extractor=Ngram(weighting = 'TfIdf', ngram_length=2), char_feature_extractor=Ngram(weighting = 'Tf',\n",
        "                                                                                                                                ngram_length=3),\n",
        "                                columns = {\"features\": \"Text\"}), PcaTransformer(rank = 80, columns = \"features\"), \n",
        "                AveragedPerceptronBinaryClassifier(l2_regularization=0.3, number_of_iterations=3, feature = [\"features\"], label = \"Label\"),])\n",
        "\n",
        "#The model is fitted using the FileDataStream created by the training file name, \n",
        "#unlike with Pandas where the model was fit using the entire dataframe\n",
        "#The data file is then loaded using NimbusML data loader in a streamline, after which the FileDataStream is passed in the pipeline.\n",
        "ppl.fit(data_stream_train)\n",
        "\n",
        "print(\"Training time: \"  + str(round(time.time() - t0, 2)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.\n",
            "Training calibrator.\n",
            "Elapsed time: 00:00:00.2082816\n",
            "Training time: 0.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G2XI-i-BXMQ",
        "colab_type": "text"
      },
      "source": [
        "In general, using nimbusml to load the dataset in this way is much faster than loading the whole dataset into memory beforehand, like we do using Pandas, since all the processes have been optimized to boost performance.\n",
        "\n",
        "Now we can test the above model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrDs-XVTQske",
        "colab_type": "code",
        "outputId": "28ca7bf7-a70a-41d1-95cc-be73e175adf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "#Testing the pipeline-trained model using the FileDataStream of the test file\n",
        "metrics, scores = ppl.test(data_stream_test, output_scores = True)\n",
        "\n",
        "print(\"Performance metrics: \")\n",
        "display(metrics)\n",
        "print(\"\\nIndividual scores: \")\n",
        "\n",
        "display(scores[0:5]) #Displaying the first 5 scores\n",
        "print(\"Total runtime: \" + str(round(time.time() - t0)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance metrics: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AUC</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Positive precision</th>\n",
              "      <th>Positive recall</th>\n",
              "      <th>Negative precision</th>\n",
              "      <th>Negative recall</th>\n",
              "      <th>Log-loss</th>\n",
              "      <th>Log-loss reduction</th>\n",
              "      <th>Test-set entropy (prior Log-Loss/instance)</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>AUPRC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.550514</td>\n",
              "      <td>0.627907</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.650602</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.123148</td>\n",
              "      <td>-0.218002</td>\n",
              "      <td>0.922123</td>\n",
              "      <td>0</td>\n",
              "      <td>0.340803</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        AUC  Accuracy  ...  F1 Score     AUPRC\n",
              "0  0.550514  0.627907  ...         0  0.340803\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Individual scores: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PredictedLabel</th>\n",
              "      <th>Score</th>\n",
              "      <th>Probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.179711</td>\n",
              "      <td>0.032265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.046403</td>\n",
              "      <td>0.600952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.105915</td>\n",
              "      <td>0.215577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.131839</td>\n",
              "      <td>0.115820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.088173</td>\n",
              "      <td>0.313349</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PredictedLabel     Score  Probability\n",
              "0               0 -0.179711     0.032265\n",
              "1               0 -0.046403     0.600952\n",
              "2               0 -0.105915     0.215577\n",
              "3               0 -0.131839     0.115820\n",
              "4               0 -0.088173     0.313349"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Total runtime: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdPeMG4eDF-i",
        "colab_type": "text"
      },
      "source": [
        "Therefore, we have the predicted sentiment labels and scores of the first 5 entries of input Twitter test text data using a binary classifier, using the help of nimbusml to load and process data. \n",
        "\n",
        "We also obtained a test accuracy of 0.627907 and an AUC of 0.550514."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDHmU8lMT3Vh",
        "colab_type": "text"
      },
      "source": [
        "(iii) We will now see how we can combine Scikit-learn with nimbusml to develop a binary classifier that can perform Sentiment Analysis.\n",
        "\n",
        "In particular, we will use a binary classifier to identify if a comment on a Wikipedia talk webpage is a personal attack or not.\n",
        "\n",
        "We will use the Wikipedia Detox dataset, and take the comment column as the input and the (sentiment) label column as the label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HVD06FnI4Hc",
        "colab_type": "text"
      },
      "source": [
        "We will create two models for the purpose of demonstration. Like in (ii), the data will be loaded and processed using nimbusml's DataFileStream.\n",
        "\n",
        "Our first model will be based on an sklearn pipeline with NimbusML's NGramFeaturizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwMs7zgtRRlD",
        "colab_type": "code",
        "outputId": "210635c4-bdeb-49aa-c17a-c6289fbedd5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import sklearn\n",
        "import os\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from nimbusml.datasets import get_dataset\n",
        "from nimbusml.feature_extraction.text.extractor import Ngram\n",
        "from nimbusml.feature_extraction.text import NGramFeaturizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer as sklearn_TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD as sklearn_TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline as sklearn_ppl\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import auc as sk_auc\n",
        "\n",
        "#Function to plot the ROC (Receiver Operating Characteristic) curve\n",
        "def plot_roc_score(label_test, score):\n",
        "    x, y, _ = roc_curve(label_test, score)\n",
        "    roc_auc = sk_auc(x, y)\n",
        "    \n",
        "    plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(x, y, color='darkorange',\n",
        "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate') #the x-axis represents the false positive rate\n",
        "    plt.ylabel('True Positive Rate') #the y-axis represents the true positive rate\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "#Obtaining input data file path from the package\n",
        "Train_file = get_dataset('wiki_detox_train').as_filepath()\n",
        "Test_file = get_dataset('wiki_detox_test').as_filepath()\n",
        "\n",
        "print(os.path.basename(Train_file))\n",
        "print(os.path.basename(Test_file))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train-250.wikipedia.sample.tsv\n",
            "test.wikipedia.sample.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nSPktnwRXFY",
        "colab_type": "code",
        "outputId": "c7d7479a-ab88-4500-8bf7-444c9a55407d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Loading the data\n",
        "dataTrain = pd.read_csv(Train_file, sep = \"\\t\")\n",
        "dataTest = pd.read_csv(Test_file, sep = \"\\t\")\n",
        "\n",
        "dataTrain.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>==RUDE== Dude, you are rude upload that carl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>== OK! ==  IM GOING TO VANDALIZE WILD ONES W...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Stop trolling, zapatancas, calling me a lia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>==You're cool==  You seem like a really cool...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>::::: Why are you threatening me? I'm not bei...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText\n",
              "0          1    ==RUDE== Dude, you are rude upload that carl...\n",
              "1          1    == OK! ==  IM GOING TO VANDALIZE WILD ONES W...\n",
              "2          1     Stop trolling, zapatancas, calling me a lia...\n",
              "3          1    ==You're cool==  You seem like a really cool...\n",
              "4          1   ::::: Why are you threatening me? I'm not bei..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awd7pZ7xqeVW",
        "colab_type": "text"
      },
      "source": [
        "The above output shows the first 5 entries of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9DcCTe3Rxje",
        "colab_type": "code",
        "outputId": "ce0d1aba-118f-42a5-83ac-96d54a536b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "featurizer = NGramFeaturizer(word_feature_extractor=Ngram()) #Creating our nimbusml featurizer\n",
        "\n",
        "#Using sklearn's Truncated SVD (Singular Value Decomposition) for linear dimensionality reduction\n",
        "svd = sklearn_TruncatedSVD(random_state = 1, n_components = 400)  \n",
        "\n",
        "#Using logistic regression from sklearn as our binary classifier\n",
        "lr = sklearn.linear_model.LogisticRegression()\n",
        "\n",
        "#Creating an sklearn pipeline using the above\n",
        "sk_ppl = sklearn_ppl([(\"featurizer\",featurizer), (\"svd\",svd), (\"lr\", lr)]) \n",
        "\n",
        "#Training the above pipeline\n",
        "sk_ppl.fit(dataTrain[[\"SentimentText\"]], dataTrain[\"Sentiment\"]) \n",
        "\n",
        "train_time_sec = time.time() - t0\n",
        "print(\"Training time: \" + str(round(train_time_sec,2)) + \"s\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 1.64s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UVyAEIZSOLC",
        "colab_type": "code",
        "outputId": "d02c72a9-9079-45d8-d6bf-001ab2584477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "#Testing the pipeline-trained model on the test data\n",
        "Y_pred = sk_ppl.predict(dataTest[\"SentimentText\"].to_frame()) \n",
        "Y_prob = sk_ppl.predict_proba(dataTest[\"SentimentText\"].to_frame())\n",
        "\n",
        "test_time_sec = time.time() - t0\n",
        "print(\"Testing time: \" + str(round(test_time_sec,2)) + \"s\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing time: 1.02s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJrAQeZySV2R",
        "colab_type": "code",
        "outputId": "355b29dc-b78b-4638-9b5a-3171bcc89a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#Plotting the ROC (Receiver Operating Characteristic) curve based on the test data\n",
        "plot_roc_score(dataTest[\"Sentiment\"], Y_prob[:,1]);"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCmOMMan6f1u1ZwoJm4kcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLiOnnZEOEfz",
        "colab_type": "text"
      },
      "source": [
        "Our second model will be based on a pure nimbusml pipeline with NimbusML's NGramFeaturizer, WordEmbedding, and FastLinearBinaryClassifier.\n",
        "\n",
        "Our binary classifier will be a FastLinearBinaryClassifier. \n",
        "\n",
        "NimbusML's WordEmbedding is based on a pre-trained DNN model to generate word embeddings for given corps. NimbusML supports many kinds of word embedding models; we will be using a Sentiment-Specific Word Embedding (SSWE) model. That is, the pre-trained DNN model our word embedding will be based on is the SSWE model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8JBUTfZSjDb",
        "colab_type": "code",
        "outputId": "99aa7cda-234b-4af0-c4cf-f32bcc0945d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "import time\n",
        "from nimbusml.linear_model import FastLinearBinaryClassifier\n",
        "from nimbusml.feature_extraction.text import WordEmbedding\n",
        "from nimbusml import Pipeline, FileDataStream\n",
        "\n",
        "#Creating the featurizer, which will convert the input text into a format acceptable by the SSWE \n",
        "featurizer = NGramFeaturizer(columns=['SentimentText'], output_tokens_column_name='SentimentText_TransformedText') \n",
        "#The column containing the output tokens will be named 'SentimentText_TransformedText'\n",
        "\n",
        "#Creating our word embedding model (SSWE)\n",
        "glove = WordEmbedding(columns='SentimentText_TransformedText', model_kind='SentimentSpecificWordEmbedding') \n",
        "\n",
        "ppl = Pipeline([featurizer, glove]) #Creating our nimbusml pipeline using the above\n",
        "ppl.fit_transform(dataTrain[[\"SentimentText\"]])[0:3] #Training the pipeline"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText.Char.&lt;␂&gt;|&lt;␠&gt;|&lt;␠&gt;</th>\n",
              "      <th>SentimentText.Char.&lt;␠&gt;|&lt;␠&gt;|=</th>\n",
              "      <th>SentimentText.Char.&lt;␠&gt;|=|=</th>\n",
              "      <th>SentimentText.Char.=|=|r</th>\n",
              "      <th>SentimentText.Char.=|r|u</th>\n",
              "      <th>SentimentText.Char.r|u|d</th>\n",
              "      <th>SentimentText.Char.u|d|e</th>\n",
              "      <th>SentimentText.Char.d|e|=</th>\n",
              "      <th>SentimentText.Char.e|=|=</th>\n",
              "      <th>SentimentText.Char.=|=|&lt;␠&gt;</th>\n",
              "      <th>SentimentText.Char.=|&lt;␠&gt;|d</th>\n",
              "      <th>SentimentText.Char.&lt;␠&gt;|d|u</th>\n",
              "      <th>SentimentText.Char.d|u|d</th>\n",
              "      <th>SentimentText.Char.d|e|,</th>\n",
              "      <th>SentimentText.Char.e|,|&lt;␠&gt;</th>\n",
              "      <th>SentimentText.Char.,|&lt;␠&gt;|y</th>\n",
              "      <th>SentimentText.Char.&lt;␠&gt;|y|o</th>\n",
              "      <th>SentimentText.Char.y|o|u</th>\n",
              "      <th>SentimentText.Char.o|u|&lt;␠&gt;</th>\n",
              "      <th>SentimentText.Char.u|&lt;␠&gt;|a</th>\n",
              "      <th>SentimentText.Char.&lt;␠&gt;|a|r</th>\n",
              "      <th>SentimentText.Char.a|r|e</th>\n",
              "      <th>SentimentText.Char.r|e|&lt;␠&gt;</th>\n",
              "      <th>SentimentText.Char.e|&lt;␠&gt;|r</th>\n",
              "      <th>SentimentText.Char.&lt;␠&gt;|r|u</th>\n",
              "      <th>SentimentText.Char.d|e|&lt;␠&gt;</th>\n",
              "      <th>SentimentText.Char.e|&lt;␠&gt;|u</th>\n",
              "      <th>SentimentText.Char.&lt;␠&gt;|u|p</th>\n",
              "      <th>SentimentText.Char.u|p|l</th>\n",
              "      <th>SentimentText.Char.p|l|o</th>\n",
              "      <th>SentimentText.Char.l|o|a</th>\n",
              "      <th>SentimentText.Char.o|a|d</th>\n",
              "      <th>SentimentText.Char.a|d|&lt;␠&gt;</th>\n",
              "      <th>SentimentText.Char.d|&lt;␠&gt;|t</th>\n",
              "      <th>SentimentText.Char.&lt;␠&gt;|t|h</th>\n",
              "      <th>SentimentText.Char.t|h|a</th>\n",
              "      <th>SentimentText.Char.h|a|t</th>\n",
              "      <th>SentimentText.Char.a|t|&lt;␠&gt;</th>\n",
              "      <th>SentimentText.Char.t|&lt;␠&gt;|c</th>\n",
              "      <th>SentimentText.Char.&lt;␠&gt;|c|a</th>\n",
              "      <th>...</th>\n",
              "      <th>SentimentText_TransformedText.110</th>\n",
              "      <th>SentimentText_TransformedText.111</th>\n",
              "      <th>SentimentText_TransformedText.112</th>\n",
              "      <th>SentimentText_TransformedText.113</th>\n",
              "      <th>SentimentText_TransformedText.114</th>\n",
              "      <th>SentimentText_TransformedText.115</th>\n",
              "      <th>SentimentText_TransformedText.116</th>\n",
              "      <th>SentimentText_TransformedText.117</th>\n",
              "      <th>SentimentText_TransformedText.118</th>\n",
              "      <th>SentimentText_TransformedText.119</th>\n",
              "      <th>SentimentText_TransformedText.120</th>\n",
              "      <th>SentimentText_TransformedText.121</th>\n",
              "      <th>SentimentText_TransformedText.122</th>\n",
              "      <th>SentimentText_TransformedText.123</th>\n",
              "      <th>SentimentText_TransformedText.124</th>\n",
              "      <th>SentimentText_TransformedText.125</th>\n",
              "      <th>SentimentText_TransformedText.126</th>\n",
              "      <th>SentimentText_TransformedText.127</th>\n",
              "      <th>SentimentText_TransformedText.128</th>\n",
              "      <th>SentimentText_TransformedText.129</th>\n",
              "      <th>SentimentText_TransformedText.130</th>\n",
              "      <th>SentimentText_TransformedText.131</th>\n",
              "      <th>SentimentText_TransformedText.132</th>\n",
              "      <th>SentimentText_TransformedText.133</th>\n",
              "      <th>SentimentText_TransformedText.134</th>\n",
              "      <th>SentimentText_TransformedText.135</th>\n",
              "      <th>SentimentText_TransformedText.136</th>\n",
              "      <th>SentimentText_TransformedText.137</th>\n",
              "      <th>SentimentText_TransformedText.138</th>\n",
              "      <th>SentimentText_TransformedText.139</th>\n",
              "      <th>SentimentText_TransformedText.140</th>\n",
              "      <th>SentimentText_TransformedText.141</th>\n",
              "      <th>SentimentText_TransformedText.142</th>\n",
              "      <th>SentimentText_TransformedText.143</th>\n",
              "      <th>SentimentText_TransformedText.144</th>\n",
              "      <th>SentimentText_TransformedText.145</th>\n",
              "      <th>SentimentText_TransformedText.146</th>\n",
              "      <th>SentimentText_TransformedText.147</th>\n",
              "      <th>SentimentText_TransformedText.148</th>\n",
              "      <th>SentimentText_TransformedText.149</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.225018</td>\n",
              "      <td>0.337526</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.225018</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>0.112509</td>\n",
              "      <td>...</td>\n",
              "      <td>1.462784</td>\n",
              "      <td>0.528055</td>\n",
              "      <td>0.698277</td>\n",
              "      <td>0.249722</td>\n",
              "      <td>0.855730</td>\n",
              "      <td>0.858960</td>\n",
              "      <td>0.052535</td>\n",
              "      <td>0.637218</td>\n",
              "      <td>0.302744</td>\n",
              "      <td>0.641849</td>\n",
              "      <td>1.275095</td>\n",
              "      <td>1.569134</td>\n",
              "      <td>1.453716</td>\n",
              "      <td>1.825948</td>\n",
              "      <td>2.058919</td>\n",
              "      <td>0.825440</td>\n",
              "      <td>0.154269</td>\n",
              "      <td>1.684076</td>\n",
              "      <td>0.844316</td>\n",
              "      <td>0.474553</td>\n",
              "      <td>1.962824</td>\n",
              "      <td>-0.341685</td>\n",
              "      <td>0.574575</td>\n",
              "      <td>1.473381</td>\n",
              "      <td>1.423488</td>\n",
              "      <td>1.290106</td>\n",
              "      <td>0.760995</td>\n",
              "      <td>0.347281</td>\n",
              "      <td>0.938264</td>\n",
              "      <td>2.886388</td>\n",
              "      <td>-0.463254</td>\n",
              "      <td>-0.081983</td>\n",
              "      <td>1.956025</td>\n",
              "      <td>0.483862</td>\n",
              "      <td>1.145274</td>\n",
              "      <td>0.730155</td>\n",
              "      <td>1.072578</td>\n",
              "      <td>1.693832</td>\n",
              "      <td>-0.134111</td>\n",
              "      <td>3.389347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.123091</td>\n",
              "      <td>0.123091</td>\n",
              "      <td>0.246183</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.246183</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.123091</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.904996</td>\n",
              "      <td>1.730018</td>\n",
              "      <td>-0.507464</td>\n",
              "      <td>0.444537</td>\n",
              "      <td>0.542321</td>\n",
              "      <td>-0.166392</td>\n",
              "      <td>0.135847</td>\n",
              "      <td>0.147397</td>\n",
              "      <td>0.635437</td>\n",
              "      <td>0.183642</td>\n",
              "      <td>0.571249</td>\n",
              "      <td>1.210280</td>\n",
              "      <td>1.532342</td>\n",
              "      <td>1.225561</td>\n",
              "      <td>2.105577</td>\n",
              "      <td>0.654956</td>\n",
              "      <td>0.489232</td>\n",
              "      <td>0.486822</td>\n",
              "      <td>0.041106</td>\n",
              "      <td>0.056376</td>\n",
              "      <td>1.771506</td>\n",
              "      <td>1.859128</td>\n",
              "      <td>1.963889</td>\n",
              "      <td>1.518326</td>\n",
              "      <td>1.054131</td>\n",
              "      <td>0.818986</td>\n",
              "      <td>0.275295</td>\n",
              "      <td>1.228935</td>\n",
              "      <td>0.431228</td>\n",
              "      <td>1.252122</td>\n",
              "      <td>-0.529051</td>\n",
              "      <td>-0.036032</td>\n",
              "      <td>0.686052</td>\n",
              "      <td>0.628498</td>\n",
              "      <td>0.456804</td>\n",
              "      <td>0.624132</td>\n",
              "      <td>2.142290</td>\n",
              "      <td>1.189043</td>\n",
              "      <td>-0.230683</td>\n",
              "      <td>2.423379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.036936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184679</td>\n",
              "      <td>0.184679</td>\n",
              "      <td>0.110808</td>\n",
              "      <td>0.073872</td>\n",
              "      <td>0.073872</td>\n",
              "      <td>0.073872</td>\n",
              "      <td>0.110808</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.073872</td>\n",
              "      <td>0.184679</td>\n",
              "      <td>0.073872</td>\n",
              "      <td>0.073872</td>\n",
              "      <td>0.073872</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036936</td>\n",
              "      <td>...</td>\n",
              "      <td>3.476445</td>\n",
              "      <td>4.525440</td>\n",
              "      <td>2.924021</td>\n",
              "      <td>0.883503</td>\n",
              "      <td>2.382992</td>\n",
              "      <td>0.983437</td>\n",
              "      <td>1.911982</td>\n",
              "      <td>2.371340</td>\n",
              "      <td>1.374931</td>\n",
              "      <td>0.552659</td>\n",
              "      <td>2.672227</td>\n",
              "      <td>1.828075</td>\n",
              "      <td>2.560276</td>\n",
              "      <td>2.543293</td>\n",
              "      <td>2.374752</td>\n",
              "      <td>3.557652</td>\n",
              "      <td>2.376891</td>\n",
              "      <td>2.919363</td>\n",
              "      <td>2.543924</td>\n",
              "      <td>2.315325</td>\n",
              "      <td>2.776464</td>\n",
              "      <td>3.803963</td>\n",
              "      <td>2.165327</td>\n",
              "      <td>1.744349</td>\n",
              "      <td>2.197365</td>\n",
              "      <td>3.803279</td>\n",
              "      <td>1.292597</td>\n",
              "      <td>2.675485</td>\n",
              "      <td>1.978979</td>\n",
              "      <td>2.731369</td>\n",
              "      <td>1.069717</td>\n",
              "      <td>1.617938</td>\n",
              "      <td>3.589748</td>\n",
              "      <td>2.685204</td>\n",
              "      <td>2.276195</td>\n",
              "      <td>1.861450</td>\n",
              "      <td>2.721365</td>\n",
              "      <td>2.395365</td>\n",
              "      <td>0.310610</td>\n",
              "      <td>4.255451</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 8781 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   SentimentText.Char.<␂>|<␠>|<␠>  ...  SentimentText_TransformedText.149\n",
              "0                        0.112509  ...                           3.389347\n",
              "1                        0.123091  ...                           2.423379\n",
              "2                        0.036936  ...                           4.255451\n",
              "\n",
              "[3 rows x 8781 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNNEEEMped6E",
        "colab_type": "text"
      },
      "source": [
        "The above output contains the output from the NGramFeaturizer, i.e, the weighted word/sequence count. These columns are given as input to the SSWE model as an array of words. The above output contains the output of SSWE as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXYqvFYqh_Le",
        "colab_type": "text"
      },
      "source": [
        "Now we will create a pipeline for the full classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wznyUjFlS09l",
        "colab_type": "code",
        "outputId": "98e34ac7-ffef-4964-f0b2-8d92f9ec1cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Generating the training and test data file stream    \n",
        "data_stream_train = FileDataStream.read_csv(Train_file, sep='\\t',collapse=False)\n",
        "data_stream_test = FileDataStream.read_csv(Test_file, sep='\\t',collapse=False)\n",
        "\n",
        "#Creating a pipeline using the NGramFeaturizer, WordEmbedding, and FastLinearBinaryClassifier transforms\n",
        "pipeline = Pipeline([NGramFeaturizer(word_feature_extractor=Ngram(), output_tokens_column_name='SentimentText_TransformedText', columns=['SentimentText']),\n",
        "                     WordEmbedding(columns = \"SentimentText_TransformedText\"), \n",
        "                     FastLinearBinaryClassifier(feature = [\"SentimentText_TransformedText\"], label = 'Sentiment')])\n",
        "\n",
        "#Training the pipeline\n",
        "pipeline.fit(data_stream_train)  "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.\n",
            "Using 1 thread to train.\n",
            "Automatically choosing a check frequency of 1.\n",
            "Auto-tuning parameters: maxIterations = 6300.\n",
            "Auto-tuning parameters: L2 = 2.667734E-05.\n",
            "Auto-tuning parameters: L1Threshold (L1/L2) = 0.25.\n",
            "Using best model from iteration 2591.\n",
            "Not training a calibrator because it is not needed.\n",
            "Elapsed time: 00:00:01.0356104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nimbusml.pipeline.Pipeline at 0x7fb7ba939b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27SlczJkTInH",
        "colab_type": "code",
        "outputId": "35ddb548-6606-40b2-f3dd-7811c2b7d8c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "#Testing the pipeline on our test data\n",
        "metrics, scores = pipeline.test(data_stream_test,'Sentiment', output_scores = True)\n",
        "metrics"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AUC</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Positive precision</th>\n",
              "      <th>Positive recall</th>\n",
              "      <th>Negative precision</th>\n",
              "      <th>Negative recall</th>\n",
              "      <th>Log-loss</th>\n",
              "      <th>Log-loss reduction</th>\n",
              "      <th>Test-set entropy (prior Log-Loss/instance)</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>AUPRC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.938272</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.564218</td>\n",
              "      <td>0.435782</td>\n",
              "      <td>1</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.958791</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        AUC  Accuracy  ...  F1 Score     AUPRC\n",
              "0  0.938272  0.833333  ...  0.842105  0.958791\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA3swuFbi6LP",
        "colab_type": "text"
      },
      "source": [
        "Therefore, we have obtained a test accuracy of 0.833333 and an AUC of 0.938272 using our FastLinearBinaryClassifier to detect whether a comment on a Wikipedia talk page is a personal comment or not, using a pure nimbusml pipeline with the SSWE model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKhuTI1yTUHv",
        "colab_type": "code",
        "outputId": "51371b26-1f19-4073-8614-afd98831372c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#Plotting the ROC (Receiver Operating Characteristic) curve based on the test data\n",
        "plot_roc_score(dataTest[\"Sentiment\"], scores[\"Probability\"]);"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMemyRGGMMSZd/w85q37NwHUK5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCMNfnvg1QDF",
        "colab_type": "text"
      },
      "source": [
        "(iv) NimbusML provides a Sentiment class for scoring natural language text and measuring the probability that the sentiments of text are positive.\n",
        "\n",
        "The syntax is as follows: *Sentiment(columns=None, **params)*\n",
        "\n",
        "The 'columns' parameter specifies a dictionary of key-value pairs, where key is the output column name and value is a list of input column names.\n",
        "\n",
        "'**params' denotes a set of additional arguments sent to the compute engine.\n",
        "\n",
        "The Sentiment transform outputs the sentiment prediction as a score between 0 and 1, where 0 is a negative sentiment and 1 is a positive sentiment. The model was trained on Twitter sentiment data and currently supports only English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjjHIx3QYJUE",
        "colab_type": "text"
      },
      "source": [
        "**II. Removing Stopwords**\n",
        "\n",
        "Now we will see how we can use NimbusML to remove stopwords from input text.\n",
        "\n",
        "NimbusML's CustomStopWordsRemover class removes a list of stopwords that the user specifies.\n",
        "\n",
        "The syntax is as follows: *CustomStopWordsRemover(stopword = None, **params)*\n",
        "\n",
        "The 'stopword' parameter is a list of stopwords to be removed, while '**params' specifies a set of additional arguments sent to the compute engine.\n",
        "\n",
        "Below is an example using the CustomStopWordsRemover class. The NGramFeaturizer can help us use the class. \n",
        "\n",
        "We will use the Wikipedia Detox dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvDyOBp9YKf8",
        "colab_type": "code",
        "outputId": "d19ca12b-9f5e-4b61-ecbf-660855bfa119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from nimbusml import FileDataStream\n",
        "from nimbusml.datasets import get_dataset\n",
        "from nimbusml.feature_extraction.text import NGramFeaturizer\n",
        "from nimbusml.feature_extraction.text.extractor import Ngram\n",
        "from nimbusml.feature_extraction.text.stopwords import CustomStopWordsRemover\n",
        "\n",
        "#Obtaining data input as a FileDataStream\n",
        "path = get_dataset('wiki_detox_train').as_filepath()\n",
        "\n",
        "data = FileDataStream.read_csv(path, sep='\\t')\n",
        "print(data.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "#Creating the featurizer\n",
        "xf = NGramFeaturizer(word_feature_extractor=Ngram(),\n",
        "                        stop_words_remover=CustomStopWordsRemover(['!','$','%','&','\\'','\\'d']), #Specifying the list of stopwords to be removed\n",
        "                        columns={'features': ['SentimentText']})\n",
        "\n",
        "features = xf.fit_transform(data)\n",
        "\n",
        "#Printing the extracted features for the first 5 entries of the training dataset\n",
        "print(features.head())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Sentiment                                      SentimentText\n",
            "0          1  ==RUDE== Dude, you are rude upload that carl p...\n",
            "1          1  == OK! ==  IM GOING TO VANDALIZE WILD ONES WIK...\n",
            "2          1  Stop trolling, zapatancas, calling me a liar m...\n",
            "3          1  ==You're cool==  You seem like a really cool g...\n",
            "4          1  ::::: Why are you threatening me? I'm not bein...\n",
            "\n",
            "\n",
            "   Sentiment  ... features.Word.award.\n",
            "0          1  ...                  0.0\n",
            "1          1  ...                  0.0\n",
            "2          1  ...                  0.0\n",
            "3          1  ...                  0.0\n",
            "4          1  ...                  0.0\n",
            "\n",
            "[5 rows x 8602 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H_7_yZmcvZJ",
        "colab_type": "text"
      },
      "source": [
        "NimbusML's PredefinedStopWordsRemover class is used to remove a predefined list of stopwords from text.\n",
        "\n",
        "The syntax is as follows: *PredefinedStopWordsRemover(**params)*\n",
        "\n",
        "'**params' is a set of additional arguments sent to the compute engine.\n",
        "\n",
        "Below is an example using the PredefinedStopWordsRemover class. The NGramFeaturizer can help us use the class. \n",
        "\n",
        "We will again use the Wikipedia Detox dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb5ZqRKQdNQr",
        "colab_type": "code",
        "outputId": "7ba24331-cbb3-4067-8470-813f0dbfe7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from nimbusml import FileDataStream\n",
        "from nimbusml.datasets import get_dataset\n",
        "from nimbusml.feature_extraction.text import NGramFeaturizer\n",
        "from nimbusml.feature_extraction.text.extractor import Ngram\n",
        "\n",
        "#Obtaining data input as a FileDataStream\n",
        "path = get_dataset(\"wiki_detox_train\").as_filepath()\n",
        "\n",
        "data = FileDataStream.read_csv(path, sep='\\t')\n",
        "print(data.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "#Creating the featurizer\n",
        "xf = NGramFeaturizer(\n",
        "       word_feature_extractor=Ngram(),\n",
        "       columns={\n",
        "           'features': ['SentimentText']})\n",
        "\n",
        "features = xf.fit_transform(data)\n",
        "\n",
        "#Printing the extracted features for the first 5 entries of the training dataset\n",
        "print(features.head())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Sentiment                                      SentimentText\n",
            "0          1  ==RUDE== Dude, you are rude upload that carl p...\n",
            "1          1  == OK! ==  IM GOING TO VANDALIZE WILD ONES WIK...\n",
            "2          1  Stop trolling, zapatancas, calling me a liar m...\n",
            "3          1  ==You're cool==  You seem like a really cool g...\n",
            "4          1  ::::: Why are you threatening me? I'm not bein...\n",
            "\n",
            "\n",
            "   Sentiment  ... features.Word.award.\n",
            "0          1  ...                  0.0\n",
            "1          1  ...                  0.0\n",
            "2          1  ...                  0.0\n",
            "3          1  ...                  0.0\n",
            "4          1  ...                  0.0\n",
            "\n",
            "[5 rows x 8651 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1DmvTG1vB12",
        "colab_type": "text"
      },
      "source": [
        "**III. Working with N-grams**\n",
        "\n",
        "NGramFeaturizer, as discussed previously, is a transform that outputs a bag of counts of sequences of consecutive words, or n-grams, from a corpus of text. It can extract features, i.e, the n-grams, in one of two ways. We will see these two methods here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHVVGXp1pfSF",
        "colab_type": "text"
      },
      "source": [
        "(i) NGram class\n",
        "\n",
        "NimbusML's NGram class can be used to extract n-grams from text and then convert them to vector form using a dictionary.\n",
        "\n",
        "We had been using the NGram feature extractor in the examples thus far.\n",
        "\n",
        "The syntax is as follows: *Ngram(ngram_length=1, skip_length=0, all_lengths=True, max_num_terms=[10000000], weighting='Tf', **params)*\n",
        "\n",
        "The 'ngram_length' parameter denotes the length of the n-gram we want.\n",
        "\n",
        "The 'skip_length' parameter denotes the maximum number of tokens to skip when constructing the n-gram.\n",
        "\n",
        "The 'all_lengths' parameter denotes whether we want to include all n-gram lengths up to the specified 'ngram_length' or only the 'ngram_length'.\n",
        "\n",
        "The 'max_num_terms' parameter denotes the maximum number of n-grams we want to store in the dictionary.\n",
        "\n",
        "The 'weighting' parameter specifies the weighting criteria.\n",
        "\n",
        "'**params' specifies a set of additional arguments sent to the compute engine.\n",
        "\n",
        "Below is an example that demonstrates use of the NGram class, again using the Wikipedia Detox dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7rxvDf8caw3",
        "colab_type": "code",
        "outputId": "6bcf5b95-8fff-448b-d957-024abf7358d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from nimbusml import FileDataStream\n",
        "from nimbusml.datasets import get_dataset\n",
        "from nimbusml.feature_extraction.text import NGramFeaturizer\n",
        "from nimbusml.feature_extraction.text.extractor import Ngram\n",
        "\n",
        "#Obtaining data input as a FileDataStream\n",
        "path = get_dataset(\"wiki_detox_train\").as_filepath()\n",
        "\n",
        "data = FileDataStream.read_csv(path, sep='\\t')\n",
        "print(data.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "#Creating the featurizer and specifying it to extract n-grams using the NGram class\n",
        "xf = NGramFeaturizer(\n",
        "word_feature_extractor=Ngram(),\n",
        "columns={'features': ['SentimentText']})\n",
        "#The 'columns' parameter denotes the name of the output column\n",
        "\n",
        "features = xf.fit_transform(data)\n",
        "\n",
        "#Printing the extracted features for the first 5 entries of the training dataset\n",
        "print(features.head())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Sentiment                                      SentimentText\n",
            "0          1  ==RUDE== Dude, you are rude upload that carl p...\n",
            "1          1  == OK! ==  IM GOING TO VANDALIZE WILD ONES WIK...\n",
            "2          1  Stop trolling, zapatancas, calling me a liar m...\n",
            "3          1  ==You're cool==  You seem like a really cool g...\n",
            "4          1  ::::: Why are you threatening me? I'm not bein...\n",
            "\n",
            "\n",
            "   Sentiment  ... features.Word.award.\n",
            "0          1  ...                  0.0\n",
            "1          1  ...                  0.0\n",
            "2          1  ...                  0.0\n",
            "3          1  ...                  0.0\n",
            "4          1  ...                  0.0\n",
            "\n",
            "[5 rows x 8651 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HESStINIc3sK",
        "colab_type": "text"
      },
      "source": [
        "(ii) NGramHash class\n",
        "\n",
        "The NGramHash class in NimbusML is also used to extract n-grams from text and convert them to vector form using a dictionary, but using hashing.\n",
        "\n",
        "The syntax is as follows: *NgramHash(number_of_bits=16, ngram_length=1, skip_length=0, all_lengths=True, seed=314489979, ordered=True, maximum_number_of_inverts=0, **params)*\n",
        "\n",
        "The parameter 'number_of_bits' denotes the number of bits to hash into (between 1 and 30, inclusive).\n",
        "\n",
        "The 'seed' parameter specifies the hashing seed value.\n",
        "\n",
        "The 'ordered' parameter denotes whether the position of each source column should be included in the hash when there are multiple source columns.\n",
        "\n",
        "The 'maximum_number_of_inverts' denotes the value to which we are limiting the number of keys used to generate the slot. 0 means no invert hashing and -1 means no limit.\n",
        "\n",
        "The remaining parameters mean the same as in the NGram class.\n",
        "\n",
        "Below is an example that demonstrates use of the NGramHash class, again using the Wikipedia Detox dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-95ZdIjscrDG",
        "colab_type": "code",
        "outputId": "d1aacb89-77fe-4f1c-cace-8ac7eaef742c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from nimbusml import FileDataStream\n",
        "from nimbusml.datasets import get_dataset\n",
        "from nimbusml.feature_extraction.text import NGramFeaturizer\n",
        "from nimbusml.feature_extraction.text.extractor import NgramHash\n",
        "\n",
        "#Obtaining data input as a FileDataStream\n",
        "path = get_dataset('wiki_detox_train').as_filepath()\n",
        "\n",
        "data = FileDataStream.read_csv(path, sep='\\t')\n",
        "print(data.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "#Creating the featurizer and specifying it to extract n-grams using the NGramHash class\n",
        "xf = NGramFeaturizer(word_feature_extractor=NgramHash(),\n",
        "                        columns=['SentimentText'])\n",
        "#The 'columns' parameter denotes the name of the output column\n",
        "\n",
        "features = xf.fit_transform(data)\n",
        "\n",
        "#Printing the extracted features for the first 5 entries of the training dataset\n",
        "print(features.head())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Sentiment                                      SentimentText\n",
            "0          1  ==RUDE== Dude, you are rude upload that carl p...\n",
            "1          1  == OK! ==  IM GOING TO VANDALIZE WILD ONES WIK...\n",
            "2          1  Stop trolling, zapatancas, calling me a liar m...\n",
            "3          1  ==You're cool==  You seem like a really cool g...\n",
            "4          1  ::::: Why are you threatening me? I'm not bein...\n",
            "\n",
            "\n",
            "   Sentiment  ...  SentimentText.70837\n",
            "0          1  ...                  0.0\n",
            "1          1  ...                  0.0\n",
            "2          1  ...                  0.0\n",
            "3          1  ...                  0.0\n",
            "4          1  ...                  0.0\n",
            "\n",
            "[5 rows x 70839 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd13CBandvlW",
        "colab_type": "text"
      },
      "source": [
        "Now we will finally inspect the NGramFeaturizer class itself.\n",
        "\n",
        "The syntax is as follows: *NGramFeaturizer(language='English', stop_words_remover=None, text_case='Lower', keep_diacritics=False, keep_punctuations=True, keep_numbers=True, output_tokens_column_name=None, dictionary=None, word_feature_extractor={'Name': 'NGram', 'Settings': {'NgramLength': 1, 'SkipLength': 0, 'AllLengths': True, 'MaxNumTerms': [10000000], 'Weighting': 'Tf'}}, char_feature_extractor={'Name': 'NGram', 'Settings': {'NgramLength': 3, 'SkipLength': 0, 'AllLengths': False, 'MaxNumTerms': [10000000], 'Weighting': 'Tf'}}, vector_normalizer='L2', columns=None, **params)*\n",
        "\n",
        "The 'columns' parameter specifies a dictionary of key-value pairs, where key is the output column name and value is a list of input column names.\n",
        "\n",
        "The 'language' parameter specifies the language used in the dataset.\n",
        "\n",
        "The 'stop_words_remover' parameter denotes the stopword remover we want to use, i.e, whether CustomStopWordsRemover, PredefinedStopWordsRemover, or neither.\n",
        "\n",
        "The 'text_case' parameter specifies the text casing.\n",
        "\n",
        "The 'keep_diacritics' parameter denotes whether to retain the diacritical marks or not.\n",
        "\n",
        "The 'keep_punctuations' parameter denotes whether we want to keep punctuation or not.\n",
        "\n",
        "The 'keep_numbers' parameter denotes whether we want to keep or remove numbers.\n",
        "\n",
        "The 'output_tokens_column_name' parameter denotes the name of the column containing the transformed text tokens.\n",
        "\n",
        "The 'dictionary' parameter denotes a dictionary of whitelisted terms.\n",
        "\n",
        "The parameter 'word_feature_extractor' denotes the word feature extraction arguments, i.e whether we want to use the NGram class or the NGramHash class for feature extraction.\n",
        "\n",
        "The parameter 'char_feature_extractor' denotes the character feature extraction arguments. Again this can be done using the NGram class or the NGramHash class.\n",
        "\n",
        "The parameter 'vector_normalizer' specifies which norm we want to use to normalize rows individually by rescaling them.\n",
        "\n",
        "'**params' denotes the set of additional arguments sent to the compute engine.\n",
        "\n",
        "We will use an example from above to demonstrate how we can use this class to extract n-grams from text using the NGram class.\n",
        "\n",
        "We will again use the Wikipedia Detox dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwdvvmgGdtwT",
        "colab_type": "code",
        "outputId": "b286e2a1-7266-4c59-acb6-d88bcbab1a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from nimbusml import FileDataStream\n",
        "from nimbusml.datasets import get_dataset\n",
        "from nimbusml.feature_extraction.text import NGramFeaturizer\n",
        "from nimbusml.feature_extraction.text.extractor import Ngram\n",
        "\n",
        "#Obtaining data input as a FileDataStream\n",
        "path = get_dataset(\"wiki_detox_train\").as_filepath()\n",
        "\n",
        "data = FileDataStream.read_csv(path, sep='\\t')\n",
        "print(data.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "#Creating the featurizer\n",
        "xf = NGramFeaturizer(\n",
        "       word_feature_extractor=Ngram(),\n",
        "       columns={\n",
        "           'features': ['SentimentText']})\n",
        "\n",
        "features = xf.fit_transform(data)\n",
        "\n",
        "#Printing the extracted features for the first 5 entries of the training dataset\n",
        "print(features.head())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Sentiment                                      SentimentText\n",
            "0          1  ==RUDE== Dude, you are rude upload that carl p...\n",
            "1          1  == OK! ==  IM GOING TO VANDALIZE WILD ONES WIK...\n",
            "2          1  Stop trolling, zapatancas, calling me a liar m...\n",
            "3          1  ==You're cool==  You seem like a really cool g...\n",
            "4          1  ::::: Why are you threatening me? I'm not bein...\n",
            "\n",
            "\n",
            "   Sentiment  ... features.Word.award.\n",
            "0          1  ...                  0.0\n",
            "1          1  ...                  0.0\n",
            "2          1  ...                  0.0\n",
            "3          1  ...                  0.0\n",
            "4          1  ...                  0.0\n",
            "\n",
            "[5 rows x 8651 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMvuRUjaeL5L",
        "colab_type": "text"
      },
      "source": [
        "**IV. Word Embedding**\n",
        "\n",
        "We will see how NimbusML provides support for word embedding. The WordEmbedding class can be used to convert the vectors of text tokens obtained from the featurizer into sentence vectors using a pre-trained model. In our Sentiment Analysis section, we had used the SSWE model for word embedding.\n",
        "\n",
        "The syntax is as follows: *WordEmbedding(model_kind='SentimentSpecificWordEmbedding', custom_lookup_table=None, columns=None, **params)*\n",
        "\n",
        "The 'model_kind' parameter specifies which pre-trained DNN model we want to use for our word embedding, i.e, to create our vocabulary. Options include SSWE, GloVe50D, GloVeTwitter100D, and FastTextWikipedia300D.\n",
        "\n",
        "The 'custom_lookup_table' parameter denotes a filename for a custom word embedding model.\n",
        "\n",
        "'**params' denotes the set of additional arguments sent to the compute engine.\n",
        "\n",
        "Below is an example that demonstrates use of the WordEmbedding class, again using the Wikipedia Detox dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8unEk5IYj5wr",
        "colab_type": "code",
        "outputId": "219602dc-f584-4cd7-e0cb-473c8a01fc9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from nimbusml import FileDataStream, Pipeline\n",
        "from nimbusml.datasets import get_dataset\n",
        "from nimbusml.feature_extraction.text import NGramFeaturizer, WordEmbedding\n",
        "from nimbusml.feature_extraction.text.extractor import Ngram\n",
        "\n",
        "#Obtaining data input as a FileDataStream\n",
        "path = get_dataset('wiki_detox_train').as_filepath()\n",
        "data = FileDataStream.read_csv(path, sep='\\t')\n",
        "print(data.head())\n",
        "print(\"\\n\")\n",
        "  \n",
        "#Creating the pipeline using NGramFeaturizer and WordEmbedding\n",
        "pipeline = Pipeline([\n",
        "       NGramFeaturizer(word_feature_extractor=Ngram(), output_tokens_column_name='ngram_TransformedText',\n",
        "                       columns={'ngram': ['SentimentText']}),\n",
        "\n",
        "       WordEmbedding(columns='ngram_TransformedText') #In this example, we are not specifying any specific pre-trained DNN model\n",
        "   ])\n",
        "\n",
        "features = pipeline.fit_transform(data)\n",
        "\n",
        "#Printing the extracted features for the first 5 entries of the training dataset\n",
        "print(features.head())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Sentiment                                      SentimentText\n",
            "0          1  ==RUDE== Dude, you are rude upload that carl p...\n",
            "1          1  == OK! ==  IM GOING TO VANDALIZE WILD ONES WIK...\n",
            "2          1  Stop trolling, zapatancas, calling me a liar m...\n",
            "3          1  ==You're cool==  You seem like a really cool g...\n",
            "4          1  ::::: Why are you threatening me? I'm not bein...\n",
            "\n",
            "\n",
            "   Sentiment  ... ngram.Word.award.\n",
            "0          1  ...               0.0\n",
            "1          1  ...               0.0\n",
            "2          1  ...               0.0\n",
            "3          1  ...               0.0\n",
            "4          1  ...               0.0\n",
            "\n",
            "[5 rows x 8801 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Ud_8sQ7CDj",
        "colab_type": "text"
      },
      "source": [
        "Thus, to conclude, we have seen an in-depth exploration of NimbusML's NLP features in sentiment analysis and text processing, particularly in word feature extraction, stopword removal, and word embedding."
      ]
    }
  ]
}